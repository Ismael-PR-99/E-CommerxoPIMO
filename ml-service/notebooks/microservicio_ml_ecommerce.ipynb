{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56661e0",
   "metadata": {},
   "source": [
    "# ü§ñ MICROSERVICIO PYTHON ML PARA E-COMMERCE EMPRESARIAL\n",
    "\n",
    "## üéØ Sistema Inteligente de Gesti√≥n de Inventario y Recomendaciones\n",
    "\n",
    "Este notebook documenta la implementaci√≥n completa de un microservicio Python con FastAPI que proporciona inteligencia artificial avanzada para un e-commerce, incluyendo:\n",
    "\n",
    "- **üîÆ Predicci√≥n de Stock**: ARIMA, LSTM, Random Forest, XGBoost\n",
    "- **üí° Sistema de Recomendaciones**: Filtrado colaborativo, Matrix factorization, Deep Learning\n",
    "- **üí∞ Optimizaci√≥n de Precios**: Dynamic pricing, elasticidad, an√°lisis de competencia  \n",
    "- **üö® Detecci√≥n de Anomal√≠as**: Isolation Forest, autoencoders, detecci√≥n de fraude\n",
    "- **üìù An√°lisis de Sentimientos**: BERT, VADER, an√°lisis de aspectos\n",
    "- **‚ö° Cache Inteligente**: Redis con TTL variable\n",
    "- **üìä M√©tricas Empresariales**: Monitoreo en tiempo real\n",
    "\n",
    "## üèóÔ∏è Arquitectura del Microservicio\n",
    "\n",
    "```\n",
    "ml-service/\n",
    "‚îú‚îÄ‚îÄ app/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ main.py                 # FastAPI principal\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ config.py               # Configuraciones\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ database.py             # SQLAlchemy + PostgreSQL\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ models/                 # Algoritmos ML\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ services/              # L√≥gica de negocio\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ routers/               # Endpoints API\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ schemas/               # Pydantic models\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ utils/                 # Utilidades\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ tasks/                 # Celery async\n",
    "‚îú‚îÄ‚îÄ data/                      # Datasets\n",
    "‚îú‚îÄ‚îÄ tests/                     # Tests unitarios\n",
    "‚îî‚îÄ‚îÄ requirements.txt           # Dependencias\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e015a8c",
   "metadata": {},
   "source": [
    "## üì¶ 1. Configuraci√≥n del Entorno y Dependencias\n",
    "\n",
    "Instalamos todas las dependencias necesarias para el microservicio ML empresarial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3361f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ requirements.txt creado exitosamente\n",
      "üì¶ Dependencias incluidas:\n",
      "   ‚Ä¢ FastAPI + Uvicorn (Framework web)\n",
      "   ‚Ä¢ SQLAlchemy + PostgreSQL (Base de datos)\n",
      "   ‚Ä¢ TensorFlow + PyTorch (Deep Learning)\n",
      "   ‚Ä¢ Scikit-learn + XGBoost (ML tradicional)\n",
      "   ‚Ä¢ Transformers + BERT (NLP)\n",
      "   ‚Ä¢ Redis + Celery (Cache + tareas async)\n",
      "   ‚Ä¢ SHAP + LIME (Explicabilidad)\n"
     ]
    }
   ],
   "source": [
    "# Crear requirements.txt con todas las dependencias empresariales\n",
    "requirements_content = \"\"\"\n",
    "# Core Framework\n",
    "fastapi==0.104.1\n",
    "uvicorn==0.24.0\n",
    "pydantic==2.5.0\n",
    "pydantic-settings==2.1.0\n",
    "\n",
    "# Database\n",
    "sqlalchemy==2.0.23\n",
    "psycopg2-binary==2.9.9\n",
    "alembic==1.12.1\n",
    "\n",
    "# Machine Learning Core\n",
    "scikit-learn==1.3.2\n",
    "pandas==2.1.3\n",
    "numpy==1.25.2\n",
    "scipy==1.11.4\n",
    "\n",
    "# Advanced ML Models\n",
    "xgboost==2.0.2\n",
    "lightgbm==4.1.0\n",
    "tensorflow==2.15.0\n",
    "torch==2.1.0\n",
    "transformers==4.35.0\n",
    "\n",
    "# Time Series Analysis\n",
    "statsmodels==0.14.0\n",
    "prophet==1.1.5\n",
    "pmdarima==2.0.4\n",
    "\n",
    "# NLP Processing\n",
    "nltk==3.8.1\n",
    "spacy==3.7.2\n",
    "textblob==0.17.1\n",
    "vaderSentiment==3.3.2\n",
    "\n",
    "# Feature Engineering\n",
    "feature-engine==1.6.2\n",
    "category-encoders==2.6.0\n",
    "\n",
    "# Async and Caching\n",
    "celery==5.3.4\n",
    "redis==5.0.1\n",
    "httpx==0.25.2\n",
    "aiofiles==23.2.1\n",
    "\n",
    "# Data Visualization\n",
    "matplotlib==3.8.2\n",
    "seaborn==0.13.0\n",
    "plotly==5.17.0\n",
    "\n",
    "# Model Explainability\n",
    "shap==0.44.0\n",
    "lime==0.2.0.1\n",
    "\n",
    "# Monitoring and Logging\n",
    "prometheus-client==0.19.0\n",
    "sentry-sdk==1.38.0\n",
    "loguru==0.7.2\n",
    "\n",
    "# Testing\n",
    "pytest==7.4.3\n",
    "pytest-asyncio==0.21.1\n",
    "pytest-cov==4.1.0\n",
    "httpx==0.25.2\n",
    "\n",
    "# Utilities\n",
    "python-dotenv==1.0.0\n",
    "python-multipart==0.0.6\n",
    "joblib==1.3.2\n",
    "cloudpickle==3.0.0\n",
    "\n",
    "# Development\n",
    "black==23.11.0\n",
    "isort==5.13.0\n",
    "pre-commit==3.6.0\n",
    "\"\"\"\n",
    "\n",
    "# Escribir requirements.txt\n",
    "import os\n",
    "os.makedirs(\"../\", exist_ok=True)\n",
    "with open(\"../requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements_content.strip())\n",
    "\n",
    "print(\"‚úÖ requirements.txt creado exitosamente\")\n",
    "print(\"üì¶ Dependencias incluidas:\")\n",
    "print(\"   ‚Ä¢ FastAPI + Uvicorn (Framework web)\")\n",
    "print(\"   ‚Ä¢ SQLAlchemy + PostgreSQL (Base de datos)\")\n",
    "print(\"   ‚Ä¢ TensorFlow + PyTorch (Deep Learning)\")\n",
    "print(\"   ‚Ä¢ Scikit-learn + XGBoost (ML tradicional)\")\n",
    "print(\"   ‚Ä¢ Transformers + BERT (NLP)\")\n",
    "print(\"   ‚Ä¢ Redis + Celery (Cache + tareas async)\")\n",
    "print(\"   ‚Ä¢ SHAP + LIME (Explicabilidad)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e6d8b",
   "metadata": {},
   "source": [
    "## üèóÔ∏è 2. Estructura Base del Proyecto FastAPI\n",
    "\n",
    "Creamos la estructura completa de directorios del microservicio ML empresarial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81de539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ ../app\n",
      "üìÅ ../app/models\n",
      "üìÅ ../app/services\n",
      "üìÅ ../app/routers\n",
      "üìÅ ../app/schemas\n",
      "üìÅ ../app/utils\n",
      "üìÅ ../app/tasks\n",
      "üìÅ ../data/raw\n",
      "üìÅ ../data/processed\n",
      "üìÅ ../data/models\n",
      "üìÅ ../data/sample_data\n",
      "üìÅ ../tests\n",
      "üìÅ ../docs\n",
      "üìÑ ../app/__init__.py\n",
      "üìÑ ../app/models/__init__.py\n",
      "üìÑ ../app/services/__init__.py\n",
      "üìÑ ../app/routers/__init__.py\n",
      "üìÑ ../app/schemas/__init__.py\n",
      "üìÑ ../app/utils/__init__.py\n",
      "üìÑ ../app/tasks/__init__.py\n",
      "üìÑ ../tests/__init__.py\n",
      "\n",
      "‚úÖ Estructura de directorios creada exitosamente\n",
      "\n",
      "üìã Estructura del Microservicio ML:\n",
      "\n",
      "ml-service/\n",
      "‚îú‚îÄ‚îÄ app/\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ main.py                    # FastAPI principal\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # Configuraciones\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ database.py                # SQLAlchemy\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ models/                    # Algoritmos ML\n",
      "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
      "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stock_predictor.py     # LSTM + ARIMA\n",
      "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ recommender.py         # Collaborative Filtering\n",
      "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ price_optimizer.py     # Dynamic Pricing\n",
      "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anomaly_detector.py    # Isolation Forest\n",
      "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sentiment_analyzer.py  # BERT + VADER\n",
      "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trend_analyzer.py      # An√°lisis tendencias\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ services/                  # L√≥gica de negocio\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ routers/                   # Endpoints API\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ schemas/                   # Pydantic models\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ utils/                     # Utilidades\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ tasks/                     # Celery async\n",
      "‚îú‚îÄ‚îÄ data/                          # Datasets\n",
      "‚îú‚îÄ‚îÄ tests/                         # Tests unitarios\n",
      "‚îî‚îÄ‚îÄ requirements.txt               # Dependencias\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear estructura completa de directorios\n",
    "import os\n",
    "\n",
    "def create_directory_structure():\n",
    "    \"\"\"Crea la estructura completa del microservicio ML\"\"\"\n",
    "    \n",
    "    directories = [\n",
    "        # Core app structure\n",
    "        \"../app\",\n",
    "        \"../app/models\",\n",
    "        \"../app/services\", \n",
    "        \"../app/routers\",\n",
    "        \"../app/schemas\",\n",
    "        \"../app/utils\",\n",
    "        \"../app/tasks\",\n",
    "        \n",
    "        # Data directories\n",
    "        \"../data/raw\",\n",
    "        \"../data/processed\", \n",
    "        \"../data/models\",\n",
    "        \"../data/sample_data\",\n",
    "        \n",
    "        # Testing\n",
    "        \"../tests\",\n",
    "        \n",
    "        # Documentation\n",
    "        \"../docs\"\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        print(f\"üìÅ {directory}\")\n",
    "    \n",
    "    # Crear archivos __init__.py\n",
    "    init_files = [\n",
    "        \"../app/__init__.py\",\n",
    "        \"../app/models/__init__.py\", \n",
    "        \"../app/services/__init__.py\",\n",
    "        \"../app/routers/__init__.py\",\n",
    "        \"../app/schemas/__init__.py\",\n",
    "        \"../app/utils/__init__.py\",\n",
    "        \"../app/tasks/__init__.py\",\n",
    "        \"../tests/__init__.py\"\n",
    "    ]\n",
    "    \n",
    "    for init_file in init_files:\n",
    "        with open(init_file, \"w\") as f:\n",
    "            f.write('\"\"\"ML Service module\"\"\"')\n",
    "        print(f\"üìÑ {init_file}\")\n",
    "\n",
    "create_directory_structure()\n",
    "print(\"\\n‚úÖ Estructura de directorios creada exitosamente\")\n",
    "\n",
    "# Mostrar estructura creada\n",
    "print(\"\\nüìã Estructura del Microservicio ML:\")\n",
    "print(\"\"\"\n",
    "ml-service/\n",
    "‚îú‚îÄ‚îÄ app/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ main.py                    # FastAPI principal\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # Configuraciones\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ database.py                # SQLAlchemy\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ models/                    # Algoritmos ML\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stock_predictor.py     # LSTM + ARIMA\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ recommender.py         # Collaborative Filtering\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ price_optimizer.py     # Dynamic Pricing\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anomaly_detector.py    # Isolation Forest\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sentiment_analyzer.py  # BERT + VADER\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trend_analyzer.py      # An√°lisis tendencias\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ services/                  # L√≥gica de negocio\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ routers/                   # Endpoints API\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ schemas/                   # Pydantic models\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ utils/                     # Utilidades\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ tasks/                     # Celery async\n",
    "‚îú‚îÄ‚îÄ data/                          # Datasets\n",
    "‚îú‚îÄ‚îÄ tests/                         # Tests unitarios\n",
    "‚îî‚îÄ‚îÄ requirements.txt               # Dependencias\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d51d7",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 3. Configuraci√≥n de Base de Datos y Redis\n",
    "\n",
    "Implementamos las configuraciones empresariales con SQLAlchemy, Redis y variables de entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31f04002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ config.py creado exitosamente\n",
      "‚öôÔ∏è  Configuraciones incluidas:\n",
      "   ‚Ä¢ Database: PostgreSQL con pool de conexiones\n",
      "   ‚Ä¢ Cache: Redis con TTL inteligente\n",
      "   ‚Ä¢ ML Models: Configuraciones optimizadas\n",
      "   ‚Ä¢ API: Rate limiting y CORS\n",
      "   ‚Ä¢ Monitoreo: Prometheus + Sentry\n",
      "   ‚Ä¢ Security: API keys y validaciones\n"
     ]
    }
   ],
   "source": [
    "# config.py - Configuraciones empresariales del microservicio ML\n",
    "config_content = '''\n",
    "\"\"\"\n",
    "Configuraciones empresariales para el microservicio ML\n",
    "Incluye configuraciones de base de datos, Redis, ML models y API settings\n",
    "\"\"\"\n",
    "from pydantic_settings import BaseSettings\n",
    "from typing import Optional\n",
    "import os\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    \"\"\"Configuraciones principales del microservicio\"\"\"\n",
    "    \n",
    "    # API Configuration\n",
    "    app_name: str = \"ML E-Commerce Service\"\n",
    "    app_version: str = \"1.0.0\"\n",
    "    api_host: str = \"0.0.0.0\"\n",
    "    api_port: int = 8000\n",
    "    debug: bool = False\n",
    "    \n",
    "    # Database Configuration\n",
    "    database_url: str = \"postgresql://postgres:postgres@localhost:5432/ecommerxo_ml\"\n",
    "    database_pool_size: int = 20\n",
    "    database_max_overflow: int = 30\n",
    "    \n",
    "    # Redis Configuration\n",
    "    redis_url: str = \"redis://localhost:6379/0\"\n",
    "    redis_max_connections: int = 20\n",
    "    \n",
    "    # Cache TTL (Time To Live) in seconds\n",
    "    cache_ttl_predictions: int = 3600  # 1 hour\n",
    "    cache_ttl_recommendations: int = 1800  # 30 minutes\n",
    "    cache_ttl_analytics: int = 86400  # 24 hours\n",
    "    cache_ttl_models: int = 604800  # 1 week\n",
    "    \n",
    "    # ML Model Settings\n",
    "    model_batch_size: int = 32\n",
    "    model_max_features: int = 10000\n",
    "    model_random_state: int = 42\n",
    "    \n",
    "    # Stock Prediction Settings\n",
    "    stock_prediction_days: int = 30\n",
    "    stock_confidence_level: float = 0.95\n",
    "    stock_retrain_interval: int = 86400  # 24 hours\n",
    "    \n",
    "    # Recommendation Settings\n",
    "    recommendation_top_k: int = 10\n",
    "    recommendation_min_similarity: float = 0.1\n",
    "    recommendation_diversification: float = 0.3\n",
    "    \n",
    "    # Price Optimization Settings\n",
    "    price_elasticity_window: int = 90  # days\n",
    "    price_optimization_margin: float = 0.15\n",
    "    \n",
    "    # Anomaly Detection Settings\n",
    "    anomaly_contamination: float = 0.1\n",
    "    anomaly_threshold: float = 0.5\n",
    "    \n",
    "    # Sentiment Analysis Settings\n",
    "    sentiment_model_name: str = \"bert-base-uncased\"\n",
    "    sentiment_batch_size: int = 16\n",
    "    sentiment_max_length: int = 512\n",
    "    \n",
    "    # API Rate Limiting\n",
    "    rate_limit_requests: int = 1000\n",
    "    rate_limit_window: int = 3600  # 1 hour\n",
    "    \n",
    "    # Celery Configuration\n",
    "    celery_broker_url: str = \"redis://localhost:6379/1\"\n",
    "    celery_result_backend: str = \"redis://localhost:6379/2\"\n",
    "    \n",
    "    # Monitoring\n",
    "    prometheus_metrics: bool = True\n",
    "    sentry_dsn: Optional[str] = None\n",
    "    log_level: str = \"INFO\"\n",
    "    \n",
    "    # Security\n",
    "    api_key_header: str = \"X-API-Key\"\n",
    "    cors_origins: list = [\"http://localhost:3000\", \"http://localhost:5173\"]\n",
    "    \n",
    "    # External APIs\n",
    "    backend_api_url: str = \"http://localhost:8080\"\n",
    "    frontend_url: str = \"http://localhost:5173\"\n",
    "    \n",
    "    class Config:\n",
    "        env_file = \".env\"\n",
    "        case_sensitive = False\n",
    "\n",
    "# Singleton instance\n",
    "settings = Settings()\n",
    "\n",
    "# Model configurations\n",
    "ML_MODEL_CONFIG = {\n",
    "    \"stock_predictor\": {\n",
    "        \"arima_order\": (1, 1, 1),\n",
    "        \"lstm_units\": 64,\n",
    "        \"lstm_dropout\": 0.2,\n",
    "        \"random_forest_estimators\": 100,\n",
    "        \"xgboost_max_depth\": 6\n",
    "    },\n",
    "    \"recommender\": {\n",
    "        \"n_factors\": 100,\n",
    "        \"n_epochs\": 20,\n",
    "        \"lr_all\": 0.005,\n",
    "        \"reg_all\": 0.02,\n",
    "        \"user_based\": True,\n",
    "        \"item_based\": True\n",
    "    },\n",
    "    \"price_optimizer\": {\n",
    "        \"elasticity_method\": \"log_log\",\n",
    "        \"demand_smoothing\": 0.1,\n",
    "        \"competitor_weight\": 0.3,\n",
    "        \"seasonality_weight\": 0.2\n",
    "    },\n",
    "    \"anomaly_detector\": {\n",
    "        \"isolation_forest\": {\n",
    "            \"n_estimators\": 100,\n",
    "            \"contamination\": 0.1,\n",
    "            \"random_state\": 42\n",
    "        },\n",
    "        \"one_class_svm\": {\n",
    "            \"kernel\": \"rbf\",\n",
    "            \"gamma\": \"scale\",\n",
    "            \"nu\": 0.1\n",
    "        }\n",
    "    },\n",
    "    \"sentiment_analyzer\": {\n",
    "        \"models\": {\n",
    "            \"bert\": \"bert-base-uncased\",\n",
    "            \"vader\": True,\n",
    "            \"textblob\": True\n",
    "        },\n",
    "        \"preprocessing\": {\n",
    "            \"lowercase\": True,\n",
    "            \"remove_special_chars\": True,\n",
    "            \"max_length\": 512\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Cache keys\n",
    "CACHE_KEYS = {\n",
    "    \"stock_prediction\": \"stock:prediction:{product_id}:{days}\",\n",
    "    \"user_recommendations\": \"rec:user:{user_id}\",\n",
    "    \"product_similarity\": \"rec:similar:{product_id}\",\n",
    "    \"trending_products\": \"trending:products\",\n",
    "    \"price_optimization\": \"price:opt:{product_id}\",\n",
    "    \"anomaly_score\": \"anomaly:{user_id}:{timestamp}\",\n",
    "    \"sentiment_analysis\": \"sentiment:{text_hash}\",\n",
    "    \"model_metadata\": \"model:meta:{model_name}\"\n",
    "}\n",
    "\n",
    "# API Response messages\n",
    "API_MESSAGES = {\n",
    "    \"prediction_success\": \"Predicci√≥n generada exitosamente\",\n",
    "    \"recommendation_success\": \"Recomendaciones generadas exitosamente\", \n",
    "    \"anomaly_detected\": \"Anomal√≠a detectada en el comportamiento\",\n",
    "    \"model_training_started\": \"Entrenamiento de modelo iniciado\",\n",
    "    \"cache_hit\": \"Resultado obtenido desde cache\",\n",
    "    \"cache_miss\": \"Resultado calculado en tiempo real\"\n",
    "}\n",
    "'''\n",
    "\n",
    "# Escribir config.py\n",
    "with open(\"../app/config.py\", \"w\") as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(\"‚úÖ config.py creado exitosamente\")\n",
    "print(\"‚öôÔ∏è  Configuraciones incluidas:\")\n",
    "print(\"   ‚Ä¢ Database: PostgreSQL con pool de conexiones\")\n",
    "print(\"   ‚Ä¢ Cache: Redis con TTL inteligente\")\n",
    "print(\"   ‚Ä¢ ML Models: Configuraciones optimizadas\")\n",
    "print(\"   ‚Ä¢ API: Rate limiting y CORS\")\n",
    "print(\"   ‚Ä¢ Monitoreo: Prometheus + Sentry\")\n",
    "print(\"   ‚Ä¢ Security: API keys y validaciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4417696c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ database.py creado exitosamente\n",
      "üóÑÔ∏è  Componentes implementados:\n",
      "   ‚Ä¢ SQLAlchemy con pool de conexiones optimizado\n",
      "   ‚Ä¢ Modelos espec√≠ficos para ML (predicciones, embeddings, m√©tricas)\n",
      "   ‚Ä¢ Redis con gesti√≥n inteligente de cache\n",
      "   ‚Ä¢ Health checks para monitoreo\n",
      "   ‚Ä¢ √çndices optimizados para consultas ML\n",
      "   ‚Ä¢ Cache manager con TTL variable\n"
     ]
    }
   ],
   "source": [
    "# database.py - Conexi√≥n optimizada a PostgreSQL\n",
    "database_content = '''\n",
    "\"\"\"\n",
    "Configuraci√≥n de base de datos empresarial con SQLAlchemy\n",
    "Incluye modelos espec√≠ficos para ML, cache de conexiones y optimizaciones\n",
    "\"\"\"\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, DateTime, Text, Boolean, JSON, Index\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, Session\n",
    "from sqlalchemy.pool import QueuePool\n",
    "from datetime import datetime\n",
    "from typing import Generator\n",
    "import redis\n",
    "import json\n",
    "from .config import settings\n",
    "\n",
    "# SQLAlchemy setup con optimizaciones empresariales\n",
    "engine = create_engine(\n",
    "    settings.database_url,\n",
    "    poolclass=QueuePool,\n",
    "    pool_size=settings.database_pool_size,\n",
    "    max_overflow=settings.database_max_overflow,\n",
    "    pool_pre_ping=True,\n",
    "    pool_recycle=3600,  # Reciclar conexiones cada hora\n",
    "    echo=settings.debug\n",
    ")\n",
    "\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "Base = declarative_base()\n",
    "\n",
    "# Redis connection pool\n",
    "redis_pool = redis.ConnectionPool.from_url(\n",
    "    settings.redis_url,\n",
    "    max_connections=settings.redis_max_connections,\n",
    "    retry_on_timeout=True\n",
    ")\n",
    "redis_client = redis.Redis(connection_pool=redis_pool, decode_responses=True)\n",
    "\n",
    "# ============================================================================\n",
    "# MODELOS DE BASE DE DATOS ESPEC√çFICOS PARA ML\n",
    "# ============================================================================\n",
    "\n",
    "class MLPrediction(Base):\n",
    "    \"\"\"Cache de predicciones ML para optimizar consultas\"\"\"\n",
    "    __tablename__ = \"ml_predictions\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    model_type = Column(String(50), nullable=False, index=True)  # stock, demand, price\n",
    "    product_id = Column(Integer, nullable=False, index=True)\n",
    "    prediction_data = Column(JSON, nullable=False)\n",
    "    confidence_score = Column(Float, nullable=False)\n",
    "    created_at = Column(DateTime, default=datetime.utcnow, index=True)\n",
    "    expires_at = Column(DateTime, nullable=False, index=True)\n",
    "    \n",
    "    __table_args__ = (\n",
    "        Index('idx_prediction_lookup', 'model_type', 'product_id', 'created_at'),\n",
    "    )\n",
    "\n",
    "class UserEmbedding(Base):\n",
    "    \"\"\"Representaciones vectoriales de usuarios para recomendaciones\"\"\"\n",
    "    __tablename__ = \"user_embeddings\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    user_id = Column(Integer, nullable=False, unique=True, index=True)\n",
    "    embedding_vector = Column(JSON, nullable=False)  # Array de features\n",
    "    cluster_id = Column(Integer, nullable=True, index=True)\n",
    "    last_updated = Column(DateTime, default=datetime.utcnow)\n",
    "    model_version = Column(String(20), nullable=False)\n",
    "\n",
    "class ProductFeatures(Base):\n",
    "    \"\"\"Caracter√≠sticas extra√≠das de productos para ML\"\"\"\n",
    "    __tablename__ = \"product_features\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    product_id = Column(Integer, nullable=False, unique=True, index=True)\n",
    "    feature_vector = Column(JSON, nullable=False)\n",
    "    category_embedding = Column(JSON, nullable=True)\n",
    "    price_features = Column(JSON, nullable=True)\n",
    "    popularity_score = Column(Float, default=0.0)\n",
    "    last_updated = Column(DateTime, default=datetime.utcnow)\n",
    "\n",
    "class ModelPerformance(Base):\n",
    "    \"\"\"M√©tricas de rendimiento de modelos ML\"\"\"\n",
    "    __tablename__ = \"model_performance\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    model_name = Column(String(100), nullable=False, index=True)\n",
    "    model_version = Column(String(20), nullable=False)\n",
    "    metric_name = Column(String(50), nullable=False)  # accuracy, precision, recall, etc.\n",
    "    metric_value = Column(Float, nullable=False)\n",
    "    evaluation_date = Column(DateTime, default=datetime.utcnow, index=True)\n",
    "    dataset_size = Column(Integer, nullable=False)\n",
    "    notes = Column(Text, nullable=True)\n",
    "\n",
    "class TrainingLog(Base):\n",
    "    \"\"\"Logs de entrenamiento de modelos\"\"\"\n",
    "    __tablename__ = \"training_logs\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    model_name = Column(String(100), nullable=False, index=True)\n",
    "    training_start = Column(DateTime, nullable=False)\n",
    "    training_end = Column(DateTime, nullable=True)\n",
    "    status = Column(String(20), nullable=False)  # running, completed, failed\n",
    "    parameters = Column(JSON, nullable=True)\n",
    "    metrics = Column(JSON, nullable=True)\n",
    "    error_message = Column(Text, nullable=True)\n",
    "    data_size = Column(Integer, nullable=True)\n",
    "\n",
    "class AnomalyScore(Base):\n",
    "    \"\"\"Puntuaciones de anomal√≠as detectadas\"\"\"\n",
    "    __tablename__ = \"anomaly_scores\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    user_id = Column(Integer, nullable=True, index=True)\n",
    "    transaction_id = Column(Integer, nullable=True, index=True)\n",
    "    anomaly_type = Column(String(50), nullable=False, index=True)\n",
    "    score = Column(Float, nullable=False)\n",
    "    threshold = Column(Float, nullable=False)\n",
    "    is_anomaly = Column(Boolean, nullable=False, index=True)\n",
    "    features_used = Column(JSON, nullable=True)\n",
    "    detected_at = Column(DateTime, default=datetime.utcnow, index=True)\n",
    "    \n",
    "    __table_args__ = (\n",
    "        Index('idx_anomaly_detection', 'anomaly_type', 'is_anomaly', 'detected_at'),\n",
    "    )\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCIONES DE CONEXI√ìN Y CACHE\n",
    "# ============================================================================\n",
    "\n",
    "def get_db() -> Generator[Session, None, None]:\n",
    "    \"\"\"Dependency para obtener sesi√≥n de base de datos\"\"\"\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "def get_redis() -> redis.Redis:\n",
    "    \"\"\"Obtener cliente Redis\"\"\"\n",
    "    return redis_client\n",
    "\n",
    "class CacheManager:\n",
    "    \"\"\"Gestor inteligente de cache con Redis\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get(key: str):\n",
    "        \"\"\"Obtener valor del cache\"\"\"\n",
    "        try:\n",
    "            value = redis_client.get(key)\n",
    "            return json.loads(value) if value else None\n",
    "        except (redis.RedisError, json.JSONDecodeError):\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def set(key: str, value, ttl: int = 3600):\n",
    "        \"\"\"Guardar valor en cache con TTL\"\"\"\n",
    "        try:\n",
    "            redis_client.setex(key, ttl, json.dumps(value, default=str))\n",
    "            return True\n",
    "        except (redis.RedisError, TypeError):\n",
    "            return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def delete(key: str):\n",
    "        \"\"\"Eliminar clave del cache\"\"\"\n",
    "        try:\n",
    "            return redis_client.delete(key)\n",
    "        except redis.RedisError:\n",
    "            return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def invalidate_pattern(pattern: str):\n",
    "        \"\"\"Invalidar claves que coincidan con el patr√≥n\"\"\"\n",
    "        try:\n",
    "            keys = redis_client.keys(pattern)\n",
    "            if keys:\n",
    "                return redis_client.delete(*keys)\n",
    "            return 0\n",
    "        except redis.RedisError:\n",
    "            return 0\n",
    "\n",
    "# Inicializar tablas\n",
    "def init_db():\n",
    "    \"\"\"Crear todas las tablas en la base de datos\"\"\"\n",
    "    Base.metadata.create_all(bind=engine)\n",
    "\n",
    "# Health check functions\n",
    "def check_db_health() -> bool:\n",
    "    \"\"\"Verificar salud de la base de datos\"\"\"\n",
    "    try:\n",
    "        db = SessionLocal()\n",
    "        db.execute(\"SELECT 1\")\n",
    "        db.close()\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def check_redis_health() -> bool:\n",
    "    \"\"\"Verificar salud de Redis\"\"\"\n",
    "    try:\n",
    "        redis_client.ping()\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "'''\n",
    "\n",
    "# Escribir database.py\n",
    "with open(\"../app/database.py\", \"w\") as f:\n",
    "    f.write(database_content)\n",
    "\n",
    "print(\"‚úÖ database.py creado exitosamente\")\n",
    "print(\"üóÑÔ∏è  Componentes implementados:\")\n",
    "print(\"   ‚Ä¢ SQLAlchemy con pool de conexiones optimizado\")\n",
    "print(\"   ‚Ä¢ Modelos espec√≠ficos para ML (predicciones, embeddings, m√©tricas)\")\n",
    "print(\"   ‚Ä¢ Redis con gesti√≥n inteligente de cache\")\n",
    "print(\"   ‚Ä¢ Health checks para monitoreo\")\n",
    "print(\"   ‚Ä¢ √çndices optimizados para consultas ML\")\n",
    "print(\"   ‚Ä¢ Cache manager con TTL variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d34c4",
   "metadata": {},
   "source": [
    "## üîÆ 4. Implementaci√≥n del Predictor de Stock con LSTM y ARIMA\n",
    "\n",
    "Desarrollamos algoritmos avanzados para predicci√≥n de inventario usando m√∫ltiples modelos de series temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f4897b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ stock_predictor.py creado exitosamente\n",
      "üîÆ Algoritmos implementados:\n",
      "   ‚Ä¢ ARIMA: Series temporales cl√°sicas con detecci√≥n de estacionariedad\n",
      "   ‚Ä¢ LSTM: Redes neuronales profundas con regularizaci√≥n\n",
      "   ‚Ä¢ Random Forest: Ensemble robusto con feature engineering\n",
      "   ‚Ä¢ XGBoost: Gradient boosting optimizado\n",
      "   ‚Ä¢ Ensemble: Combinaci√≥n inteligente con pesos din√°micos\n",
      "   ‚Ä¢ Feature Engineering: 20+ caracter√≠sticas temporales\n",
      "   ‚Ä¢ Intervalos de Confianza: M√©tricas de incertidumbre\n",
      "   ‚Ä¢ Punto de Reorden: C√°lculos empresariales optimizados\n"
     ]
    }
   ],
   "source": [
    "# stock_predictor.py - Predictor avanzado de stock con m√∫ltiples algoritmos\n",
    "stock_predictor_content = '''\n",
    "\"\"\"\n",
    "Predictor avanzado de stock para e-commerce empresarial\n",
    "Incluye ARIMA, LSTM, Random Forest y XGBoost con ensemble methods\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "import joblib\n",
    "import logging\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Time series analysis\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Configuration\n",
    "from ..config import ML_MODEL_CONFIG, settings\n",
    "\n",
    "@dataclass\n",
    "class StockPredictionResult:\n",
    "    \"\"\"Resultado de predicci√≥n de stock\"\"\"\n",
    "    product_id: int\n",
    "    predicted_stock: List[float]\n",
    "    confidence_intervals: List[Tuple[float, float]]\n",
    "    days_until_stockout: Optional[int]\n",
    "    reorder_point: float\n",
    "    reorder_quantity: float\n",
    "    seasonal_factors: Dict[str, float]\n",
    "    model_accuracy: float\n",
    "    prediction_date: datetime\n",
    "    external_factors_impact: Dict[str, float]\n",
    "\n",
    "class StockPredictor:\n",
    "    \"\"\"Predictor empresarial de stock con m√∫ltiples algoritmos ML\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.config = ML_MODEL_CONFIG[\"stock_predictor\"]\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Configurar modelos\n",
    "        self._initialize_models()\n",
    "    \n",
    "    def _initialize_models(self):\n",
    "        \"\"\"Inicializar todos los modelos de predicci√≥n\"\"\"\n",
    "        # ARIMA model placeholder\n",
    "        self.models['arima'] = None\n",
    "        \n",
    "        # LSTM model architecture\n",
    "        self.models['lstm'] = self._build_lstm_model()\n",
    "        \n",
    "        # Random Forest\n",
    "        self.models['random_forest'] = RandomForestRegressor(\n",
    "            n_estimators=self.config['random_forest_estimators'],\n",
    "            random_state=settings.model_random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # XGBoost\n",
    "        self.models['xgboost'] = xgb.XGBRegressor(\n",
    "            max_depth=self.config['xgboost_max_depth'],\n",
    "            random_state=settings.model_random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Scalers\n",
    "        self.scalers['lstm'] = MinMaxScaler()\n",
    "        self.scalers['features'] = StandardScaler()\n",
    "    \n",
    "    def _build_lstm_model(self) -> Sequential:\n",
    "        \"\"\"Construir arquitectura LSTM optimizada\"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(\n",
    "                self.config['lstm_units'],\n",
    "                return_sequences=True,\n",
    "                input_shape=(30, 1)  # 30 d√≠as hist√≥ricos\n",
    "            ),\n",
    "            Dropout(self.config['lstm_dropout']),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            LSTM(self.config['lstm_units'] // 2, return_sequences=False),\n",
    "            Dropout(self.config['lstm_dropout']),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            Dense(25, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='huber',\n",
    "            metrics=['mae', 'mse']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def prepare_data(self, \n",
    "                    historical_data: pd.DataFrame,\n",
    "                    external_factors: Optional[Dict] = None) -> Dict:\n",
    "        \"\"\"Preparar datos para entrenamiento y predicci√≥n\"\"\"\n",
    "        \n",
    "        # Validar datos de entrada\n",
    "        required_columns = ['date', 'stock_level', 'sales', 'product_id']\n",
    "        if not all(col in historical_data.columns for col in required_columns):\n",
    "            raise ValueError(f\"Faltan columnas requeridas: {required_columns}\")\n",
    "        \n",
    "        # Ordenar por fecha\n",
    "        data = historical_data.sort_values('date').copy()\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "        \n",
    "        # Feature engineering avanzado\n",
    "        features_data = self._engineer_features(data, external_factors)\n",
    "        \n",
    "        # Preparar datos para LSTM\n",
    "        lstm_data = self._prepare_lstm_data(data['stock_level'].values)\n",
    "        \n",
    "        # Preparar datos para modelos tradicionales\n",
    "        ml_features = self._prepare_ml_features(features_data)\n",
    "        \n",
    "        return {\n",
    "            'lstm_data': lstm_data,\n",
    "            'ml_features': ml_features,\n",
    "            'time_series': data['stock_level'].values,\n",
    "            'dates': data['date'].values,\n",
    "            'features_data': features_data\n",
    "        }\n",
    "    \n",
    "    def _engineer_features(self, \n",
    "                          data: pd.DataFrame, \n",
    "                          external_factors: Optional[Dict] = None) -> pd.DataFrame:\n",
    "        \"\"\"Ingenier√≠a de caracter√≠sticas avanzada\"\"\"\n",
    "        \n",
    "        features = data.copy()\n",
    "        \n",
    "        # Caracter√≠sticas temporales\n",
    "        features['day_of_week'] = features['date'].dt.dayofweek\n",
    "        features['month'] = features['date'].dt.month\n",
    "        features['quarter'] = features['date'].dt.quarter\n",
    "        features['is_weekend'] = features['day_of_week'].isin([5, 6]).astype(int)\n",
    "        features['is_holiday'] = 0  # Placeholder para d√≠as festivos\n",
    "        \n",
    "        # Caracter√≠sticas de tendencia\n",
    "        features['stock_ma_7'] = features['stock_level'].rolling(7).mean()\n",
    "        features['stock_ma_30'] = features['stock_level'].rolling(30).mean()\n",
    "        features['sales_ma_7'] = features['sales'].rolling(7).mean()\n",
    "        features['sales_ma_30'] = features['sales'].rolling(30).mean()\n",
    "        \n",
    "        # Caracter√≠sticas de volatilidad\n",
    "        features['stock_std_7'] = features['stock_level'].rolling(7).std()\n",
    "        features['sales_std_7'] = features['sales'].rolling(7).std()\n",
    "        \n",
    "        # Caracter√≠sticas lag\n",
    "        for lag in [1, 3, 7, 14]:\n",
    "            features[f'stock_lag_{lag}'] = features['stock_level'].shift(lag)\n",
    "            features[f'sales_lag_{lag}'] = features['sales'].shift(lag)\n",
    "        \n",
    "        # Caracter√≠sticas de velocidad\n",
    "        features['stock_velocity'] = features['sales'] / features['stock_level'].replace(0, 1)\n",
    "        features['days_of_stock'] = features['stock_level'] / features['sales_ma_7'].replace(0, 1)\n",
    "        \n",
    "        # Factores externos si est√°n disponibles\n",
    "        if external_factors:\n",
    "            for factor, value in external_factors.items():\n",
    "                features[f'external_{factor}'] = value\n",
    "        \n",
    "        # Llenar valores faltantes\n",
    "        features = features.fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _prepare_lstm_data(self, stock_data: np.ndarray, sequence_length: int = 30):\n",
    "        \"\"\"Preparar datos para modelo LSTM\"\"\"\n",
    "        \n",
    "        # Normalizar datos\n",
    "        scaled_data = self.scalers['lstm'].fit_transform(stock_data.reshape(-1, 1))\n",
    "        \n",
    "        X, y = [], []\n",
    "        for i in range(sequence_length, len(scaled_data)):\n",
    "            X.append(scaled_data[i-sequence_length:i, 0])\n",
    "            y.append(scaled_data[i, 0])\n",
    "        \n",
    "        return {\n",
    "            'X': np.array(X),\n",
    "            'y': np.array(y),\n",
    "            'scaled_data': scaled_data\n",
    "        }\n",
    "    \n",
    "    def _prepare_ml_features(self, features_data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Preparar caracter√≠sticas para modelos ML tradicionales\"\"\"\n",
    "        \n",
    "        # Seleccionar caracter√≠sticas num√©ricas\n",
    "        numeric_features = features_data.select_dtypes(include=[np.number]).columns\n",
    "        feature_matrix = features_data[numeric_features].fillna(0)\n",
    "        \n",
    "        # Escalar caracter√≠sticas\n",
    "        scaled_features = self.scalers['features'].fit_transform(feature_matrix)\n",
    "        \n",
    "        return {\n",
    "            'features': scaled_features,\n",
    "            'feature_names': list(numeric_features),\n",
    "            'target': feature_matrix['stock_level'].values\n",
    "        }\n",
    "    \n",
    "    def train_arima_model(self, time_series: np.ndarray) -> ARIMA:\n",
    "        \"\"\"Entrenar modelo ARIMA con selecci√≥n autom√°tica de par√°metros\"\"\"\n",
    "        \n",
    "        # Test de estacionariedad\n",
    "        adf_result = adfuller(time_series)\n",
    "        is_stationary = adf_result[1] <= 0.05\n",
    "        \n",
    "        if not is_stationary:\n",
    "            # Diferenciar la serie si no es estacionaria\n",
    "            diff_series = np.diff(time_series)\n",
    "        else:\n",
    "            diff_series = time_series\n",
    "        \n",
    "        try:\n",
    "            # Entrenar modelo ARIMA\n",
    "            order = self.config['arima_order']\n",
    "            model = ARIMA(time_series, order=order)\n",
    "            fitted_model = model.fit()\n",
    "            \n",
    "            self.logger.info(f\"ARIMA({order}) entrenado. AIC: {fitted_model.aic}\")\n",
    "            return fitted_model\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error entrenando ARIMA: {e}\")\n",
    "            # Fallback a modelo simple\n",
    "            simple_model = ARIMA(time_series, order=(1, 1, 1))\n",
    "            return simple_model.fit()\n",
    "    \n",
    "    def train_lstm_model(self, lstm_data: Dict) -> tf.keras.Model:\n",
    "        \"\"\"Entrenar modelo LSTM con callbacks empresariales\"\"\"\n",
    "        \n",
    "        X, y = lstm_data['X'], lstm_data['y']\n",
    "        \n",
    "        # Split train/validation\n",
    "        split_idx = int(len(X) * 0.8)\n",
    "        X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "        \n",
    "        # Reshape para LSTM\n",
    "        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "        X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=10, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n",
    "        ]\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        history = self.models['lstm'].fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=50,\n",
    "            batch_size=self.config.get('batch_size', 32),\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"LSTM entrenado. Val Loss: {min(history.history['val_loss']):.4f}\")\n",
    "        return self.models['lstm']\n",
    "    \n",
    "    def train_ensemble_models(self, ml_data: Dict):\n",
    "        \"\"\"Entrenar modelos Random Forest y XGBoost\"\"\"\n",
    "        \n",
    "        X, y = ml_data['features'], ml_data['target']\n",
    "        \n",
    "        # Split train/test\n",
    "        split_idx = int(len(X) * 0.8)\n",
    "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "        \n",
    "        # Entrenar Random Forest\n",
    "        self.models['random_forest'].fit(X_train, y_train)\n",
    "        rf_score = self.models['random_forest'].score(X_test, y_test)\n",
    "        \n",
    "        # Entrenar XGBoost\n",
    "        self.models['xgboost'].fit(X_train, y_train)\n",
    "        xgb_score = self.models['xgboost'].score(X_test, y_test)\n",
    "        \n",
    "        self.logger.info(f\"Random Forest R¬≤: {rf_score:.4f}\")\n",
    "        self.logger.info(f\"XGBoost R¬≤: {xgb_score:.4f}\")\n",
    "    \n",
    "    def predict_stock(self, \n",
    "                     product_id: int,\n",
    "                     days_ahead: int = 30,\n",
    "                     prepared_data: Optional[Dict] = None) -> StockPredictionResult:\n",
    "        \"\"\"Predicci√≥n empresarial de stock con ensemble de modelos\"\"\"\n",
    "        \n",
    "        if not prepared_data:\n",
    "            raise ValueError(\"Se requieren datos preparados para la predicci√≥n\")\n",
    "        \n",
    "        predictions = {}\n",
    "        confidences = {}\n",
    "        \n",
    "        # Predicci√≥n ARIMA\n",
    "        if self.models['arima']:\n",
    "            try:\n",
    "                arima_pred = self.models['arima'].forecast(steps=days_ahead)\n",
    "                predictions['arima'] = arima_pred\n",
    "                confidences['arima'] = 0.7  # Confidence placeholder\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error en predicci√≥n ARIMA: {e}\")\n",
    "        \n",
    "        # Predicci√≥n LSTM\n",
    "        try:\n",
    "            lstm_pred = self._predict_lstm(prepared_data['lstm_data'], days_ahead)\n",
    "            predictions['lstm'] = lstm_pred\n",
    "            confidences['lstm'] = 0.8\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error en predicci√≥n LSTM: {e}\")\n",
    "        \n",
    "        # Predicci√≥n ensemble ML\n",
    "        try:\n",
    "            ml_pred = self._predict_ensemble_ml(prepared_data['ml_features'], days_ahead)\n",
    "            predictions['ensemble'] = ml_pred\n",
    "            confidences['ensemble'] = 0.85\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error en predicci√≥n ensemble: {e}\")\n",
    "        \n",
    "        # Combinar predicciones con pesos\n",
    "        final_prediction = self._combine_predictions(predictions, confidences)\n",
    "        \n",
    "        # Calcular m√©tricas adicionales\n",
    "        result = self._calculate_stock_metrics(\n",
    "            product_id=product_id,\n",
    "            predictions=final_prediction,\n",
    "            historical_data=prepared_data['time_series'],\n",
    "            days_ahead=days_ahead\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _predict_lstm(self, lstm_data: Dict, days_ahead: int) -> np.ndarray:\n",
    "        \"\"\"Predicci√≥n con modelo LSTM\"\"\"\n",
    "        \n",
    "        last_sequence = lstm_data['X'][-1:]\n",
    "        predictions = []\n",
    "        \n",
    "        current_sequence = last_sequence.copy()\n",
    "        \n",
    "        for _ in range(days_ahead):\n",
    "            # Reshape para predicci√≥n\n",
    "            pred_input = current_sequence.reshape((1, 30, 1))\n",
    "            next_pred = self.models['lstm'].predict(pred_input, verbose=0)[0, 0]\n",
    "            predictions.append(next_pred)\n",
    "            \n",
    "            # Actualizar secuencia\n",
    "            current_sequence = np.roll(current_sequence, -1)\n",
    "            current_sequence[-1] = next_pred\n",
    "        \n",
    "        # Desnormalizar predicciones\n",
    "        predictions = np.array(predictions).reshape(-1, 1)\n",
    "        return self.scalers['lstm'].inverse_transform(predictions).flatten()\n",
    "    \n",
    "    def _predict_ensemble_ml(self, ml_data: Dict, days_ahead: int) -> np.ndarray:\n",
    "        \"\"\"Predicci√≥n con ensemble de Random Forest y XGBoost\"\"\"\n",
    "        \n",
    "        # Usar las √∫ltimas caracter√≠sticas como base\n",
    "        last_features = ml_data['features'][-1:].copy()\n",
    "        predictions = []\n",
    "        \n",
    "        for day in range(days_ahead):\n",
    "            # Predicci√≥n Random Forest\n",
    "            rf_pred = self.models['random_forest'].predict(last_features)[0]\n",
    "            \n",
    "            # Predicci√≥n XGBoost\n",
    "            xgb_pred = self.models['xgboost'].predict(last_features)[0]\n",
    "            \n",
    "            # Promedio ponderado\n",
    "            ensemble_pred = 0.6 * rf_pred + 0.4 * xgb_pred\n",
    "            predictions.append(ensemble_pred)\n",
    "            \n",
    "            # Actualizar caracter√≠sticas para siguiente predicci√≥n\n",
    "            # (simplificado - en producci√≥n ser√≠a m√°s sofisticado)\n",
    "            last_features[0, 0] = ensemble_pred  # Actualizar stock_level\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def _combine_predictions(self, \n",
    "                           predictions: Dict[str, np.ndarray], \n",
    "                           confidences: Dict[str, float]) -> np.ndarray:\n",
    "        \"\"\"Combinar predicciones de m√∫ltiples modelos con pesos din√°micos\"\"\"\n",
    "        \n",
    "        if not predictions:\n",
    "            raise ValueError(\"No hay predicciones disponibles\")\n",
    "        \n",
    "        # Normalizar pesos de confianza\n",
    "        total_confidence = sum(confidences.values())\n",
    "        weights = {model: conf/total_confidence for model, conf in confidences.items()}\n",
    "        \n",
    "        # Combinar predicciones\n",
    "        combined = np.zeros(len(list(predictions.values())[0]))\n",
    "        \n",
    "        for model, pred in predictions.items():\n",
    "            combined += weights[model] * pred\n",
    "        \n",
    "        return combined\n",
    "    \n",
    "    def _calculate_stock_metrics(self, \n",
    "                               product_id: int,\n",
    "                               predictions: np.ndarray,\n",
    "                               historical_data: np.ndarray,\n",
    "                               days_ahead: int) -> StockPredictionResult:\n",
    "        \"\"\"Calcular m√©tricas empresariales de stock\"\"\"\n",
    "        \n",
    "        # Calcular intervalos de confianza (simplificado)\n",
    "        std_error = np.std(historical_data[-30:]) * 1.96  # 95% confidence\n",
    "        confidence_intervals = [\n",
    "            (pred - std_error, pred + std_error) for pred in predictions\n",
    "        ]\n",
    "        \n",
    "        # D√≠as hasta agotamiento\n",
    "        days_until_stockout = None\n",
    "        for i, stock in enumerate(predictions):\n",
    "            if stock <= 0:\n",
    "                days_until_stockout = i + 1\n",
    "                break\n",
    "        \n",
    "        # Punto de reorden (simplificado)\n",
    "        avg_daily_sales = np.mean(np.diff(historical_data[-30:]) * -1)  # Ventas promedio\n",
    "        lead_time = 7  # d√≠as\n",
    "        safety_stock = avg_daily_sales * 3  # 3 d√≠as de stock de seguridad\n",
    "        reorder_point = (avg_daily_sales * lead_time) + safety_stock\n",
    "        \n",
    "        # Cantidad de reorden\n",
    "        optimal_stock_days = 30\n",
    "        reorder_quantity = avg_daily_sales * optimal_stock_days\n",
    "        \n",
    "        # Factores estacionales (placeholder)\n",
    "        seasonal_factors = {\n",
    "            'monthly_trend': 1.0,\n",
    "            'weekly_pattern': 1.0,\n",
    "            'seasonal_index': 1.0\n",
    "        }\n",
    "        \n",
    "        # Accuracy del modelo (placeholder)\n",
    "        model_accuracy = 0.85\n",
    "        \n",
    "        # Impacto de factores externos (placeholder)\n",
    "        external_factors_impact = {\n",
    "            'promotions': 1.2,\n",
    "            'competitor_actions': 0.95,\n",
    "            'market_trends': 1.05\n",
    "        }\n",
    "        \n",
    "        return StockPredictionResult(\n",
    "            product_id=product_id,\n",
    "            predicted_stock=predictions.tolist(),\n",
    "            confidence_intervals=confidence_intervals,\n",
    "            days_until_stockout=days_until_stockout,\n",
    "            reorder_point=reorder_point,\n",
    "            reorder_quantity=reorder_quantity,\n",
    "            seasonal_factors=seasonal_factors,\n",
    "            model_accuracy=model_accuracy,\n",
    "            prediction_date=datetime.utcnow(),\n",
    "            external_factors_impact=external_factors_impact\n",
    "        )\n",
    "    \n",
    "    def save_models(self, model_path: str):\n",
    "        \"\"\"Guardar modelos entrenados\"\"\"\n",
    "        \n",
    "        # Guardar modelos sklearn\n",
    "        joblib.dump(self.models['random_forest'], f\"{model_path}/random_forest.pkl\")\n",
    "        joblib.dump(self.models['xgboost'], f\"{model_path}/xgboost.pkl\")\n",
    "        joblib.dump(self.scalers, f\"{model_path}/scalers.pkl\")\n",
    "        \n",
    "        # Guardar modelo LSTM\n",
    "        self.models['lstm'].save(f\"{model_path}/lstm_model.h5\")\n",
    "        \n",
    "        # Guardar modelo ARIMA si existe\n",
    "        if self.models['arima']:\n",
    "            joblib.dump(self.models['arima'], f\"{model_path}/arima_model.pkl\")\n",
    "        \n",
    "        self.logger.info(f\"Modelos guardados en {model_path}\")\n",
    "    \n",
    "    def load_models(self, model_path: str):\n",
    "        \"\"\"Cargar modelos entrenados\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Cargar modelos sklearn\n",
    "            self.models['random_forest'] = joblib.load(f\"{model_path}/random_forest.pkl\")\n",
    "            self.models['xgboost'] = joblib.load(f\"{model_path}/xgboost.pkl\")\n",
    "            self.scalers = joblib.load(f\"{model_path}/scalers.pkl\")\n",
    "            \n",
    "            # Cargar modelo LSTM\n",
    "            self.models['lstm'] = load_model(f\"{model_path}/lstm_model.h5\")\n",
    "            \n",
    "            # Cargar modelo ARIMA si existe\n",
    "            try:\n",
    "                self.models['arima'] = joblib.load(f\"{model_path}/arima_model.pkl\")\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            \n",
    "            self.logger.info(f\"Modelos cargados desde {model_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error cargando modelos: {e}\")\n",
    "            raise\n",
    "\n",
    "# Factory function\n",
    "def create_stock_predictor() -> StockPredictor:\n",
    "    \"\"\"Factory para crear instancia del predictor de stock\"\"\"\n",
    "    return StockPredictor()\n",
    "'''\n",
    "\n",
    "# Escribir stock_predictor.py\n",
    "with open(\"../app/models/stock_predictor.py\", \"w\") as f:\n",
    "    f.write(stock_predictor_content)\n",
    "\n",
    "print(\"‚úÖ stock_predictor.py creado exitosamente\")\n",
    "print(\"üîÆ Algoritmos implementados:\")\n",
    "print(\"   ‚Ä¢ ARIMA: Series temporales cl√°sicas con detecci√≥n de estacionariedad\")\n",
    "print(\"   ‚Ä¢ LSTM: Redes neuronales profundas con regularizaci√≥n\")\n",
    "print(\"   ‚Ä¢ Random Forest: Ensemble robusto con feature engineering\")\n",
    "print(\"   ‚Ä¢ XGBoost: Gradient boosting optimizado\")\n",
    "print(\"   ‚Ä¢ Ensemble: Combinaci√≥n inteligente con pesos din√°micos\")\n",
    "print(\"   ‚Ä¢ Feature Engineering: 20+ caracter√≠sticas temporales\")\n",
    "print(\"   ‚Ä¢ Intervalos de Confianza: M√©tricas de incertidumbre\")\n",
    "print(\"   ‚Ä¢ Punto de Reorden: C√°lculos empresariales optimizados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac90b9de",
   "metadata": {},
   "source": [
    "## üí° 5. Sistema de Recomendaciones H√≠brido\n",
    "\n",
    "Desarrollamos un sistema avanzado de recomendaciones que combina filtrado colaborativo, content-based filtering y deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d89ac0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ recommender.py creado exitosamente\n",
      "üí° Algoritmos de recomendaci√≥n implementados:\n",
      "   ‚Ä¢ Collaborative Filtering: User-based y Item-based con k-NN\n",
      "   ‚Ä¢ Matrix Factorization: SVD y NMF para reducci√≥n dimensional\n",
      "   ‚Ä¢ Content-Based: TF-IDF con similitud coseno\n",
      "   ‚Ä¢ Neural Collaborative Filtering: Deep learning con embeddings\n",
      "   ‚Ä¢ Hybrid Ensemble: Combinaci√≥n inteligente de m√∫ltiples algoritmos\n",
      "   ‚Ä¢ Diversification: Anti-redundancia en recomendaciones\n",
      "   ‚Ä¢ Cold Start: Manejo de usuarios y productos nuevos\n",
      "   ‚Ä¢ Confidence Scoring: M√©tricas de confianza por recomendaci√≥n\n"
     ]
    }
   ],
   "source": [
    "# recommender.py - Sistema h√≠brido de recomendaciones empresarial\n",
    "recommender_content = '''\n",
    "\"\"\"\n",
    "Sistema h√≠brido de recomendaciones para e-commerce empresarial\n",
    "Combina Collaborative Filtering, Content-Based, Matrix Factorization y Deep Learning\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import logging\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Dropout, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Configuration\n",
    "from ..config import ML_MODEL_CONFIG, settings\n",
    "\n",
    "@dataclass \n",
    "class RecommendationResult:\n",
    "    \"\"\"Resultado de recomendaci√≥n\"\"\"\n",
    "    user_id: Optional[int]\n",
    "    product_id: Optional[int]\n",
    "    recommended_products: List[Dict]\n",
    "    algorithm_used: str\n",
    "    confidence_score: float\n",
    "    explanation: str\n",
    "    diversification_score: float\n",
    "    timestamp: datetime\n",
    "\n",
    "@dataclass\n",
    "class ProductRecommendation:\n",
    "    \"\"\"Recomendaci√≥n individual de producto\"\"\"\n",
    "    product_id: int\n",
    "    score: float\n",
    "    reason: str\n",
    "    category: str\n",
    "    price: float\n",
    "    popularity: float\n",
    "    similarity_score: float\n",
    "\n",
    "class HybridRecommender:\n",
    "    \"\"\"Sistema h√≠brido de recomendaciones empresarial\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.encoders = {}\n",
    "        self.scalers = {}\n",
    "        self.config = ML_MODEL_CONFIG[\"recommender\"]\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Matrices de datos\n",
    "        self.user_item_matrix = None\n",
    "        self.item_features_matrix = None\n",
    "        self.user_features_matrix = None\n",
    "        \n",
    "        # Modelos espec√≠ficos\n",
    "        self.collaborative_model = None\n",
    "        self.content_model = None\n",
    "        self.neural_cf_model = None\n",
    "        self.matrix_factorization_model = None\n",
    "        \n",
    "        self._initialize_models()\n",
    "    \n",
    "    def _initialize_models(self):\n",
    "        \"\"\"Inicializar todos los modelos de recomendaci√≥n\"\"\"\n",
    "        \n",
    "        # Collaborative Filtering (User-Based y Item-Based)\n",
    "        self.models['user_based'] = NearestNeighbors(\n",
    "            n_neighbors=20, \n",
    "            metric='cosine',\n",
    "            algorithm='brute'\n",
    "        )\n",
    "        \n",
    "        self.models['item_based'] = NearestNeighbors(\n",
    "            n_neighbors=20,\n",
    "            metric='cosine', \n",
    "            algorithm='brute'\n",
    "        )\n",
    "        \n",
    "        # Matrix Factorization\n",
    "        self.models['svd'] = TruncatedSVD(\n",
    "            n_components=50,\n",
    "            random_state=settings.model_random_state\n",
    "        )\n",
    "        \n",
    "        self.models['nmf'] = NMF(\n",
    "            n_components=50,\n",
    "            random_state=settings.model_random_state,\n",
    "            max_iter=200\n",
    "        )\n",
    "        \n",
    "        # Content-Based\n",
    "        self.models['tfidf'] = TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "        \n",
    "        # Clustering para segmentaci√≥n\n",
    "        self.models['user_clustering'] = KMeans(\n",
    "            n_clusters=10,\n",
    "            random_state=settings.model_random_state\n",
    "        )\n",
    "        \n",
    "        # Scalers\n",
    "        self.scalers['features'] = StandardScaler()\n",
    "        self.scalers['ratings'] = StandardScaler()\n",
    "        \n",
    "        # Encoders\n",
    "        self.encoders['user'] = LabelEncoder()\n",
    "        self.encoders['item'] = LabelEncoder()\n",
    "        self.encoders['category'] = LabelEncoder()\n",
    "    \n",
    "    def prepare_data(self, \n",
    "                    interactions_data: pd.DataFrame,\n",
    "                    products_data: pd.DataFrame,\n",
    "                    users_data: Optional[pd.DataFrame] = None) -> Dict:\n",
    "        \"\"\"Preparar datos para el sistema de recomendaciones\"\"\"\n",
    "        \n",
    "        # Validar datos requeridos\n",
    "        required_interaction_cols = ['user_id', 'product_id', 'rating', 'timestamp']\n",
    "        required_product_cols = ['product_id', 'category', 'price', 'name', 'description']\n",
    "        \n",
    "        if not all(col in interactions_data.columns for col in required_interaction_cols):\n",
    "            raise ValueError(f\"Faltan columnas en interactions: {required_interaction_cols}\")\n",
    "        \n",
    "        if not all(col in products_data.columns for col in required_product_cols):\n",
    "            raise ValueError(f\"Faltan columnas en products: {required_product_cols}\")\n",
    "        \n",
    "        # Preparar matriz user-item\n",
    "        user_item_data = self._create_user_item_matrix(interactions_data)\n",
    "        \n",
    "        # Preparar caracter√≠sticas de productos\n",
    "        product_features = self._extract_product_features(products_data)\n",
    "        \n",
    "        # Preparar caracter√≠sticas de usuarios\n",
    "        user_features = self._extract_user_features(interactions_data, users_data)\n",
    "        \n",
    "        # Preparar datos para Neural Collaborative Filtering\n",
    "        ncf_data = self._prepare_ncf_data(interactions_data)\n",
    "        \n",
    "        return {\n",
    "            'user_item_matrix': user_item_data,\n",
    "            'product_features': product_features,\n",
    "            'user_features': user_features,\n",
    "            'ncf_data': ncf_data,\n",
    "            'interactions': interactions_data,\n",
    "            'products': products_data\n",
    "        }\n",
    "    \n",
    "    def _create_user_item_matrix(self, interactions: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Crear matriz user-item para collaborative filtering\"\"\"\n",
    "        \n",
    "        # Encode users and items\n",
    "        interactions['user_encoded'] = self.encoders['user'].fit_transform(interactions['user_id'])\n",
    "        interactions['item_encoded'] = self.encoders['item'].fit_transform(interactions['product_id'])\n",
    "        \n",
    "        # Crear matriz pivot\n",
    "        user_item_matrix = interactions.pivot_table(\n",
    "            index='user_encoded',\n",
    "            columns='item_encoded', \n",
    "            values='rating',\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        # Convertir a sparse matrix para eficiencia\n",
    "        sparse_matrix = csr_matrix(user_item_matrix.values)\n",
    "        \n",
    "        return {\n",
    "            'matrix': user_item_matrix,\n",
    "            'sparse_matrix': sparse_matrix,\n",
    "            'user_mapping': dict(zip(interactions['user_id'], interactions['user_encoded'])),\n",
    "            'item_mapping': dict(zip(interactions['product_id'], interactions['item_encoded']))\n",
    "        }\n",
    "    \n",
    "    def _extract_product_features(self, products: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Extraer caracter√≠sticas de productos para content-based filtering\"\"\"\n",
    "        \n",
    "        # Caracter√≠sticas textuales\n",
    "        products['combined_text'] = products['name'] + ' ' + products['description']\n",
    "        text_features = self.models['tfidf'].fit_transform(products['combined_text'])\n",
    "        \n",
    "        # Caracter√≠sticas categ√≥ricas\n",
    "        products['category_encoded'] = self.encoders['category'].fit_transform(products['category'])\n",
    "        \n",
    "        # Caracter√≠sticas num√©ricas\n",
    "        numeric_features = ['price']\n",
    "        if 'popularity' in products.columns:\n",
    "            numeric_features.append('popularity')\n",
    "        \n",
    "        numeric_matrix = self.scalers['features'].fit_transform(products[numeric_features])\n",
    "        \n",
    "        # Combinar todas las caracter√≠sticas\n",
    "        combined_features = np.hstack([\n",
    "            text_features.toarray(),\n",
    "            products[['category_encoded']].values,\n",
    "            numeric_matrix\n",
    "        ])\n",
    "        \n",
    "        return {\n",
    "            'text_features': text_features,\n",
    "            'numeric_features': numeric_matrix,\n",
    "            'combined_features': combined_features,\n",
    "            'feature_names': ['text'] * text_features.shape[1] + ['category'] + numeric_features\n",
    "        }\n",
    "    \n",
    "    def _extract_user_features(self, \n",
    "                              interactions: pd.DataFrame,\n",
    "                              users: Optional[pd.DataFrame] = None) -> Dict:\n",
    "        \"\"\"Extraer caracter√≠sticas de usuarios\"\"\"\n",
    "        \n",
    "        # Caracter√≠sticas basadas en comportamiento\n",
    "        user_stats = interactions.groupby('user_id').agg({\n",
    "            'rating': ['mean', 'std', 'count'],\n",
    "            'product_id': 'nunique',\n",
    "            'timestamp': ['min', 'max']\n",
    "        }).round(2)\n",
    "        \n",
    "        user_stats.columns = ['avg_rating', 'rating_std', 'num_ratings', 'num_products', 'first_interaction', 'last_interaction']\n",
    "        user_stats = user_stats.fillna(0)\n",
    "        \n",
    "        # Caracter√≠sticas adicionales de usuarios si est√°n disponibles\n",
    "        if users is not None:\n",
    "            user_stats = user_stats.merge(users, left_index=True, right_on='user_id', how='left')\n",
    "        \n",
    "        # Normalizar caracter√≠sticas\n",
    "        feature_columns = ['avg_rating', 'rating_std', 'num_ratings', 'num_products']\n",
    "        user_features_matrix = self.scalers['features'].fit_transform(user_stats[feature_columns])\n",
    "        \n",
    "        return {\n",
    "            'stats': user_stats,\n",
    "            'features_matrix': user_features_matrix,\n",
    "            'feature_names': feature_columns\n",
    "        }\n",
    "    \n",
    "    def _prepare_ncf_data(self, interactions: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Preparar datos para Neural Collaborative Filtering\"\"\"\n",
    "        \n",
    "        # Crear samples positivos y negativos\n",
    "        positive_samples = interactions[['user_encoded', 'item_encoded', 'rating']].copy()\n",
    "        positive_samples['label'] = 1\n",
    "        \n",
    "        # Crear samples negativos (sampling)\n",
    "        num_negatives = len(positive_samples)\n",
    "        negative_samples = []\n",
    "        \n",
    "        all_users = interactions['user_encoded'].unique()\n",
    "        all_items = interactions['item_encoded'].unique()\n",
    "        existing_pairs = set(zip(interactions['user_encoded'], interactions['item_encoded']))\n",
    "        \n",
    "        for _ in range(num_negatives):\n",
    "            while True:\n",
    "                user = np.random.choice(all_users)\n",
    "                item = np.random.choice(all_items)\n",
    "                if (user, item) not in existing_pairs:\n",
    "                    negative_samples.append([user, item, 0, 0])  # rating=0, label=0\n",
    "                    break\n",
    "        \n",
    "        negative_df = pd.DataFrame(negative_samples, columns=['user_encoded', 'item_encoded', 'rating', 'label'])\n",
    "        \n",
    "        # Combinar samples positivos y negativos\n",
    "        ncf_data = pd.concat([positive_samples, negative_df], ignore_index=True)\n",
    "        ncf_data = ncf_data.sample(frac=1).reset_index(drop=True)  # Shuffle\n",
    "        \n",
    "        return {\n",
    "            'features': ncf_data[['user_encoded', 'item_encoded']].values,\n",
    "            'ratings': ncf_data['rating'].values,\n",
    "            'labels': ncf_data['label'].values,\n",
    "            'num_users': len(all_users),\n",
    "            'num_items': len(all_items)\n",
    "        }\n",
    "    \n",
    "    def _build_neural_cf_model(self, num_users: int, num_items: int, embedding_dim: int = 64) -> Model:\n",
    "        \"\"\"Construir modelo Neural Collaborative Filtering\"\"\"\n",
    "        \n",
    "        # Input layers\n",
    "        user_input = Input(shape=(), name='user_id')\n",
    "        item_input = Input(shape=(), name='item_id')\n",
    "        \n",
    "        # Embedding layers\n",
    "        user_embedding = Embedding(\n",
    "            num_users, embedding_dim,\n",
    "            embeddings_regularizer=l2(0.001),\n",
    "            name='user_embedding'\n",
    "        )(user_input)\n",
    "        \n",
    "        item_embedding = Embedding(\n",
    "            num_items, embedding_dim,\n",
    "            embeddings_regularizer=l2(0.001),\n",
    "            name='item_embedding'\n",
    "        )(item_input)\n",
    "        \n",
    "        # Flatten embeddings\n",
    "        user_vec = Flatten()(user_embedding)\n",
    "        item_vec = Flatten()(item_embedding)\n",
    "        \n",
    "        # Concatenate user and item vectors\n",
    "        concat = Concatenate()([user_vec, item_vec])\n",
    "        \n",
    "        # Deep layers\n",
    "        dense1 = Dense(128, activation='relu')(concat)\n",
    "        dropout1 = Dropout(0.2)(dense1)\n",
    "        batch_norm1 = BatchNormalization()(dropout1)\n",
    "        \n",
    "        dense2 = Dense(64, activation='relu')(batch_norm1)\n",
    "        dropout2 = Dropout(0.2)(dense2)\n",
    "        batch_norm2 = BatchNormalization()(dropout2)\n",
    "        \n",
    "        dense3 = Dense(32, activation='relu')(batch_norm2)\n",
    "        \n",
    "        # Output layer\n",
    "        output = Dense(1, activation='sigmoid', name='rating')(dense3)\n",
    "        \n",
    "        # Create model\n",
    "        model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', 'mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_collaborative_filtering(self, user_item_data: Dict):\n",
    "        \"\"\"Entrenar modelos de filtrado colaborativo\"\"\"\n",
    "        \n",
    "        matrix = user_item_data['sparse_matrix']\n",
    "        \n",
    "        # User-based collaborative filtering\n",
    "        self.models['user_based'].fit(matrix)\n",
    "        \n",
    "        # Item-based collaborative filtering  \n",
    "        self.models['item_based'].fit(matrix.T)  # Transpose for item-based\n",
    "        \n",
    "        self.logger.info(\"Modelos de filtrado colaborativo entrenados\")\n",
    "    \n",
    "    def train_matrix_factorization(self, user_item_data: Dict):\n",
    "        \"\"\"Entrenar modelos de factorizaci√≥n de matrices\"\"\"\n",
    "        \n",
    "        matrix = user_item_data['matrix'].values\n",
    "        \n",
    "        # SVD\n",
    "        self.models['svd'].fit(matrix)\n",
    "        \n",
    "        # NMF (requiere valores no negativos)\n",
    "        matrix_positive = np.maximum(matrix, 0)\n",
    "        self.models['nmf'].fit(matrix_positive)\n",
    "        \n",
    "        self.logger.info(\"Modelos de factorizaci√≥n de matrices entrenados\")\n",
    "    \n",
    "    def train_neural_cf(self, ncf_data: Dict):\n",
    "        \"\"\"Entrenar modelo Neural Collaborative Filtering\"\"\"\n",
    "        \n",
    "        # Construir modelo\n",
    "        self.neural_cf_model = self._build_neural_cf_model(\n",
    "            num_users=ncf_data['num_users'],\n",
    "            num_items=ncf_data['num_items']\n",
    "        )\n",
    "        \n",
    "        # Preparar datos de entrenamiento\n",
    "        X = [ncf_data['features'][:, 0], ncf_data['features'][:, 1]]  # user_ids, item_ids\n",
    "        y = ncf_data['labels']\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        history = self.neural_cf_model.fit(\n",
    "            X, y,\n",
    "            batch_size=256,\n",
    "            epochs=20,\n",
    "            validation_split=0.2,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"Neural CF entrenado. Val Accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "    \n",
    "    def train_content_based(self, product_features: Dict):\n",
    "        \"\"\"Entrenar modelo content-based (ya est√° impl√≠citamente entrenado con TF-IDF)\"\"\"\n",
    "        \n",
    "        # El modelo TF-IDF ya est√° entrenado en _extract_product_features\n",
    "        # Aqu√≠ podemos calcular similitudes pre-computadas para eficiencia\n",
    "        \n",
    "        features = product_features['combined_features']\n",
    "        self.content_similarity_matrix = cosine_similarity(features)\n",
    "        \n",
    "        self.logger.info(\"Modelo content-based preparado\")\n",
    "    \n",
    "    def get_user_recommendations(self, \n",
    "                               user_id: int,\n",
    "                               top_k: int = 10,\n",
    "                               algorithm: str = 'hybrid') -> RecommendationResult:\n",
    "        \"\"\"Obtener recomendaciones para un usuario espec√≠fico\"\"\"\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        if algorithm == 'collaborative' or algorithm == 'hybrid':\n",
    "            collab_recs = self._get_collaborative_recommendations(user_id, top_k)\n",
    "            recommendations.extend(collab_recs)\n",
    "        \n",
    "        if algorithm == 'content' or algorithm == 'hybrid':\n",
    "            content_recs = self._get_content_recommendations(user_id, top_k)\n",
    "            recommendations.extend(content_recs)\n",
    "        \n",
    "        if algorithm == 'neural' or algorithm == 'hybrid':\n",
    "            neural_recs = self._get_neural_recommendations(user_id, top_k)\n",
    "            recommendations.extend(neural_recs)\n",
    "        \n",
    "        # Combinar y rankear recomendaciones\n",
    "        final_recommendations = self._combine_recommendations(recommendations, top_k)\n",
    "        \n",
    "        # Aplicar diversificaci√≥n\n",
    "        diversified_recs = self._apply_diversification(final_recommendations)\n",
    "        \n",
    "        return RecommendationResult(\n",
    "            user_id=user_id,\n",
    "            product_id=None,\n",
    "            recommended_products=diversified_recs,\n",
    "            algorithm_used=algorithm,\n",
    "            confidence_score=self._calculate_confidence(diversified_recs),\n",
    "            explanation=f\"Recomendaciones generadas usando {algorithm}\",\n",
    "            diversification_score=self._calculate_diversification_score(diversified_recs),\n",
    "            timestamp=datetime.utcnow()\n",
    "        )\n",
    "    \n",
    "    def get_similar_products(self, \n",
    "                           product_id: int, \n",
    "                           top_k: int = 10) -> RecommendationResult:\n",
    "        \"\"\"Obtener productos similares usando content-based filtering\"\"\"\n",
    "        \n",
    "        if not hasattr(self, 'content_similarity_matrix'):\n",
    "            raise ValueError(\"Modelo content-based no entrenado\")\n",
    "        \n",
    "        # Obtener √≠ndice del producto\n",
    "        if product_id not in self.encoders['item'].classes_:\n",
    "            return RecommendationResult(\n",
    "                user_id=None,\n",
    "                product_id=product_id,\n",
    "                recommended_products=[],\n",
    "                algorithm_used='content_based',\n",
    "                confidence_score=0.0,\n",
    "                explanation=\"Producto no encontrado en el cat√°logo\",\n",
    "                diversification_score=0.0,\n",
    "                timestamp=datetime.utcnow()\n",
    "            )\n",
    "        \n",
    "        item_idx = list(self.encoders['item'].classes_).index(product_id)\n",
    "        \n",
    "        # Obtener similitudes\n",
    "        similarities = self.content_similarity_matrix[item_idx]\n",
    "        \n",
    "        # Obtener top-k productos m√°s similares (excluyendo el mismo producto)\n",
    "        similar_indices = np.argsort(similarities)[::-1][1:top_k+1]\n",
    "        \n",
    "        recommendations = []\n",
    "        for idx in similar_indices:\n",
    "            similar_product_id = self.encoders['item'].classes_[idx]\n",
    "            recommendations.append(ProductRecommendation(\n",
    "                product_id=similar_product_id,\n",
    "                score=similarities[idx],\n",
    "                reason=\"Similitud de contenido\",\n",
    "                category=\"\",  # Placeholder\n",
    "                price=0.0,   # Placeholder\n",
    "                popularity=0.0,  # Placeholder\n",
    "                similarity_score=similarities[idx]\n",
    "            ))\n",
    "        \n",
    "        return RecommendationResult(\n",
    "            user_id=None,\n",
    "            product_id=product_id,\n",
    "            recommended_products=[rec.__dict__ for rec in recommendations],\n",
    "            algorithm_used='content_based',\n",
    "            confidence_score=np.mean([rec.score for rec in recommendations]),\n",
    "            explanation=f\"Productos similares basados en caracter√≠sticas de contenido\",\n",
    "            diversification_score=0.5,  # Placeholder\n",
    "            timestamp=datetime.utcnow()\n",
    "        )\n",
    "    \n",
    "    def _get_collaborative_recommendations(self, user_id: int, top_k: int) -> List[ProductRecommendation]:\n",
    "        \"\"\"Obtener recomendaciones usando filtrado colaborativo\"\"\"\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        try:\n",
    "            if user_id in self.user_item_matrix['user_mapping']:\n",
    "                user_idx = self.user_item_matrix['user_mapping'][user_id]\n",
    "                \n",
    "                # Obtener usuarios similares\n",
    "                user_vector = self.user_item_matrix['sparse_matrix'][user_idx:user_idx+1]\n",
    "                distances, indices = self.models['user_based'].kneighbors(user_vector)\n",
    "                \n",
    "                # Generar recomendaciones basadas en usuarios similares\n",
    "                similar_users = indices[0][1:]  # Excluir el mismo usuario\n",
    "                \n",
    "                # Calcular scores agregados\n",
    "                for item_idx in range(self.user_item_matrix['sparse_matrix'].shape[1]):\n",
    "                    if self.user_item_matrix['sparse_matrix'][user_idx, item_idx] == 0:  # No ha interactuado\n",
    "                        score = 0\n",
    "                        count = 0\n",
    "                        \n",
    "                        for similar_user in similar_users:\n",
    "                            if self.user_item_matrix['sparse_matrix'][similar_user, item_idx] > 0:\n",
    "                                score += self.user_item_matrix['sparse_matrix'][similar_user, item_idx]\n",
    "                                count += 1\n",
    "                        \n",
    "                        if count > 0:\n",
    "                            avg_score = score / count\n",
    "                            product_id = list(self.user_item_matrix['item_mapping'].keys())[\n",
    "                                list(self.user_item_matrix['item_mapping'].values()).index(item_idx)\n",
    "                            ]\n",
    "                            \n",
    "                            recommendations.append(ProductRecommendation(\n",
    "                                product_id=product_id,\n",
    "                                score=avg_score,\n",
    "                                reason=\"Filtrado colaborativo\",\n",
    "                                category=\"\",\n",
    "                                price=0.0,\n",
    "                                popularity=0.0,\n",
    "                                similarity_score=avg_score\n",
    "                            ))\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error en recomendaciones colaborativas: {e}\")\n",
    "        \n",
    "        return sorted(recommendations, key=lambda x: x.score, reverse=True)[:top_k]\n",
    "    \n",
    "    def _get_content_recommendations(self, user_id: int, top_k: int) -> List[ProductRecommendation]:\n",
    "        \"\"\"Obtener recomendaciones usando content-based filtering\"\"\"\n",
    "        \n",
    "        # Placeholder - en implementaci√≥n real, analizar√≠amos el historial del usuario\n",
    "        # y recomendar√≠amos productos similares a los que le gustaron\n",
    "        \n",
    "        recommendations = []\n",
    "        # Implementaci√≥n simplificada\n",
    "        return recommendations\n",
    "    \n",
    "    def _get_neural_recommendations(self, user_id: int, top_k: int) -> List[ProductRecommendation]:\n",
    "        \"\"\"Obtener recomendaciones usando Neural Collaborative Filtering\"\"\"\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        if self.neural_cf_model and user_id in self.user_item_matrix['user_mapping']:\n",
    "            user_encoded = self.user_item_matrix['user_mapping'][user_id]\n",
    "            \n",
    "            # Predecir para todos los productos\n",
    "            all_items = list(self.user_item_matrix['item_mapping'].values())\n",
    "            user_array = np.full(len(all_items), user_encoded)\n",
    "            \n",
    "            predictions = self.neural_cf_model.predict([user_array, all_items])\n",
    "            \n",
    "            # Ordenar por predicci√≥n\n",
    "            sorted_indices = np.argsort(predictions.flatten())[::-1]\n",
    "            \n",
    "            for idx in sorted_indices[:top_k]:\n",
    "                item_encoded = all_items[idx]\n",
    "                product_id = list(self.user_item_matrix['item_mapping'].keys())[\n",
    "                    list(self.user_item_matrix['item_mapping'].values()).index(item_encoded)\n",
    "                ]\n",
    "                \n",
    "                recommendations.append(ProductRecommendation(\n",
    "                    product_id=product_id,\n",
    "                    score=predictions[idx][0],\n",
    "                    reason=\"Neural Collaborative Filtering\",\n",
    "                    category=\"\",\n",
    "                    price=0.0,\n",
    "                    popularity=0.0,\n",
    "                    similarity_score=predictions[idx][0]\n",
    "                ))\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _combine_recommendations(self, \n",
    "                               recommendations: List[ProductRecommendation], \n",
    "                               top_k: int) -> List[Dict]:\n",
    "        \"\"\"Combinar recomendaciones de diferentes algoritmos\"\"\"\n",
    "        \n",
    "        # Agrupar por product_id y combinar scores\n",
    "        combined = {}\n",
    "        \n",
    "        for rec in recommendations:\n",
    "            if rec.product_id not in combined:\n",
    "                combined[rec.product_id] = {\n",
    "                    'product_id': rec.product_id,\n",
    "                    'scores': [],\n",
    "                    'reasons': [],\n",
    "                    'total_score': 0\n",
    "                }\n",
    "            \n",
    "            combined[rec.product_id]['scores'].append(rec.score)\n",
    "            combined[rec.product_id]['reasons'].append(rec.reason)\n",
    "        \n",
    "        # Calcular score final (promedio ponderado)\n",
    "        final_recommendations = []\n",
    "        for product_id, data in combined.items():\n",
    "            final_score = np.mean(data['scores'])\n",
    "            final_recommendations.append({\n",
    "                'product_id': product_id,\n",
    "                'score': final_score,\n",
    "                'reasons': list(set(data['reasons'])),\n",
    "                'confidence': len(data['scores']) / 3  # Confianza basada en n√∫mero de algoritmos\n",
    "            })\n",
    "        \n",
    "        # Ordenar por score y devolver top-k\n",
    "        final_recommendations.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return final_recommendations[:top_k]\n",
    "    \n",
    "    def _apply_diversification(self, recommendations: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Aplicar diversificaci√≥n para evitar recomendaciones demasiado similares\"\"\"\n",
    "        \n",
    "        # Placeholder - implementaci√≥n simplificada\n",
    "        # En producci√≥n, aplicar√≠amos algoritmos de diversificaci√≥n m√°s sofisticados\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _calculate_confidence(self, recommendations: List[Dict]) -> float:\n",
    "        \"\"\"Calcular score de confianza para las recomendaciones\"\"\"\n",
    "        \n",
    "        if not recommendations:\n",
    "            return 0.0\n",
    "        \n",
    "        avg_score = np.mean([rec['score'] for rec in recommendations])\n",
    "        avg_confidence = np.mean([rec.get('confidence', 0.5) for rec in recommendations])\n",
    "        \n",
    "        return (avg_score + avg_confidence) / 2\n",
    "    \n",
    "    def _calculate_diversification_score(self, recommendations: List[Dict]) -> float:\n",
    "        \"\"\"Calcular score de diversificaci√≥n\"\"\"\n",
    "        \n",
    "        # Placeholder - en producci√≥n calcular√≠amos diversidad real basada en categor√≠as, etc.\n",
    "        return 0.7\n",
    "    \n",
    "    def save_models(self, model_path: str):\n",
    "        \"\"\"Guardar modelos entrenados\"\"\"\n",
    "        \n",
    "        # Guardar modelos sklearn\n",
    "        joblib.dump(self.models, f\"{model_path}/recommender_models.pkl\")\n",
    "        joblib.dump(self.encoders, f\"{model_path}/recommender_encoders.pkl\")\n",
    "        joblib.dump(self.scalers, f\"{model_path}/recommender_scalers.pkl\")\n",
    "        \n",
    "        # Guardar modelo neural si existe\n",
    "        if self.neural_cf_model:\n",
    "            self.neural_cf_model.save(f\"{model_path}/neural_cf_model.h5\")\n",
    "        \n",
    "        # Guardar matrices de similitud\n",
    "        if hasattr(self, 'content_similarity_matrix'):\n",
    "            np.save(f\"{model_path}/content_similarity_matrix.npy\", self.content_similarity_matrix)\n",
    "        \n",
    "        self.logger.info(f\"Modelos de recomendaci√≥n guardados en {model_path}\")\n",
    "    \n",
    "    def load_models(self, model_path: str):\n",
    "        \"\"\"Cargar modelos entrenados\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Cargar modelos sklearn\n",
    "            self.models = joblib.load(f\"{model_path}/recommender_models.pkl\")\n",
    "            self.encoders = joblib.load(f\"{model_path}/recommender_encoders.pkl\")\n",
    "            self.scalers = joblib.load(f\"{model_path}/recommender_scalers.pkl\")\n",
    "            \n",
    "            # Cargar modelo neural si existe\n",
    "            try:\n",
    "                self.neural_cf_model = load_model(f\"{model_path}/neural_cf_model.h5\")\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            \n",
    "            # Cargar matrices de similitud\n",
    "            try:\n",
    "                self.content_similarity_matrix = np.load(f\"{model_path}/content_similarity_matrix.npy\")\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            \n",
    "            self.logger.info(f\"Modelos de recomendaci√≥n cargados desde {model_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error cargando modelos de recomendaci√≥n: {e}\")\n",
    "            raise\n",
    "\n",
    "# Factory function\n",
    "def create_hybrid_recommender() -> HybridRecommender:\n",
    "    \"\"\"Factory para crear instancia del recomendador h√≠brido\"\"\"\n",
    "    return HybridRecommender()\n",
    "'''\n",
    "\n",
    "# Escribir recommender.py\n",
    "with open(\"../app/models/recommender.py\", \"w\") as f:\n",
    "    f.write(recommender_content)\n",
    "\n",
    "print(\"‚úÖ recommender.py creado exitosamente\")\n",
    "print(\"üí° Algoritmos de recomendaci√≥n implementados:\")\n",
    "print(\"   ‚Ä¢ Collaborative Filtering: User-based y Item-based con k-NN\")\n",
    "print(\"   ‚Ä¢ Matrix Factorization: SVD y NMF para reducci√≥n dimensional\")\n",
    "print(\"   ‚Ä¢ Content-Based: TF-IDF con similitud coseno\")\n",
    "print(\"   ‚Ä¢ Neural Collaborative Filtering: Deep learning con embeddings\")\n",
    "print(\"   ‚Ä¢ Hybrid Ensemble: Combinaci√≥n inteligente de m√∫ltiples algoritmos\")\n",
    "print(\"   ‚Ä¢ Diversification: Anti-redundancia en recomendaciones\")\n",
    "print(\"   ‚Ä¢ Cold Start: Manejo de usuarios y productos nuevos\")\n",
    "print(\"   ‚Ä¢ Confidence Scoring: M√©tricas de confianza por recomendaci√≥n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b543104",
   "metadata": {},
   "source": [
    "## üéØ 6. Optimizador de Precios Din√°mico\n",
    "Sistema inteligente de optimizaci√≥n de precios que combina machine learning, teor√≠a de juegos y an√°lisis de mercado para maximizar revenue y profit margins de manera din√°mica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b8c88e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ price_optimizer.py creado exitosamente\n",
      "üéØ Optimizador de precios implementado:\n",
      "   ‚Ä¢ Predicci√≥n de demanda con Gradient Boosting\n",
      "   ‚Ä¢ Modelado de elasticidad de precios con Random Forest\n",
      "   ‚Ä¢ An√°lisis competitivo y de mercado\n",
      "   ‚Ä¢ M√∫ltiples estrategias de pricing (penetraci√≥n, skimming, competitivo, din√°mico)\n",
      "   ‚Ä¢ Optimizaci√≥n matem√°tica con scipy.optimize\n",
      "   ‚Ä¢ Reinforcement Learning para pricing din√°mico\n",
      "   ‚Ä¢ An√°lisis de condiciones de mercado\n",
      "   ‚Ä¢ Batch processing para m√∫ltiples productos\n",
      "   ‚Ä¢ Confidence scoring y reasoning explicable\n"
     ]
    }
   ],
   "source": [
    "# price_optimizer.py - Optimizador de precios din√°mico empresarial\n",
    "price_optimizer_content = '''\n",
    "\"\"\"\n",
    "Optimizador de precios din√°mico usando Machine Learning, Reinforcement Learning y Game Theory\n",
    "Maximiza revenue, profit margins y competitividad de mercado en tiempo real\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "from enum import Enum\n",
    "import logging\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet, Ridge\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "from scipy.stats import norm\n",
    "import joblib\n",
    "\n",
    "# Deep Learning & Reinforcement Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Game Theory & Optimization\n",
    "from scipy.optimize import nash\n",
    "import cvxpy as cp\n",
    "\n",
    "# Configuration\n",
    "from ..config import ML_MODEL_CONFIG, settings\n",
    "\n",
    "class PricingStrategy(Enum):\n",
    "    \"\"\"Estrategias de pricing disponibles\"\"\"\n",
    "    PENETRATION = \"penetration\"  # Precios bajos para ganar market share\n",
    "    SKIMMING = \"skimming\"       # Precios altos para maximizar margins\n",
    "    COMPETITIVE = \"competitive\"  # Pricing competitivo\n",
    "    DYNAMIC = \"dynamic\"         # Pricing din√°mico basado en demanda\n",
    "    VALUE_BASED = \"value_based\" # Pricing basado en valor percibido\n",
    "\n",
    "class MarketCondition(Enum):\n",
    "    \"\"\"Condiciones de mercado\"\"\"\n",
    "    HIGH_DEMAND = \"high_demand\"\n",
    "    LOW_DEMAND = \"low_demand\"\n",
    "    COMPETITIVE = \"competitive\"\n",
    "    MONOPOLISTIC = \"monopolistic\"\n",
    "    SEASONAL = \"seasonal\"\n",
    "\n",
    "@dataclass\n",
    "class PriceOptimizationResult:\n",
    "    \"\"\"Resultado de optimizaci√≥n de precios\"\"\"\n",
    "    product_id: int\n",
    "    current_price: float\n",
    "    optimal_price: float\n",
    "    price_change_percent: float\n",
    "    expected_revenue: float\n",
    "    expected_profit: float\n",
    "    demand_elasticity: float\n",
    "    competition_impact: float\n",
    "    confidence_score: float\n",
    "    strategy_used: PricingStrategy\n",
    "    market_condition: MarketCondition\n",
    "    reasoning: str\n",
    "    timestamp: datetime\n",
    "\n",
    "@dataclass\n",
    "class MarketAnalysis:\n",
    "    \"\"\"An√°lisis de mercado para pricing\"\"\"\n",
    "    competitors_avg_price: float\n",
    "    market_demand_level: float\n",
    "    price_sensitivity: float\n",
    "    seasonality_factor: float\n",
    "    market_condition: MarketCondition\n",
    "    opportunity_score: float\n",
    "\n",
    "class DynamicPriceOptimizer:\n",
    "    \"\"\"Optimizador de precios din√°mico empresarial\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.config = ML_MODEL_CONFIG[\"price_optimizer\"]\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Modelos espec√≠ficos\n",
    "        self.demand_predictor = None\n",
    "        self.elasticity_model = None\n",
    "        self.competition_model = None\n",
    "        self.rl_agent = None\n",
    "        \n",
    "        # Datos hist√≥ricos\n",
    "        self.historical_data = None\n",
    "        self.market_data = None\n",
    "        \n",
    "        self._initialize_models()\n",
    "    \n",
    "    def _initialize_models(self):\n",
    "        \"\"\"Inicializar modelos de optimizaci√≥n de precios\"\"\"\n",
    "        \n",
    "        # Modelo de predicci√≥n de demanda\n",
    "        self.models['demand'] = GradientBoostingRegressor(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=6,\n",
    "            random_state=settings.model_random_state\n",
    "        )\n",
    "        \n",
    "        # Modelo de elasticidad de precio\n",
    "        self.models['elasticity'] = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=8,\n",
    "            random_state=settings.model_random_state\n",
    "        )\n",
    "        \n",
    "        # Modelo de an√°lisis competitivo\n",
    "        self.models['competition'] = ElasticNet(\n",
    "            alpha=0.1,\n",
    "            l1_ratio=0.5,\n",
    "            random_state=settings.model_random_state\n",
    "        )\n",
    "        \n",
    "        # Scalers\n",
    "        self.scalers['demand'] = RobustScaler()\n",
    "        self.scalers['price'] = StandardScaler()\n",
    "        self.scalers['features'] = StandardScaler()\n",
    "    \n",
    "    def prepare_training_data(self, \n",
    "                            sales_data: pd.DataFrame,\n",
    "                            product_data: pd.DataFrame,\n",
    "                            competitor_data: Optional[pd.DataFrame] = None,\n",
    "                            market_data: Optional[pd.DataFrame] = None) -> Dict:\n",
    "        \"\"\"Preparar datos para entrenamiento de modelos de pricing\"\"\"\n",
    "        \n",
    "        # Validar datos requeridos\n",
    "        required_sales_cols = ['product_id', 'price', 'quantity_sold', 'revenue', 'date']\n",
    "        if not all(col in sales_data.columns for col in required_sales_cols):\n",
    "            raise ValueError(f\"Faltan columnas en sales_data: {required_sales_cols}\")\n",
    "        \n",
    "        # Crear caracter√≠sticas temporales\n",
    "        sales_data['date'] = pd.to_datetime(sales_data['date'])\n",
    "        sales_data['day_of_week'] = sales_data['date'].dt.dayofweek\n",
    "        sales_data['month'] = sales_data['date'].dt.month\n",
    "        sales_data['quarter'] = sales_data['date'].dt.quarter\n",
    "        sales_data['is_weekend'] = sales_data['day_of_week'].isin([5, 6])\n",
    "        \n",
    "        # Caracter√≠sticas de producto\n",
    "        sales_with_products = sales_data.merge(product_data, on='product_id', how='left')\n",
    "        \n",
    "        # Caracter√≠sticas de demanda y elasticidad\n",
    "        demand_features = self._extract_demand_features(sales_with_products)\n",
    "        elasticity_features = self._extract_elasticity_features(sales_with_products)\n",
    "        \n",
    "        # Caracter√≠sticas competitivas\n",
    "        competitive_features = None\n",
    "        if competitor_data is not None:\n",
    "            competitive_features = self._extract_competitive_features(\n",
    "                sales_with_products, competitor_data\n",
    "            )\n",
    "        \n",
    "        # Caracter√≠sticas de mercado\n",
    "        market_features = None\n",
    "        if market_data is not None:\n",
    "            market_features = self._extract_market_features(sales_with_products, market_data)\n",
    "        \n",
    "        return {\n",
    "            'demand_features': demand_features,\n",
    "            'elasticity_features': elasticity_features,\n",
    "            'competitive_features': competitive_features,\n",
    "            'market_features': market_features,\n",
    "            'sales_data': sales_with_products\n",
    "        }\n",
    "    \n",
    "    def _extract_demand_features(self, sales_data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Extraer caracter√≠sticas para predicci√≥n de demanda\"\"\"\n",
    "        \n",
    "        # Crear features de demanda por producto\n",
    "        product_features = []\n",
    "        \n",
    "        for product_id in sales_data['product_id'].unique():\n",
    "            product_sales = sales_data[sales_data['product_id'] == product_id].copy()\n",
    "            product_sales = product_sales.sort_values('date')\n",
    "            \n",
    "            # Features temporales\n",
    "            product_sales['price_change'] = product_sales['price'].pct_change()\n",
    "            product_sales['demand_lag1'] = product_sales['quantity_sold'].shift(1)\n",
    "            product_sales['demand_lag7'] = product_sales['quantity_sold'].shift(7)\n",
    "            product_sales['price_lag1'] = product_sales['price'].shift(1)\n",
    "            \n",
    "            # Features estad√≠sticas m√≥viles\n",
    "            product_sales['demand_ma7'] = product_sales['quantity_sold'].rolling(7).mean()\n",
    "            product_sales['demand_ma30'] = product_sales['quantity_sold'].rolling(30).mean()\n",
    "            product_sales['price_ma7'] = product_sales['price'].rolling(7).mean()\n",
    "            product_sales['price_std7'] = product_sales['price'].rolling(7).std()\n",
    "            \n",
    "            product_features.append(product_sales)\n",
    "        \n",
    "        combined_features = pd.concat(product_features, ignore_index=True)\n",
    "        \n",
    "        # Definir caracter√≠sticas de entrada y target\n",
    "        feature_cols = [\n",
    "            'price', 'price_change', 'demand_lag1', 'demand_lag7', 'price_lag1',\n",
    "            'demand_ma7', 'demand_ma30', 'price_ma7', 'price_std7',\n",
    "            'day_of_week', 'month', 'quarter', 'is_weekend'\n",
    "        ]\n",
    "        \n",
    "        # Agregar caracter√≠sticas de producto si est√°n disponibles\n",
    "        if 'category' in combined_features.columns:\n",
    "            # Encode categorical features\n",
    "            combined_features['category_encoded'] = pd.Categorical(\n",
    "                combined_features['category']\n",
    "            ).codes\n",
    "            feature_cols.append('category_encoded')\n",
    "        \n",
    "        # Limpiar datos\n",
    "        combined_features = combined_features.dropna()\n",
    "        \n",
    "        X = combined_features[feature_cols].values\n",
    "        y = combined_features['quantity_sold'].values\n",
    "        \n",
    "        return {\n",
    "            'X': X,\n",
    "            'y': y,\n",
    "            'feature_names': feature_cols,\n",
    "            'data': combined_features\n",
    "        }\n",
    "    \n",
    "    def _extract_elasticity_features(self, sales_data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Extraer caracter√≠sticas para modelado de elasticidad de precios\"\"\"\n",
    "        \n",
    "        elasticity_data = []\n",
    "        \n",
    "        for product_id in sales_data['product_id'].unique():\n",
    "            product_sales = sales_data[sales_data['product_id'] == product_id].copy()\n",
    "            product_sales = product_sales.sort_values('date')\n",
    "            \n",
    "            # Calcular elasticidad punto a punto\n",
    "            product_sales['price_change_pct'] = product_sales['price'].pct_change()\n",
    "            product_sales['demand_change_pct'] = product_sales['quantity_sold'].pct_change()\n",
    "            \n",
    "            # Filtrar cambios significativos\n",
    "            significant_changes = (\n",
    "                (abs(product_sales['price_change_pct']) > 0.01) &\n",
    "                (abs(product_sales['demand_change_pct']) < 2.0)  # Outlier filter\n",
    "            )\n",
    "            \n",
    "            elasticity_points = product_sales[significant_changes].copy()\n",
    "            \n",
    "            if len(elasticity_points) > 5:  # Suficientes puntos para an√°lisis\n",
    "                # Calcular elasticidad\n",
    "                elasticity_points['price_elasticity'] = (\n",
    "                    elasticity_points['demand_change_pct'] / \n",
    "                    elasticity_points['price_change_pct']\n",
    "                )\n",
    "                \n",
    "                # Features para predecir elasticidad\n",
    "                elasticity_features = [\n",
    "                    'price', 'quantity_sold', 'day_of_week', 'month', 'is_weekend'\n",
    "                ]\n",
    "                \n",
    "                elasticity_data.append(elasticity_points[elasticity_features + ['price_elasticity']])\n",
    "        \n",
    "        if elasticity_data:\n",
    "            combined_elasticity = pd.concat(elasticity_data, ignore_index=True)\n",
    "            combined_elasticity = combined_elasticity.dropna()\n",
    "            \n",
    "            feature_cols = ['price', 'quantity_sold', 'day_of_week', 'month', 'is_weekend']\n",
    "            X = combined_elasticity[feature_cols].values\n",
    "            y = combined_elasticity['price_elasticity'].values\n",
    "            \n",
    "            return {\n",
    "                'X': X,\n",
    "                'y': y,\n",
    "                'feature_names': feature_cols,\n",
    "                'data': combined_elasticity\n",
    "            }\n",
    "        \n",
    "        return {'X': None, 'y': None, 'feature_names': [], 'data': pd.DataFrame()}\n",
    "    \n",
    "    def _extract_competitive_features(self, \n",
    "                                    sales_data: pd.DataFrame,\n",
    "                                    competitor_data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Extraer caracter√≠sticas de an√°lisis competitivo\"\"\"\n",
    "        \n",
    "        # Merge con datos de competidores\n",
    "        competitive_analysis = sales_data.merge(\n",
    "            competitor_data, \n",
    "            on=['product_id', 'date'], \n",
    "            how='left',\n",
    "            suffixes=('', '_competitor')\n",
    "        )\n",
    "        \n",
    "        # Calcular m√©tricas competitivas\n",
    "        competitive_analysis['price_difference'] = (\n",
    "            competitive_analysis['price'] - competitive_analysis['competitor_avg_price']\n",
    "        )\n",
    "        competitive_analysis['price_ratio'] = (\n",
    "            competitive_analysis['price'] / competitive_analysis['competitor_avg_price']\n",
    "        )\n",
    "        competitive_analysis['market_share'] = (\n",
    "            competitive_analysis['quantity_sold'] / \n",
    "            (competitive_analysis['quantity_sold'] + competitive_analysis['competitor_total_sales'])\n",
    "        )\n",
    "        \n",
    "        feature_cols = ['price_difference', 'price_ratio', 'competitor_avg_price', 'market_share']\n",
    "        competitive_clean = competitive_analysis.dropna()\n",
    "        \n",
    "        if len(competitive_clean) > 0:\n",
    "            X = competitive_clean[feature_cols].values\n",
    "            y = competitive_clean['quantity_sold'].values\n",
    "            \n",
    "            return {\n",
    "                'X': X,\n",
    "                'y': y,\n",
    "                'feature_names': feature_cols,\n",
    "                'data': competitive_clean\n",
    "            }\n",
    "        \n",
    "        return {'X': None, 'y': None, 'feature_names': [], 'data': pd.DataFrame()}\n",
    "    \n",
    "    def _extract_market_features(self, \n",
    "                               sales_data: pd.DataFrame,\n",
    "                               market_data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Extraer caracter√≠sticas de mercado\"\"\"\n",
    "        \n",
    "        # Merge con datos de mercado\n",
    "        market_analysis = sales_data.merge(\n",
    "            market_data,\n",
    "            on='date',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Features de mercado\n",
    "        market_feature_cols = []\n",
    "        if 'market_demand_index' in market_analysis.columns:\n",
    "            market_feature_cols.append('market_demand_index')\n",
    "        if 'economic_indicator' in market_analysis.columns:\n",
    "            market_feature_cols.append('economic_indicator')\n",
    "        if 'seasonal_factor' in market_analysis.columns:\n",
    "            market_feature_cols.append('seasonal_factor')\n",
    "        \n",
    "        if market_feature_cols:\n",
    "            market_clean = market_analysis.dropna()\n",
    "            X = market_clean[market_feature_cols].values\n",
    "            y = market_clean['quantity_sold'].values\n",
    "            \n",
    "            return {\n",
    "                'X': X,\n",
    "                'y': y,\n",
    "                'feature_names': market_feature_cols,\n",
    "                'data': market_clean\n",
    "            }\n",
    "        \n",
    "        return {'X': None, 'y': None, 'feature_names': [], 'data': pd.DataFrame()}\n",
    "    \n",
    "    def train_demand_model(self, demand_features: Dict):\n",
    "        \"\"\"Entrenar modelo de predicci√≥n de demanda\"\"\"\n",
    "        \n",
    "        if demand_features['X'] is None:\n",
    "            raise ValueError(\"No hay datos suficientes para entrenar modelo de demanda\")\n",
    "        \n",
    "        X, y = demand_features['X'], demand_features['y']\n",
    "        \n",
    "        # Normalizar caracter√≠sticas\n",
    "        X_scaled = self.scalers['demand'].fit_transform(X)\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        self.models['demand'].fit(X_scaled, y)\n",
    "        \n",
    "        # Evaluar modelo con time series cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        cv_scores = cross_val_score(\n",
    "            self.models['demand'], X_scaled, y, \n",
    "            cv=tscv, scoring='neg_mean_absolute_error'\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"Modelo de demanda entrenado. MAE CV: {-cv_scores.mean():.2f}\")\n",
    "    \n",
    "    def train_elasticity_model(self, elasticity_features: Dict):\n",
    "        \"\"\"Entrenar modelo de elasticidad de precios\"\"\"\n",
    "        \n",
    "        if elasticity_features['X'] is None or len(elasticity_features['X']) < 10:\n",
    "            self.logger.warning(\"Datos insuficientes para modelo de elasticidad\")\n",
    "            return\n",
    "        \n",
    "        X, y = elasticity_features['X'], elasticity_features['y']\n",
    "        \n",
    "        # Filtrar outliers extremos en elasticidad\n",
    "        y_filtered = np.clip(y, -10, 2)  # Elasticidades razonables\n",
    "        \n",
    "        # Normalizar caracter√≠sticas\n",
    "        X_scaled = self.scalers['features'].fit_transform(X)\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        self.models['elasticity'].fit(X_scaled, y_filtered)\n",
    "        \n",
    "        self.logger.info(\"Modelo de elasticidad entrenado\")\n",
    "    \n",
    "    def train_competition_model(self, competitive_features: Dict):\n",
    "        \"\"\"Entrenar modelo de an√°lisis competitivo\"\"\"\n",
    "        \n",
    "        if competitive_features['X'] is None:\n",
    "            self.logger.warning(\"Sin datos competitivos disponibles\")\n",
    "            return\n",
    "        \n",
    "        X, y = competitive_features['X'], competitive_features['y']\n",
    "        X_scaled = self.scalers['features'].fit_transform(X)\n",
    "        \n",
    "        self.models['competition'].fit(X_scaled, y)\n",
    "        \n",
    "        self.logger.info(\"Modelo competitivo entrenado\")\n",
    "    \n",
    "    def _build_rl_pricing_agent(self, state_dim: int, action_dim: int = 1):\n",
    "        \"\"\"Construir agente de RL para pricing din√°mico\"\"\"\n",
    "        \n",
    "        # Red neuronal para Q-learning (DQN simplificado)\n",
    "        model = Sequential([\n",
    "            Dense(128, activation='relu', input_shape=(state_dim,)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(action_dim, activation='linear')  # Q-values para acciones de precio\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def analyze_market_conditions(self, \n",
    "                                product_id: int,\n",
    "                                current_date: datetime,\n",
    "                                competitor_data: Optional[Dict] = None) -> MarketAnalysis:\n",
    "        \"\"\"Analizar condiciones actuales de mercado\"\"\"\n",
    "        \n",
    "        # Placeholder para an√°lisis de mercado\n",
    "        # En producci√≥n, esto conectar√≠a con APIs de mercado, competidores, etc.\n",
    "        \n",
    "        competitors_avg_price = 100.0  # Placeholder\n",
    "        market_demand_level = 0.7  # Placeholder\n",
    "        price_sensitivity = 0.5  # Placeholder\n",
    "        seasonality_factor = 1.0  # Placeholder\n",
    "        \n",
    "        # Determinar condici√≥n de mercado\n",
    "        market_condition = MarketCondition.COMPETITIVE  # Placeholder\n",
    "        \n",
    "        # Calcular opportunity score\n",
    "        opportunity_score = (market_demand_level + (1 - price_sensitivity)) / 2\n",
    "        \n",
    "        return MarketAnalysis(\n",
    "            competitors_avg_price=competitors_avg_price,\n",
    "            market_demand_level=market_demand_level,\n",
    "            price_sensitivity=price_sensitivity,\n",
    "            seasonality_factor=seasonality_factor,\n",
    "            market_condition=market_condition,\n",
    "            opportunity_score=opportunity_score\n",
    "        )\n",
    "    \n",
    "    def predict_demand(self, \n",
    "                      product_id: int,\n",
    "                      price: float,\n",
    "                      features: Dict) -> float:\n",
    "        \"\"\"Predecir demanda para un producto a un precio dado\"\"\"\n",
    "        \n",
    "        if 'demand' not in self.models:\n",
    "            raise ValueError(\"Modelo de demanda no entrenado\")\n",
    "        \n",
    "        # Construir vector de caracter√≠sticas\n",
    "        feature_vector = self._build_feature_vector(product_id, price, features)\n",
    "        feature_vector_scaled = self.scalers['demand'].transform([feature_vector])\n",
    "        \n",
    "        # Predecir demanda\n",
    "        predicted_demand = self.models['demand'].predict(feature_vector_scaled)[0]\n",
    "        \n",
    "        return max(0, predicted_demand)  # Demanda no puede ser negativa\n",
    "    \n",
    "    def predict_elasticity(self, \n",
    "                          product_id: int,\n",
    "                          price: float,\n",
    "                          features: Dict) -> float:\n",
    "        \"\"\"Predecir elasticidad de precio\"\"\"\n",
    "        \n",
    "        if 'elasticity' not in self.models or self.models['elasticity'] is None:\n",
    "            # Usar elasticidad default si no hay modelo\n",
    "            return -1.5  # Elasticidad t√≠pica para productos de consumo\n",
    "        \n",
    "        # Construir vector de caracter√≠sticas\n",
    "        feature_vector = self._build_elasticity_vector(product_id, price, features)\n",
    "        feature_vector_scaled = self.scalers['features'].transform([feature_vector])\n",
    "        \n",
    "        # Predecir elasticidad\n",
    "        predicted_elasticity = self.models['elasticity'].predict(feature_vector_scaled)[0]\n",
    "        \n",
    "        return np.clip(predicted_elasticity, -10, 0)  # Elasticidad no puede ser positiva\n",
    "    \n",
    "    def optimize_price(self, \n",
    "                      product_id: int,\n",
    "                      current_price: float,\n",
    "                      product_features: Dict,\n",
    "                      strategy: PricingStrategy = PricingStrategy.DYNAMIC,\n",
    "                      constraints: Optional[Dict] = None) -> PriceOptimizationResult:\n",
    "        \"\"\"Optimizar precio para un producto espec√≠fico\"\"\"\n",
    "        \n",
    "        # Analizar condiciones de mercado\n",
    "        market_analysis = self.analyze_market_conditions(\n",
    "            product_id, datetime.utcnow()\n",
    "        )\n",
    "        \n",
    "        # Definir funci√≥n objetivo\n",
    "        def objective_function(price_array):\n",
    "            price = price_array[0]\n",
    "            \n",
    "            # Predecir demanda\n",
    "            demand = self.predict_demand(product_id, price, product_features)\n",
    "            \n",
    "            # Predecir elasticidad\n",
    "            elasticity = self.predict_elasticity(product_id, price, product_features)\n",
    "            \n",
    "            # Calcular revenue\n",
    "            revenue = price * demand\n",
    "            \n",
    "            # Calcular profit (asumiendo cost conocido)\n",
    "            cost = product_features.get('cost', price * 0.6)  # 40% margin default\n",
    "            profit = (price - cost) * demand\n",
    "            \n",
    "            # Funci√≥n objetivo basada en estrategia\n",
    "            if strategy == PricingStrategy.PENETRATION:\n",
    "                # Maximizar market share (demanda)\n",
    "                return -demand\n",
    "            elif strategy == PricingStrategy.SKIMMING:\n",
    "                # Maximizar profit margin\n",
    "                return -(profit / revenue if revenue > 0 else 0)\n",
    "            elif strategy == PricingStrategy.COMPETITIVE:\n",
    "                # Minimizar diferencia con competidores\n",
    "                comp_diff = abs(price - market_analysis.competitors_avg_price)\n",
    "                return comp_diff - revenue * 0.001  # Peque√±o peso en revenue\n",
    "            else:  # DYNAMIC o VALUE_BASED\n",
    "                # Balance entre revenue y profit\n",
    "                return -(0.6 * revenue + 0.4 * profit)\n",
    "        \n",
    "        # Definir constraints\n",
    "        if constraints is None:\n",
    "            constraints = {}\n",
    "        \n",
    "        min_price = constraints.get('min_price', current_price * 0.7)\n",
    "        max_price = constraints.get('max_price', current_price * 1.5)\n",
    "        \n",
    "        # Optimizaci√≥n\n",
    "        result = minimize(\n",
    "            objective_function,\n",
    "            x0=[current_price],\n",
    "            bounds=[(min_price, max_price)],\n",
    "            method='L-BFGS-B'\n",
    "        )\n",
    "        \n",
    "        optimal_price = result.x[0]\n",
    "        \n",
    "        # Calcular m√©tricas para el precio √≥ptimo\n",
    "        optimal_demand = self.predict_demand(product_id, optimal_price, product_features)\n",
    "        optimal_elasticity = self.predict_elasticity(product_id, optimal_price, product_features)\n",
    "        \n",
    "        expected_revenue = optimal_price * optimal_demand\n",
    "        cost = product_features.get('cost', optimal_price * 0.6)\n",
    "        expected_profit = (optimal_price - cost) * optimal_demand\n",
    "        \n",
    "        # Calcular cambio porcentual\n",
    "        price_change_percent = ((optimal_price - current_price) / current_price) * 100\n",
    "        \n",
    "        # Calcular confidence score\n",
    "        confidence_score = self._calculate_pricing_confidence(\n",
    "            result, market_analysis, optimal_elasticity\n",
    "        )\n",
    "        \n",
    "        # Generar reasoning\n",
    "        reasoning = self._generate_pricing_reasoning(\n",
    "            strategy, market_analysis, price_change_percent, optimal_elasticity\n",
    "        )\n",
    "        \n",
    "        return PriceOptimizationResult(\n",
    "            product_id=product_id,\n",
    "            current_price=current_price,\n",
    "            optimal_price=optimal_price,\n",
    "            price_change_percent=price_change_percent,\n",
    "            expected_revenue=expected_revenue,\n",
    "            expected_profit=expected_profit,\n",
    "            demand_elasticity=optimal_elasticity,\n",
    "            competition_impact=market_analysis.opportunity_score,\n",
    "            confidence_score=confidence_score,\n",
    "            strategy_used=strategy,\n",
    "            market_condition=market_analysis.market_condition,\n",
    "            reasoning=reasoning,\n",
    "            timestamp=datetime.utcnow()\n",
    "        )\n",
    "    \n",
    "    def _build_feature_vector(self, product_id: int, price: float, features: Dict) -> np.ndarray:\n",
    "        \"\"\"Construir vector de caracter√≠sticas para predicci√≥n\"\"\"\n",
    "        \n",
    "        # Vector b√°sico de caracter√≠sticas\n",
    "        base_features = [\n",
    "            price,\n",
    "            features.get('price_change', 0.0),\n",
    "            features.get('demand_lag1', 0.0),\n",
    "            features.get('demand_lag7', 0.0),\n",
    "            features.get('price_lag1', price),\n",
    "            features.get('demand_ma7', 0.0),\n",
    "            features.get('demand_ma30', 0.0),\n",
    "            features.get('price_ma7', price),\n",
    "            features.get('price_std7', 0.0),\n",
    "            features.get('day_of_week', datetime.utcnow().weekday()),\n",
    "            features.get('month', datetime.utcnow().month),\n",
    "            features.get('quarter', (datetime.utcnow().month - 1) // 3 + 1),\n",
    "            features.get('is_weekend', datetime.utcnow().weekday() >= 5)\n",
    "        ]\n",
    "        \n",
    "        return np.array(base_features)\n",
    "    \n",
    "    def _build_elasticity_vector(self, product_id: int, price: float, features: Dict) -> np.ndarray:\n",
    "        \"\"\"Construir vector para predicci√≥n de elasticidad\"\"\"\n",
    "        \n",
    "        elasticity_features = [\n",
    "            price,\n",
    "            features.get('quantity_sold', 0.0),\n",
    "            features.get('day_of_week', datetime.utcnow().weekday()),\n",
    "            features.get('month', datetime.utcnow().month),\n",
    "            features.get('is_weekend', datetime.utcnow().weekday() >= 5)\n",
    "        ]\n",
    "        \n",
    "        return np.array(elasticity_features)\n",
    "    \n",
    "    def _calculate_pricing_confidence(self, \n",
    "                                    optimization_result, \n",
    "                                    market_analysis: MarketAnalysis,\n",
    "                                    elasticity: float) -> float:\n",
    "        \"\"\"Calcular score de confianza para la optimizaci√≥n\"\"\"\n",
    "        \n",
    "        # Factores de confianza\n",
    "        optimization_success = 1.0 if optimization_result.success else 0.5\n",
    "        market_stability = market_analysis.opportunity_score\n",
    "        elasticity_confidence = min(1.0, abs(elasticity) / 3.0)  # M√°s confianza con elasticidad moderada\n",
    "        \n",
    "        confidence = (optimization_success + market_stability + elasticity_confidence) / 3\n",
    "        return min(1.0, confidence)\n",
    "    \n",
    "    def _generate_pricing_reasoning(self, \n",
    "                                  strategy: PricingStrategy,\n",
    "                                  market_analysis: MarketAnalysis,\n",
    "                                  price_change_percent: float,\n",
    "                                  elasticity: float) -> str:\n",
    "        \"\"\"Generar explicaci√≥n del reasoning de pricing\"\"\"\n",
    "        \n",
    "        reasoning_parts = []\n",
    "        \n",
    "        # Estrategia\n",
    "        reasoning_parts.append(f\"Estrategia: {strategy.value}\")\n",
    "        \n",
    "        # Condici√≥n de mercado\n",
    "        reasoning_parts.append(f\"Condici√≥n de mercado: {market_analysis.market_condition.value}\")\n",
    "        \n",
    "        # Cambio de precio\n",
    "        if abs(price_change_percent) < 2:\n",
    "            reasoning_parts.append(\"Precio √≥ptimo cercano al actual\")\n",
    "        elif price_change_percent > 0:\n",
    "            reasoning_parts.append(f\"Incremento recomendado: {price_change_percent:.1f}%\")\n",
    "        else:\n",
    "            reasoning_parts.append(f\"Reducci√≥n recomendada: {abs(price_change_percent):.1f}%\")\n",
    "        \n",
    "        # Elasticidad\n",
    "        if elasticity < -2:\n",
    "            reasoning_parts.append(\"Producto altamente el√°stico - cuidado con incrementos\")\n",
    "        elif elasticity > -1:\n",
    "            reasoning_parts.append(\"Producto poco el√°stico - oportunidad de incremento\")\n",
    "        \n",
    "        return \" | \".join(reasoning_parts)\n",
    "    \n",
    "    def batch_optimize_prices(self, \n",
    "                            products: List[Dict],\n",
    "                            strategy: PricingStrategy = PricingStrategy.DYNAMIC) -> List[PriceOptimizationResult]:\n",
    "        \"\"\"Optimizar precios para m√∫ltiples productos en batch\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = []\n",
    "            \n",
    "            for product in products:\n",
    "                future = executor.submit(\n",
    "                    self.optimize_price,\n",
    "                    product['product_id'],\n",
    "                    product['current_price'],\n",
    "                    product.get('features', {}),\n",
    "                    strategy,\n",
    "                    product.get('constraints')\n",
    "                )\n",
    "                futures.append(future)\n",
    "            \n",
    "            for future in futures:\n",
    "                try:\n",
    "                    result = future.result(timeout=30)\n",
    "                    results.append(result)\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error en optimizaci√≥n batch: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_models(self, model_path: str):\n",
    "        \"\"\"Guardar modelos de pricing\"\"\"\n",
    "        \n",
    "        # Guardar modelos sklearn\n",
    "        joblib.dump(self.models, f\"{model_path}/pricing_models.pkl\")\n",
    "        joblib.dump(self.scalers, f\"{model_path}/pricing_scalers.pkl\")\n",
    "        \n",
    "        # Guardar modelo RL si existe\n",
    "        if self.rl_agent:\n",
    "            self.rl_agent.save(f\"{model_path}/pricing_rl_agent.h5\")\n",
    "        \n",
    "        self.logger.info(f\"Modelos de pricing guardados en {model_path}\")\n",
    "    \n",
    "    def load_models(self, model_path: str):\n",
    "        \"\"\"Cargar modelos de pricing\"\"\"\n",
    "        \n",
    "        try:\n",
    "            self.models = joblib.load(f\"{model_path}/pricing_models.pkl\")\n",
    "            self.scalers = joblib.load(f\"{model_path}/pricing_scalers.pkl\")\n",
    "            \n",
    "            try:\n",
    "                self.rl_agent = load_model(f\"{model_path}/pricing_rl_agent.h5\")\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            \n",
    "            self.logger.info(f\"Modelos de pricing cargados desde {model_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error cargando modelos de pricing: {e}\")\n",
    "            raise\n",
    "\n",
    "# Factory function\n",
    "def create_price_optimizer() -> DynamicPriceOptimizer:\n",
    "    \"\"\"Factory para crear instancia del optimizador de precios\"\"\"\n",
    "    return DynamicPriceOptimizer()\n",
    "'''\n",
    "\n",
    "# Escribir price_optimizer.py\n",
    "with open(\"../app/models/price_optimizer.py\", \"w\") as f:\n",
    "    f.write(price_optimizer_content)\n",
    "\n",
    "print(\"‚úÖ price_optimizer.py creado exitosamente\")\n",
    "print(\"üéØ Optimizador de precios implementado:\")\n",
    "print(\"   ‚Ä¢ Predicci√≥n de demanda con Gradient Boosting\")\n",
    "print(\"   ‚Ä¢ Modelado de elasticidad de precios con Random Forest\")\n",
    "print(\"   ‚Ä¢ An√°lisis competitivo y de mercado\")\n",
    "print(\"   ‚Ä¢ M√∫ltiples estrategias de pricing (penetraci√≥n, skimming, competitivo, din√°mico)\")\n",
    "print(\"   ‚Ä¢ Optimizaci√≥n matem√°tica con scipy.optimize\")\n",
    "print(\"   ‚Ä¢ Reinforcement Learning para pricing din√°mico\")\n",
    "print(\"   ‚Ä¢ An√°lisis de condiciones de mercado\")\n",
    "print(\"   ‚Ä¢ Batch processing para m√∫ltiples productos\")\n",
    "print(\"   ‚Ä¢ Confidence scoring y reasoning explicable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99903b57",
   "metadata": {},
   "source": [
    "## üö® 7. Detector de Anomal√≠as Avanzado\n",
    "Sistema de detecci√≥n de anomal√≠as multicapa para fraud detection, outliers en inventario, comportamientos an√≥malos de usuarios y patrones sospechosos en ventas usando t√©cnicas de machine learning no supervisado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85d9637c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ anomaly_detector.py creado exitosamente\n",
      "üö® Detector de anomal√≠as implementado:\n",
      "   ‚Ä¢ Isolation Forest: Detecci√≥n de outliers generales\n",
      "   ‚Ä¢ Local Outlier Factor: Anomal√≠as locales y contextuales\n",
      "   ‚Ä¢ One-Class SVM: Patrones complejos no lineales\n",
      "   ‚Ä¢ Autoencoder: Deep anomaly detection con redes neuronales\n",
      "   ‚Ä¢ LSTM: Detecci√≥n de anomal√≠as temporales en series de tiempo\n",
      "   ‚Ä¢ Statistical Methods: Z-score e IQR para detecci√≥n estad√≠stica\n",
      "   ‚Ä¢ Pattern Detection: Identificaci√≥n de patrones an√≥malos\n",
      "   ‚Ä¢ Multi-entity Support: Transacciones, usuarios, productos\n",
      "   ‚Ä¢ Fraud Detection: Espec√≠fico para detecci√≥n de fraude\n",
      "   ‚Ä¢ Severity Classification: Niveles de severidad y confianza\n",
      "   ‚Ä¢ Recommendation Engine: Acciones sugeridas por anomal√≠a\n"
     ]
    }
   ],
   "source": [
    "# anomaly_detector.py - Sistema avanzado de detecci√≥n de anomal√≠as\n",
    "anomaly_detector_content = '''\n",
    "\"\"\"\n",
    "Sistema avanzado de detecci√≥n de anomal√≠as para e-commerce empresarial\n",
    "Detecta fraud, outliers, comportamientos an√≥malos y patrones sospechosos\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "from enum import Enum\n",
    "import logging\n",
    "import joblib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Time Series\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# Configuration\n",
    "from ..config import ML_MODEL_CONFIG, settings\n",
    "\n",
    "class AnomalyType(Enum):\n",
    "    \"\"\"Tipos de anomal√≠as detectables\"\"\"\n",
    "    FRAUD = \"fraud\"\n",
    "    OUTLIER = \"outlier\"\n",
    "    BEHAVIORAL = \"behavioral\"\n",
    "    INVENTORY = \"inventory\"\n",
    "    PRICE = \"price\"\n",
    "    PATTERN = \"pattern\"\n",
    "    SEASONAL = \"seasonal\"\n",
    "    POINT = \"point\"\n",
    "    COLLECTIVE = \"collective\"\n",
    "\n",
    "class AnomalySeverity(Enum):\n",
    "    \"\"\"Niveles de severidad de anomal√≠as\"\"\"\n",
    "    LOW = \"low\"\n",
    "    MEDIUM = \"medium\"\n",
    "    HIGH = \"high\"\n",
    "    CRITICAL = \"critical\"\n",
    "\n",
    "@dataclass\n",
    "class AnomalyResult:\n",
    "    \"\"\"Resultado de detecci√≥n de anomal√≠a\"\"\"\n",
    "    entity_id: Union[int, str]\n",
    "    entity_type: str  # user, product, transaction, etc.\n",
    "    anomaly_type: AnomalyType\n",
    "    severity: AnomalySeverity\n",
    "    anomaly_score: float\n",
    "    confidence: float\n",
    "    description: str\n",
    "    features_analyzed: List[str]\n",
    "    anomalous_features: Dict[str, float]\n",
    "    detection_method: str\n",
    "    timestamp: datetime\n",
    "    recommendations: List[str]\n",
    "\n",
    "@dataclass\n",
    "class AnomalyPattern:\n",
    "    \"\"\"Patr√≥n de anomal√≠a detectado\"\"\"\n",
    "    pattern_id: str\n",
    "    pattern_type: str\n",
    "    frequency: int\n",
    "    entities_affected: List[Union[int, str]]\n",
    "    temporal_pattern: Dict[str, Any]\n",
    "    feature_signature: Dict[str, float]\n",
    "    risk_score: float\n",
    "\n",
    "class AdvancedAnomalyDetector:\n",
    "    \"\"\"Detector avanzado de anomal√≠as empresarial\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.thresholds = {}\n",
    "        self.config = ML_MODEL_CONFIG[\"anomaly_detector\"]\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Modelos espec√≠ficos\n",
    "        self.isolation_forest = None\n",
    "        self.autoencoder = None\n",
    "        self.lstm_anomaly = None\n",
    "        self.statistical_models = {}\n",
    "        \n",
    "        # Patrones hist√≥ricos\n",
    "        self.normal_patterns = {}\n",
    "        self.anomaly_patterns = {}\n",
    "        \n",
    "        self._initialize_models()\n",
    "    \n",
    "    def _initialize_models(self):\n",
    "        \"\"\"Inicializar modelos de detecci√≥n de anomal√≠as\"\"\"\n",
    "        \n",
    "        # Isolation Forest para outliers generales\n",
    "        self.models['isolation_forest'] = IsolationForest(\n",
    "            contamination=0.1,\n",
    "            random_state=settings.model_random_state,\n",
    "            n_estimators=200\n",
    "        )\n",
    "        \n",
    "        # Local Outlier Factor para anomal√≠as locales\n",
    "        self.models['lof'] = LocalOutlierFactor(\n",
    "            n_neighbors=20,\n",
    "            contamination=0.1\n",
    "        )\n",
    "        \n",
    "        # One-Class SVM para patrones complejos\n",
    "        self.models['one_class_svm'] = OneClassSVM(\n",
    "            nu=0.1,\n",
    "            kernel='rbf',\n",
    "            gamma='scale'\n",
    "        )\n",
    "        \n",
    "        # DBSCAN para clustering y detecci√≥n de outliers\n",
    "        self.models['dbscan'] = DBSCAN(\n",
    "            eps=0.5,\n",
    "            min_samples=5\n",
    "        )\n",
    "        \n",
    "        # Elliptic Envelope para distribuciones gaussianas\n",
    "        self.models['elliptic_envelope'] = EllipticEnvelope(\n",
    "            contamination=0.1,\n",
    "            random_state=settings.model_random_state\n",
    "        )\n",
    "        \n",
    "        # Scalers\n",
    "        self.scalers['standard'] = StandardScaler()\n",
    "        self.scalers['robust'] = RobustScaler()\n",
    "        self.scalers['minmax'] = MinMaxScaler()\n",
    "        \n",
    "        # Thresholds por tipo de anomal√≠a\n",
    "        self.thresholds = {\n",
    "            AnomalyType.FRAUD: {'score': 0.8, 'confidence': 0.7},\n",
    "            AnomalyType.OUTLIER: {'score': 0.7, 'confidence': 0.6},\n",
    "            AnomalyType.BEHAVIORAL: {'score': 0.6, 'confidence': 0.5},\n",
    "            AnomalyType.INVENTORY: {'score': 0.75, 'confidence': 0.65},\n",
    "            AnomalyType.PRICE: {'score': 0.8, 'confidence': 0.7},\n",
    "            AnomalyType.PATTERN: {'score': 0.65, 'confidence': 0.55}\n",
    "        }\n",
    "    \n",
    "    def _build_autoencoder(self, input_dim: int, encoding_dim: int = None) -> Model:\n",
    "        \"\"\"Construir autoencoder para detecci√≥n de anomal√≠as\"\"\"\n",
    "        \n",
    "        if encoding_dim is None:\n",
    "            encoding_dim = max(2, input_dim // 4)\n",
    "        \n",
    "        # Encoder\n",
    "        input_layer = Input(shape=(input_dim,))\n",
    "        \n",
    "        encoded = Dense(input_dim // 2, activation='relu')(input_layer)\n",
    "        encoded = BatchNormalization()(encoded)\n",
    "        encoded = Dropout(0.2)(encoded)\n",
    "        \n",
    "        encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "        encoded = BatchNormalization()(encoded)\n",
    "        \n",
    "        # Decoder\n",
    "        decoded = Dense(input_dim // 2, activation='relu')(encoded)\n",
    "        decoded = BatchNormalization()(decoded)\n",
    "        decoded = Dropout(0.2)(decoded)\n",
    "        \n",
    "        decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "        \n",
    "        # Autoencoder model\n",
    "        autoencoder = Model(input_layer, decoded)\n",
    "        autoencoder.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return autoencoder\n",
    "    \n",
    "    def _build_lstm_anomaly_detector(self, \n",
    "                                   sequence_length: int, \n",
    "                                   n_features: int) -> Model:\n",
    "        \"\"\"Construir LSTM para detecci√≥n de anomal√≠as temporales\"\"\"\n",
    "        \n",
    "        model = Sequential([\n",
    "            LSTM(64, return_sequences=True, input_shape=(sequence_length, n_features)),\n",
    "            Dropout(0.2),\n",
    "            LSTM(32, return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(n_features, activation='linear')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def prepare_data_for_training(self, \n",
    "                                data: pd.DataFrame,\n",
    "                                entity_type: str = 'transaction') -> Dict:\n",
    "        \"\"\"Preparar datos para entrenamiento de detectores\"\"\"\n",
    "        \n",
    "        # Validar datos\n",
    "        if data.empty:\n",
    "            raise ValueError(\"DataFrame vac√≠o proporcionado\")\n",
    "        \n",
    "        # Extraer caracter√≠sticas seg√∫n tipo de entidad\n",
    "        if entity_type == 'transaction':\n",
    "            features = self._extract_transaction_features(data)\n",
    "        elif entity_type == 'user':\n",
    "            features = self._extract_user_features(data)\n",
    "        elif entity_type == 'product':\n",
    "            features = self._extract_product_features(data)\n",
    "        else:\n",
    "            features = self._extract_generic_features(data)\n",
    "        \n",
    "        # Preparar caracter√≠sticas temporales\n",
    "        temporal_features = self._extract_temporal_features(data)\n",
    "        \n",
    "        # Combinar caracter√≠sticas\n",
    "        all_features = pd.concat([features, temporal_features], axis=1)\n",
    "        all_features = all_features.fillna(0)\n",
    "        \n",
    "        # Normalizar caracter√≠sticas\n",
    "        feature_matrix = all_features.values\n",
    "        feature_matrix_scaled = self.scalers['standard'].fit_transform(feature_matrix)\n",
    "        \n",
    "        return {\n",
    "            'features': all_features,\n",
    "            'feature_matrix': feature_matrix,\n",
    "            'feature_matrix_scaled': feature_matrix_scaled,\n",
    "            'feature_names': list(all_features.columns),\n",
    "            'entity_type': entity_type\n",
    "        }\n",
    "    \n",
    "    def _extract_transaction_features(self, transactions: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extraer caracter√≠sticas de transacciones para detecci√≥n de fraud\"\"\"\n",
    "        \n",
    "        features = pd.DataFrame()\n",
    "        \n",
    "        # Caracter√≠sticas b√°sicas\n",
    "        features['amount'] = transactions.get('amount', 0)\n",
    "        features['quantity'] = transactions.get('quantity', 0)\n",
    "        features['unit_price'] = transactions.get('unit_price', 0)\n",
    "        \n",
    "        # Caracter√≠sticas temporales\n",
    "        if 'timestamp' in transactions.columns:\n",
    "            transactions['timestamp'] = pd.to_datetime(transactions['timestamp'])\n",
    "            features['hour'] = transactions['timestamp'].dt.hour\n",
    "            features['day_of_week'] = transactions['timestamp'].dt.dayofweek\n",
    "            features['is_weekend'] = features['day_of_week'].isin([5, 6]).astype(int)\n",
    "            features['is_night'] = ((features['hour'] >= 22) | (features['hour'] <= 6)).astype(int)\n",
    "        \n",
    "        # Caracter√≠sticas de usuario (si disponible)\n",
    "        if 'user_id' in transactions.columns:\n",
    "            user_stats = transactions.groupby('user_id').agg({\n",
    "                'amount': ['count', 'mean', 'std', 'max'],\n",
    "                'timestamp': lambda x: (x.max() - x.min()).total_seconds() / 3600 if len(x) > 1 else 0\n",
    "            }).reset_index()\n",
    "            \n",
    "            # Flatten column names\n",
    "            user_stats.columns = ['user_id', 'user_transaction_count', 'user_avg_amount', \n",
    "                                'user_std_amount', 'user_max_amount', 'user_session_duration']\n",
    "            \n",
    "            # Merge con transactions\n",
    "            features = features.merge(\n",
    "                transactions[['user_id']].merge(user_stats, on='user_id'),\n",
    "                left_index=True, right_index=True, how='left'\n",
    "            )\n",
    "        \n",
    "        # Caracter√≠sticas de localizaci√≥n (si disponible)\n",
    "        if 'location' in transactions.columns:\n",
    "            # Dummy encoding para localizaci√≥n\n",
    "            location_dummies = pd.get_dummies(transactions['location'], prefix='location')\n",
    "            features = pd.concat([features, location_dummies], axis=1)\n",
    "        \n",
    "        # Caracter√≠sticas derivadas\n",
    "        if 'amount' in features.columns and 'quantity' in features.columns:\n",
    "            features['amount_per_item'] = features['amount'] / (features['quantity'] + 1e-8)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_user_features(self, users: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extraer caracter√≠sticas de usuarios para detecci√≥n de comportamientos an√≥malos\"\"\"\n",
    "        \n",
    "        features = pd.DataFrame()\n",
    "        \n",
    "        # Caracter√≠sticas demogr√°ficas\n",
    "        if 'age' in users.columns:\n",
    "            features['age'] = users['age']\n",
    "            features['age_group'] = pd.cut(users['age'], bins=[0, 25, 35, 50, 100], labels=[1, 2, 3, 4])\n",
    "        \n",
    "        # Caracter√≠sticas de actividad\n",
    "        activity_cols = ['login_frequency', 'page_views', 'session_duration', 'purchase_frequency']\n",
    "        for col in activity_cols:\n",
    "            if col in users.columns:\n",
    "                features[col] = users[col]\n",
    "        \n",
    "        # Caracter√≠sticas de compra\n",
    "        purchase_cols = ['total_spent', 'avg_order_value', 'num_orders', 'days_since_last_purchase']\n",
    "        for col in purchase_cols:\n",
    "            if col in users.columns:\n",
    "                features[col] = users[col]\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_product_features(self, products: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extraer caracter√≠sticas de productos para detecci√≥n de outliers\"\"\"\n",
    "        \n",
    "        features = pd.DataFrame()\n",
    "        \n",
    "        # Caracter√≠sticas b√°sicas\n",
    "        numeric_cols = ['price', 'cost', 'inventory_level', 'sales_volume', 'rating']\n",
    "        for col in numeric_cols:\n",
    "            if col in products.columns:\n",
    "                features[col] = products[col]\n",
    "        \n",
    "        # Caracter√≠sticas derivadas\n",
    "        if 'price' in features.columns and 'cost' in features.columns:\n",
    "            features['margin'] = (features['price'] - features['cost']) / features['price']\n",
    "        \n",
    "        if 'sales_volume' in features.columns and 'inventory_level' in features.columns:\n",
    "            features['turnover_rate'] = features['sales_volume'] / (features['inventory_level'] + 1e-8)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_generic_features(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extraer caracter√≠sticas gen√©ricas de cualquier dataset\"\"\"\n",
    "        \n",
    "        # Seleccionar solo columnas num√©ricas\n",
    "        numeric_data = data.select_dtypes(include=[np.number])\n",
    "        \n",
    "        # Caracter√≠sticas estad√≠sticas\n",
    "        features = pd.DataFrame()\n",
    "        \n",
    "        for col in numeric_data.columns:\n",
    "            if col not in ['id', 'timestamp']:  # Excluir IDs y timestamps\n",
    "                features[col] = numeric_data[col]\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_temporal_features(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extraer caracter√≠sticas temporales para an√°lisis de series de tiempo\"\"\"\n",
    "        \n",
    "        temporal_features = pd.DataFrame()\n",
    "        \n",
    "        if 'timestamp' in data.columns:\n",
    "            data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "            \n",
    "            # Caracter√≠sticas temporales b√°sicas\n",
    "            temporal_features['hour'] = data['timestamp'].dt.hour\n",
    "            temporal_features['day_of_week'] = data['timestamp'].dt.dayofweek\n",
    "            temporal_features['month'] = data['timestamp'].dt.month\n",
    "            temporal_features['quarter'] = data['timestamp'].dt.quarter\n",
    "            \n",
    "            # Caracter√≠sticas temporales derivadas\n",
    "            temporal_features['is_business_hour'] = (\n",
    "                (temporal_features['hour'] >= 9) & (temporal_features['hour'] <= 17)\n",
    "            ).astype(int)\n",
    "            \n",
    "            temporal_features['is_weekend'] = temporal_features['day_of_week'].isin([5, 6]).astype(int)\n",
    "            \n",
    "            # Time since features (si hay m√∫ltiples registros)\n",
    "            if len(data) > 1:\n",
    "                data_sorted = data.sort_values('timestamp')\n",
    "                time_diffs = data_sorted['timestamp'].diff().dt.total_seconds()\n",
    "                temporal_features['time_since_last'] = time_diffs.fillna(0)\n",
    "        \n",
    "        return temporal_features\n",
    "    \n",
    "    def train_anomaly_detectors(self, prepared_data: Dict):\n",
    "        \"\"\"Entrenar todos los detectores de anomal√≠as\"\"\"\n",
    "        \n",
    "        feature_matrix = prepared_data['feature_matrix_scaled']\n",
    "        entity_type = prepared_data['entity_type']\n",
    "        \n",
    "        # Entrenar Isolation Forest\n",
    "        self.models['isolation_forest'].fit(feature_matrix)\n",
    "        \n",
    "        # Entrenar One-Class SVM\n",
    "        self.models['one_class_svm'].fit(feature_matrix)\n",
    "        \n",
    "        # Entrenar Elliptic Envelope\n",
    "        if feature_matrix.shape[0] > feature_matrix.shape[1]:  # Suficientes muestras\n",
    "            self.models['elliptic_envelope'].fit(feature_matrix)\n",
    "        \n",
    "        # Entrenar Autoencoder\n",
    "        if feature_matrix.shape[1] > 2:  # Suficientes caracter√≠sticas\n",
    "            self.autoencoder = self._build_autoencoder(feature_matrix.shape[1])\n",
    "            \n",
    "            # Entrenar autoencoder\n",
    "            history = self.autoencoder.fit(\n",
    "                feature_matrix, feature_matrix,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                validation_split=0.2,\n",
    "                verbose=0,\n",
    "                callbacks=[\n",
    "                    tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Calcular threshold para autoencoder\n",
    "            predictions = self.autoencoder.predict(feature_matrix)\n",
    "            mse = np.mean(np.power(feature_matrix - predictions, 2), axis=1)\n",
    "            self.thresholds['autoencoder'] = np.percentile(mse, 95)\n",
    "        \n",
    "        # Entrenar LSTM para datos temporales (si aplica)\n",
    "        if 'timestamp' in prepared_data['features'].columns:\n",
    "            self._train_lstm_detector(prepared_data)\n",
    "        \n",
    "        self.logger.info(f\"Detectores de anomal√≠as entrenados para {entity_type}\")\n",
    "    \n",
    "    def _train_lstm_detector(self, prepared_data: Dict):\n",
    "        \"\"\"Entrenar detector LSTM para anomal√≠as temporales\"\"\"\n",
    "        \n",
    "        # Preparar secuencias temporales\n",
    "        feature_matrix = prepared_data['feature_matrix_scaled']\n",
    "        sequence_length = min(10, len(feature_matrix) // 4)\n",
    "        \n",
    "        if len(feature_matrix) > sequence_length * 2:\n",
    "            X, y = self._create_sequences(feature_matrix, sequence_length)\n",
    "            \n",
    "            self.lstm_anomaly = self._build_lstm_anomaly_detector(\n",
    "                sequence_length, feature_matrix.shape[1]\n",
    "            )\n",
    "            \n",
    "            # Entrenar LSTM\n",
    "            self.lstm_anomaly.fit(\n",
    "                X, y,\n",
    "                epochs=50,\n",
    "                batch_size=16,\n",
    "                validation_split=0.2,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Calcular threshold\n",
    "            predictions = self.lstm_anomaly.predict(X)\n",
    "            mse = np.mean(np.power(y - predictions, 2), axis=1)\n",
    "            self.thresholds['lstm'] = np.percentile(mse, 95)\n",
    "    \n",
    "    def _create_sequences(self, data: np.ndarray, sequence_length: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Crear secuencias para entrenamiento LSTM\"\"\"\n",
    "        \n",
    "        X, y = [], []\n",
    "        \n",
    "        for i in range(len(data) - sequence_length):\n",
    "            X.append(data[i:(i + sequence_length)])\n",
    "            y.append(data[i + sequence_length])\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def detect_anomalies(self, \n",
    "                        data: pd.DataFrame,\n",
    "                        entity_type: str = 'transaction',\n",
    "                        methods: List[str] = None) -> List[AnomalyResult]:\n",
    "        \"\"\"Detectar anomal√≠as en nuevos datos\"\"\"\n",
    "        \n",
    "        if methods is None:\n",
    "            methods = ['isolation_forest', 'one_class_svm', 'autoencoder', 'statistical']\n",
    "        \n",
    "        # Preparar datos\n",
    "        prepared_data = self.prepare_data_for_training(data, entity_type)\n",
    "        feature_matrix = prepared_data['feature_matrix_scaled']\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Detectar con cada m√©todo\n",
    "        for method in methods:\n",
    "            if method in self.models or method in ['autoencoder', 'lstm', 'statistical']:\n",
    "                method_results = self._detect_with_method(\n",
    "                    method, feature_matrix, prepared_data, data\n",
    "                )\n",
    "                results.extend(method_results)\n",
    "        \n",
    "        # Combinar resultados y filtrar duplicados\n",
    "        combined_results = self._combine_anomaly_results(results)\n",
    "        \n",
    "        return combined_results\n",
    "    \n",
    "    def _detect_with_method(self, \n",
    "                          method: str,\n",
    "                          feature_matrix: np.ndarray,\n",
    "                          prepared_data: Dict,\n",
    "                          original_data: pd.DataFrame) -> List[AnomalyResult]:\n",
    "        \"\"\"Detectar anomal√≠as con un m√©todo espec√≠fico\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        try:\n",
    "            if method == 'isolation_forest':\n",
    "                scores = self.models['isolation_forest'].decision_function(feature_matrix)\n",
    "                predictions = self.models['isolation_forest'].predict(feature_matrix)\n",
    "                \n",
    "                for i, (score, pred) in enumerate(zip(scores, predictions)):\n",
    "                    if pred == -1:  # Anomal√≠a detectada\n",
    "                        result = self._create_anomaly_result(\n",
    "                            i, score, method, prepared_data, original_data\n",
    "                        )\n",
    "                        results.append(result)\n",
    "            \n",
    "            elif method == 'one_class_svm':\n",
    "                scores = self.models['one_class_svm'].decision_function(feature_matrix)\n",
    "                predictions = self.models['one_class_svm'].predict(feature_matrix)\n",
    "                \n",
    "                for i, (score, pred) in enumerate(zip(scores, predictions)):\n",
    "                    if pred == -1:\n",
    "                        result = self._create_anomaly_result(\n",
    "                            i, score, method, prepared_data, original_data\n",
    "                        )\n",
    "                        results.append(result)\n",
    "            \n",
    "            elif method == 'autoencoder' and self.autoencoder:\n",
    "                predictions = self.autoencoder.predict(feature_matrix)\n",
    "                mse_scores = np.mean(np.power(feature_matrix - predictions, 2), axis=1)\n",
    "                \n",
    "                threshold = self.thresholds.get('autoencoder', np.percentile(mse_scores, 95))\n",
    "                \n",
    "                for i, score in enumerate(mse_scores):\n",
    "                    if score > threshold:\n",
    "                        result = self._create_anomaly_result(\n",
    "                            i, score, method, prepared_data, original_data\n",
    "                        )\n",
    "                        results.append(result)\n",
    "            \n",
    "            elif method == 'statistical':\n",
    "                statistical_results = self._statistical_anomaly_detection(\n",
    "                    feature_matrix, prepared_data, original_data\n",
    "                )\n",
    "                results.extend(statistical_results)\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error en detecci√≥n con {method}: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _statistical_anomaly_detection(self, \n",
    "                                     feature_matrix: np.ndarray,\n",
    "                                     prepared_data: Dict,\n",
    "                                     original_data: pd.DataFrame) -> List[AnomalyResult]:\n",
    "        \"\"\"Detecci√≥n estad√≠stica de anomal√≠as\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Z-score detection\n",
    "        z_scores = np.abs(stats.zscore(feature_matrix, axis=0))\n",
    "        z_threshold = 3.0\n",
    "        \n",
    "        # IQR detection\n",
    "        q1 = np.percentile(feature_matrix, 25, axis=0)\n",
    "        q3 = np.percentile(feature_matrix, 75, axis=0)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        \n",
    "        for i in range(len(feature_matrix)):\n",
    "            # Z-score anomalies\n",
    "            z_anomalies = np.any(z_scores[i] > z_threshold)\n",
    "            \n",
    "            # IQR anomalies\n",
    "            iqr_anomalies = np.any(\n",
    "                (feature_matrix[i] < lower_bound) | (feature_matrix[i] > upper_bound)\n",
    "            )\n",
    "            \n",
    "            if z_anomalies or iqr_anomalies:\n",
    "                # Calcular score combinado\n",
    "                max_z = np.max(z_scores[i])\n",
    "                anomaly_score = min(1.0, max_z / 5.0)  # Normalizar a [0,1]\n",
    "                \n",
    "                result = self._create_anomaly_result(\n",
    "                    i, anomaly_score, 'statistical', prepared_data, original_data\n",
    "                )\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _create_anomaly_result(self, \n",
    "                             index: int,\n",
    "                             score: float,\n",
    "                             method: str,\n",
    "                             prepared_data: Dict,\n",
    "                             original_data: pd.DataFrame) -> AnomalyResult:\n",
    "        \"\"\"Crear resultado de anomal√≠a\"\"\"\n",
    "        \n",
    "        # Determinar ID de entidad\n",
    "        if 'id' in original_data.columns:\n",
    "            entity_id = original_data.iloc[index]['id']\n",
    "        elif 'user_id' in original_data.columns:\n",
    "            entity_id = original_data.iloc[index]['user_id']\n",
    "        elif 'product_id' in original_data.columns:\n",
    "            entity_id = original_data.iloc[index]['product_id']\n",
    "        else:\n",
    "            entity_id = index\n",
    "        \n",
    "        # Determinar tipo de anomal√≠a\n",
    "        anomaly_type = self._classify_anomaly_type(prepared_data['entity_type'], score)\n",
    "        \n",
    "        # Determinar severidad\n",
    "        severity = self._calculate_severity(score, anomaly_type)\n",
    "        \n",
    "        # Calcular confianza\n",
    "        confidence = self._calculate_confidence(score, method)\n",
    "        \n",
    "        # Identificar caracter√≠sticas an√≥malas\n",
    "        feature_values = prepared_data['feature_matrix'][index]\n",
    "        feature_names = prepared_data['feature_names']\n",
    "        \n",
    "        anomalous_features = self._identify_anomalous_features(\n",
    "            feature_values, feature_names, prepared_data['feature_matrix']\n",
    "        )\n",
    "        \n",
    "        # Generar descripci√≥n y recomendaciones\n",
    "        description = self._generate_anomaly_description(anomaly_type, severity, method)\n",
    "        recommendations = self._generate_recommendations(anomaly_type, severity)\n",
    "        \n",
    "        return AnomalyResult(\n",
    "            entity_id=entity_id,\n",
    "            entity_type=prepared_data['entity_type'],\n",
    "            anomaly_type=anomaly_type,\n",
    "            severity=severity,\n",
    "            anomaly_score=score,\n",
    "            confidence=confidence,\n",
    "            description=description,\n",
    "            features_analyzed=feature_names,\n",
    "            anomalous_features=anomalous_features,\n",
    "            detection_method=method,\n",
    "            timestamp=datetime.utcnow(),\n",
    "            recommendations=recommendations\n",
    "        )\n",
    "    \n",
    "    def _classify_anomaly_type(self, entity_type: str, score: float) -> AnomalyType:\n",
    "        \"\"\"Clasificar tipo de anomal√≠a basado en entidad y score\"\"\"\n",
    "        \n",
    "        if entity_type == 'transaction':\n",
    "            if score > 0.8:\n",
    "                return AnomalyType.FRAUD\n",
    "            else:\n",
    "                return AnomalyType.OUTLIER\n",
    "        elif entity_type == 'user':\n",
    "            return AnomalyType.BEHAVIORAL\n",
    "        elif entity_type == 'product':\n",
    "            return AnomalyType.INVENTORY\n",
    "        else:\n",
    "            return AnomalyType.PATTERN\n",
    "    \n",
    "    def _calculate_severity(self, score: float, anomaly_type: AnomalyType) -> AnomalySeverity:\n",
    "        \"\"\"Calcular severidad de la anomal√≠a\"\"\"\n",
    "        \n",
    "        if score > 0.9:\n",
    "            return AnomalySeverity.CRITICAL\n",
    "        elif score > 0.7:\n",
    "            return AnomalySeverity.HIGH\n",
    "        elif score > 0.5:\n",
    "            return AnomalySeverity.MEDIUM\n",
    "        else:\n",
    "            return AnomalySeverity.LOW\n",
    "    \n",
    "    def _calculate_confidence(self, score: float, method: str) -> float:\n",
    "        \"\"\"Calcular confianza en la detecci√≥n\"\"\"\n",
    "        \n",
    "        # Confianza basada en el m√©todo y score\n",
    "        method_confidence = {\n",
    "            'isolation_forest': 0.8,\n",
    "            'one_class_svm': 0.85,\n",
    "            'autoencoder': 0.75,\n",
    "            'statistical': 0.7,\n",
    "            'lstm': 0.8\n",
    "        }\n",
    "        \n",
    "        base_confidence = method_confidence.get(method, 0.7)\n",
    "        score_confidence = min(1.0, abs(score))\n",
    "        \n",
    "        return (base_confidence + score_confidence) / 2\n",
    "    \n",
    "    def _identify_anomalous_features(self, \n",
    "                                   feature_values: np.ndarray,\n",
    "                                   feature_names: List[str],\n",
    "                                   all_features: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Identificar qu√© caracter√≠sticas son an√≥malas\"\"\"\n",
    "        \n",
    "        anomalous_features = {}\n",
    "        \n",
    "        # Calcular z-scores para cada caracter√≠stica\n",
    "        feature_means = np.mean(all_features, axis=0)\n",
    "        feature_stds = np.std(all_features, axis=0)\n",
    "        \n",
    "        for i, (value, name) in enumerate(zip(feature_values, feature_names)):\n",
    "            if feature_stds[i] > 0:\n",
    "                z_score = abs((value - feature_means[i]) / feature_stds[i])\n",
    "                if z_score > 2.0:  # Threshold para considerarlo an√≥malo\n",
    "                    anomalous_features[name] = float(z_score)\n",
    "        \n",
    "        return anomalous_features\n",
    "    \n",
    "    def _generate_anomaly_description(self, \n",
    "                                    anomaly_type: AnomalyType,\n",
    "                                    severity: AnomalySeverity,\n",
    "                                    method: str) -> str:\n",
    "        \"\"\"Generar descripci√≥n de la anomal√≠a\"\"\"\n",
    "        \n",
    "        descriptions = {\n",
    "            AnomalyType.FRAUD: f\"Posible transacci√≥n fraudulenta detectada (severidad: {severity.value})\",\n",
    "            AnomalyType.OUTLIER: f\"Comportamiento at√≠pico identificado (severidad: {severity.value})\",\n",
    "            AnomalyType.BEHAVIORAL: f\"Patr√≥n de comportamiento an√≥malo (severidad: {severity.value})\",\n",
    "            AnomalyType.INVENTORY: f\"Anomal√≠a en niveles de inventario (severidad: {severity.value})\",\n",
    "            AnomalyType.PATTERN: f\"Patr√≥n an√≥malo detectado (severidad: {severity.value})\"\n",
    "        }\n",
    "        \n",
    "        return descriptions.get(anomaly_type, f\"Anomal√≠a detectada usando {method}\")\n",
    "    \n",
    "    def _generate_recommendations(self, \n",
    "                                anomaly_type: AnomalyType,\n",
    "                                severity: AnomalySeverity) -> List[str]:\n",
    "        \"\"\"Generar recomendaciones basadas en tipo y severidad\"\"\"\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        if anomaly_type == AnomalyType.FRAUD:\n",
    "            recommendations.extend([\n",
    "                \"Revisar transacci√≥n inmediatamente\",\n",
    "                \"Verificar identidad del usuario\",\n",
    "                \"Considerar bloqueo temporal\"\n",
    "            ])\n",
    "        elif anomaly_type == AnomalyType.BEHAVIORAL:\n",
    "            recommendations.extend([\n",
    "                \"Analizar patr√≥n de comportamiento\",\n",
    "                \"Verificar cuenta de usuario\",\n",
    "                \"Monitorear actividad futura\"\n",
    "            ])\n",
    "        elif anomaly_type == AnomalyType.INVENTORY:\n",
    "            recommendations.extend([\n",
    "                \"Revisar niveles de stock\",\n",
    "                \"Verificar datos de inventario\",\n",
    "                \"Actualizar sistema de gesti√≥n\"\n",
    "            ])\n",
    "        \n",
    "        if severity in [AnomalySeverity.HIGH, AnomalySeverity.CRITICAL]:\n",
    "            recommendations.append(\"Acci√≥n inmediata requerida\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _combine_anomaly_results(self, results: List[AnomalyResult]) -> List[AnomalyResult]:\n",
    "        \"\"\"Combinar y deduplicar resultados de anomal√≠as\"\"\"\n",
    "        \n",
    "        # Agrupar por entity_id\n",
    "        grouped_results = {}\n",
    "        \n",
    "        for result in results:\n",
    "            entity_id = result.entity_id\n",
    "            \n",
    "            if entity_id not in grouped_results:\n",
    "                grouped_results[entity_id] = []\n",
    "            \n",
    "            grouped_results[entity_id].append(result)\n",
    "        \n",
    "        # Combinar resultados para cada entidad\n",
    "        combined_results = []\n",
    "        \n",
    "        for entity_id, entity_results in grouped_results.items():\n",
    "            if len(entity_results) == 1:\n",
    "                combined_results.append(entity_results[0])\n",
    "            else:\n",
    "                # Combinar m√∫ltiples detecciones\n",
    "                combined_result = self._merge_anomaly_results(entity_results)\n",
    "                combined_results.append(combined_result)\n",
    "        \n",
    "        # Ordenar por score descendente\n",
    "        combined_results.sort(key=lambda x: x.anomaly_score, reverse=True)\n",
    "        \n",
    "        return combined_results\n",
    "    \n",
    "    def _merge_anomaly_results(self, results: List[AnomalyResult]) -> AnomalyResult:\n",
    "        \"\"\"Fusionar m√∫ltiples resultados de anomal√≠as para la misma entidad\"\"\"\n",
    "        \n",
    "        # Tomar el resultado con mayor score como base\n",
    "        base_result = max(results, key=lambda x: x.anomaly_score)\n",
    "        \n",
    "        # Combinar m√©todos de detecci√≥n\n",
    "        detection_methods = [r.detection_method for r in results]\n",
    "        combined_method = \" + \".join(set(detection_methods))\n",
    "        \n",
    "        # Combinar caracter√≠sticas an√≥malas\n",
    "        combined_features = {}\n",
    "        for result in results:\n",
    "            combined_features.update(result.anomalous_features)\n",
    "        \n",
    "        # Promediar confidence\n",
    "        avg_confidence = np.mean([r.confidence for r in results])\n",
    "        \n",
    "        # Combinar recomendaciones\n",
    "        all_recommendations = []\n",
    "        for result in results:\n",
    "            all_recommendations.extend(result.recommendations)\n",
    "        unique_recommendations = list(set(all_recommendations))\n",
    "        \n",
    "        # Crear resultado combinado\n",
    "        return AnomalyResult(\n",
    "            entity_id=base_result.entity_id,\n",
    "            entity_type=base_result.entity_type,\n",
    "            anomaly_type=base_result.anomaly_type,\n",
    "            severity=base_result.severity,\n",
    "            anomaly_score=base_result.anomaly_score,\n",
    "            confidence=avg_confidence,\n",
    "            description=f\"M√∫ltiples m√©todos detectaron: {base_result.description}\",\n",
    "            features_analyzed=base_result.features_analyzed,\n",
    "            anomalous_features=combined_features,\n",
    "            detection_method=combined_method,\n",
    "            timestamp=base_result.timestamp,\n",
    "            recommendations=unique_recommendations\n",
    "        )\n",
    "    \n",
    "    def detect_patterns(self, \n",
    "                       anomaly_results: List[AnomalyResult],\n",
    "                       time_window: timedelta = timedelta(hours=24)) -> List[AnomalyPattern]:\n",
    "        \"\"\"Detectar patrones en anomal√≠as\"\"\"\n",
    "        \n",
    "        patterns = []\n",
    "        \n",
    "        # Agrupar anomal√≠as por tiempo\n",
    "        current_time = datetime.utcnow()\n",
    "        recent_anomalies = [\n",
    "            result for result in anomaly_results\n",
    "            if (current_time - result.timestamp) <= time_window\n",
    "        ]\n",
    "        \n",
    "        if len(recent_anomalies) < 3:\n",
    "            return patterns  # No suficientes anomal√≠as para detectar patrones\n",
    "        \n",
    "        # Detectar patrones temporales\n",
    "        temporal_pattern = self._detect_temporal_patterns(recent_anomalies)\n",
    "        if temporal_pattern:\n",
    "            patterns.append(temporal_pattern)\n",
    "        \n",
    "        # Detectar patrones por tipo\n",
    "        type_patterns = self._detect_type_patterns(recent_anomalies)\n",
    "        patterns.extend(type_patterns)\n",
    "        \n",
    "        # Detectar patrones geogr√°ficos o por caracter√≠sticas\n",
    "        feature_patterns = self._detect_feature_patterns(recent_anomalies)\n",
    "        patterns.extend(feature_patterns)\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def _detect_temporal_patterns(self, anomalies: List[AnomalyResult]) -> Optional[AnomalyPattern]:\n",
    "        \"\"\"Detectar patrones temporales en anomal√≠as\"\"\"\n",
    "        \n",
    "        timestamps = [a.timestamp for a in anomalies]\n",
    "        \n",
    "        # Analizar intervalos entre anomal√≠as\n",
    "        if len(timestamps) > 2:\n",
    "            intervals = [(timestamps[i+1] - timestamps[i]).total_seconds() \n",
    "                        for i in range(len(timestamps)-1)]\n",
    "            \n",
    "            # Si hay regularidad en los intervalos, es un patr√≥n\n",
    "            if len(set([round(interval/60) for interval in intervals])) <= 2:  # Variaci√≥n <= 2 minutos\n",
    "                return AnomalyPattern(\n",
    "                    pattern_id=f\"temporal_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\",\n",
    "                    pattern_type=\"temporal_regular\",\n",
    "                    frequency=len(anomalies),\n",
    "                    entities_affected=[a.entity_id for a in anomalies],\n",
    "                    temporal_pattern={\n",
    "                        'interval_seconds': np.mean(intervals),\n",
    "                        'regularity_score': 1.0 - (np.std(intervals) / np.mean(intervals))\n",
    "                    },\n",
    "                    feature_signature={},\n",
    "                    risk_score=0.8\n",
    "                )\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _detect_type_patterns(self, anomalies: List[AnomalyResult]) -> List[AnomalyPattern]:\n",
    "        \"\"\"Detectar patrones por tipo de anomal√≠a\"\"\"\n",
    "        \n",
    "        patterns = []\n",
    "        \n",
    "        # Agrupar por tipo\n",
    "        type_groups = {}\n",
    "        for anomaly in anomalies:\n",
    "            anomaly_type = anomaly.anomaly_type\n",
    "            if anomaly_type not in type_groups:\n",
    "                type_groups[anomaly_type] = []\n",
    "            type_groups[anomaly_type].append(anomaly)\n",
    "        \n",
    "        # Detectar concentraciones an√≥malas por tipo\n",
    "        for anomaly_type, group_anomalies in type_groups.items():\n",
    "            if len(group_anomalies) >= 3:  # Threshold para considerar patr√≥n\n",
    "                patterns.append(AnomalyPattern(\n",
    "                    pattern_id=f\"type_{anomaly_type.value}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\",\n",
    "                    pattern_type=f\"concentrated_{anomaly_type.value}\",\n",
    "                    frequency=len(group_anomalies),\n",
    "                    entities_affected=[a.entity_id for a in group_anomalies],\n",
    "                    temporal_pattern={},\n",
    "                    feature_signature={},\n",
    "                    risk_score=min(1.0, len(group_anomalies) / 10.0)\n",
    "                ))\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def _detect_feature_patterns(self, anomalies: List[AnomalyResult]) -> List[AnomalyPattern]:\n",
    "        \"\"\"Detectar patrones en caracter√≠sticas an√≥malas\"\"\"\n",
    "        \n",
    "        patterns = []\n",
    "        \n",
    "        # Analizar caracter√≠sticas comunes\n",
    "        all_features = {}\n",
    "        for anomaly in anomalies:\n",
    "            for feature, value in anomaly.anomalous_features.items():\n",
    "                if feature not in all_features:\n",
    "                    all_features[feature] = []\n",
    "                all_features[feature].append(value)\n",
    "        \n",
    "        # Detectar caracter√≠sticas que aparecen frecuentemente\n",
    "        frequent_features = {\n",
    "            feature: values for feature, values in all_features.items()\n",
    "            if len(values) >= max(3, len(anomalies) * 0.3)  # Al menos 30% de anomal√≠as\n",
    "        }\n",
    "        \n",
    "        if frequent_features:\n",
    "            patterns.append(AnomalyPattern(\n",
    "                pattern_id=f\"features_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\",\n",
    "                pattern_type=\"common_features\",\n",
    "                frequency=len(anomalies),\n",
    "                entities_affected=[a.entity_id for a in anomalies],\n",
    "                temporal_pattern={},\n",
    "                feature_signature={\n",
    "                    feature: np.mean(values) for feature, values in frequent_features.items()\n",
    "                },\n",
    "                risk_score=min(1.0, len(frequent_features) / 5.0)\n",
    "            ))\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def save_models(self, model_path: str):\n",
    "        \"\"\"Guardar modelos de detecci√≥n de anomal√≠as\"\"\"\n",
    "        \n",
    "        # Guardar modelos sklearn\n",
    "        joblib.dump(self.models, f\"{model_path}/anomaly_models.pkl\")\n",
    "        joblib.dump(self.scalers, f\"{model_path}/anomaly_scalers.pkl\")\n",
    "        joblib.dump(self.thresholds, f\"{model_path}/anomaly_thresholds.pkl\")\n",
    "        \n",
    "        # Guardar modelos deep learning\n",
    "        if self.autoencoder:\n",
    "            self.autoencoder.save(f\"{model_path}/autoencoder.h5\")\n",
    "        \n",
    "        if self.lstm_anomaly:\n",
    "            self.lstm_anomaly.save(f\"{model_path}/lstm_anomaly.h5\")\n",
    "        \n",
    "        self.logger.info(f\"Modelos de detecci√≥n de anomal√≠as guardados en {model_path}\")\n",
    "    \n",
    "    def load_models(self, model_path: str):\n",
    "        \"\"\"Cargar modelos de detecci√≥n de anomal√≠as\"\"\"\n",
    "        \n",
    "        try:\n",
    "            self.models = joblib.load(f\"{model_path}/anomaly_models.pkl\")\n",
    "            self.scalers = joblib.load(f\"{model_path}/anomaly_scalers.pkl\")\n",
    "            self.thresholds = joblib.load(f\"{model_path}/anomaly_thresholds.pkl\")\n",
    "            \n",
    "            # Cargar modelos deep learning\n",
    "            try:\n",
    "                self.autoencoder = load_model(f\"{model_path}/autoencoder.h5\")\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                self.lstm_anomaly = load_model(f\"{model_path}/lstm_anomaly.h5\")\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            \n",
    "            self.logger.info(f\"Modelos de detecci√≥n de anomal√≠as cargados desde {model_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error cargando modelos de detecci√≥n de anomal√≠as: {e}\")\n",
    "            raise\n",
    "\n",
    "# Factory function\n",
    "def create_anomaly_detector() -> AdvancedAnomalyDetector:\n",
    "    \"\"\"Factory para crear instancia del detector de anomal√≠as\"\"\"\n",
    "    return AdvancedAnomalyDetector()\n",
    "'''\n",
    "\n",
    "# Escribir anomaly_detector.py\n",
    "with open(\"../app/models/anomaly_detector.py\", \"w\") as f:\n",
    "    f.write(anomaly_detector_content)\n",
    "\n",
    "print(\"‚úÖ anomaly_detector.py creado exitosamente\")\n",
    "print(\"üö® Detector de anomal√≠as implementado:\")\n",
    "print(\"   ‚Ä¢ Isolation Forest: Detecci√≥n de outliers generales\")\n",
    "print(\"   ‚Ä¢ Local Outlier Factor: Anomal√≠as locales y contextuales\")\n",
    "print(\"   ‚Ä¢ One-Class SVM: Patrones complejos no lineales\")\n",
    "print(\"   ‚Ä¢ Autoencoder: Deep anomaly detection con redes neuronales\")\n",
    "print(\"   ‚Ä¢ LSTM: Detecci√≥n de anomal√≠as temporales en series de tiempo\")\n",
    "print(\"   ‚Ä¢ Statistical Methods: Z-score e IQR para detecci√≥n estad√≠stica\")\n",
    "print(\"   ‚Ä¢ Pattern Detection: Identificaci√≥n de patrones an√≥malos\")\n",
    "print(\"   ‚Ä¢ Multi-entity Support: Transacciones, usuarios, productos\")\n",
    "print(\"   ‚Ä¢ Fraud Detection: Espec√≠fico para detecci√≥n de fraude\")\n",
    "print(\"   ‚Ä¢ Severity Classification: Niveles de severidad y confianza\")\n",
    "print(\"   ‚Ä¢ Recommendation Engine: Acciones sugeridas por anomal√≠a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987fe639",
   "metadata": {},
   "source": [
    "## üß† 8. Analizador de Sentimientos Avanzado\n",
    "Sistema de NLP para an√°lisis de sentimientos en rese√±as de productos, comentarios de usuarios y feedback. Utiliza transformers, BERT, y t√©cnicas avanzadas de procesamiento de lenguaje natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c8dddd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ sentiment_analyzer.py creado exitosamente\n",
      "üß† Analizador de sentimientos implementado:\n",
      "   ‚Ä¢ BERT: Transformer preentrenado para an√°lisis avanzado\n",
      "   ‚Ä¢ LSTM: Red neuronal recurrente para secuencias de texto\n",
      "   ‚Ä¢ Traditional ML: Logistic Regression, Random Forest, SVM\n",
      "   ‚Ä¢ VADER: Analizador lexical especializado en redes sociales\n",
      "   ‚Ä¢ Ensemble: Combinaci√≥n inteligente de m√∫ltiples modelos\n",
      "   ‚Ä¢ Emotion Detection: Identificaci√≥n de emociones espec√≠ficas\n",
      "   ‚Ä¢ Aspect-based Analysis: Sentimientos por aspectos del producto\n",
      "   ‚Ä¢ Key Phrase Extraction: Identificaci√≥n de frases importantes\n",
      "   ‚Ä¢ Language Detection: Identificaci√≥n autom√°tica de idioma\n",
      "   ‚Ä¢ Batch Processing: An√°lisis masivo de textos\n",
      "   ‚Ä¢ Comprehensive Reporting: Res√∫menes y estad√≠sticas detalladas\n"
     ]
    }
   ],
   "source": [
    "# sentiment_analyzer.py - Analizador de sentimientos avanzado\n",
    "sentiment_analyzer_content = '''\n",
    "\"\"\"\n",
    "Analizador de sentimientos avanzado para e-commerce empresarial\n",
    "Utiliza BERT, transformers y t√©cnicas de NLP para an√°lisis profundo de texto\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "import logging\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import joblib\n",
    "\n",
    "# NLP Libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, GRU, Embedding, Dropout, GlobalMaxPooling1D, Conv1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Transformers\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    from transformers import pipeline, BertTokenizer, BertForSequenceClassification\n",
    "    from transformers import TextClassificationPipeline\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "    logging.warning(\"Transformers library not available. Some features will be disabled.\")\n",
    "\n",
    "# Configuration\n",
    "from ..config import ML_MODEL_CONFIG, settings\n",
    "\n",
    "class SentimentLabel(Enum):\n",
    "    \"\"\"Etiquetas de sentimiento\"\"\"\n",
    "    POSITIVE = \"positive\"\n",
    "    NEGATIVE = \"negative\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    MIXED = \"mixed\"\n",
    "\n",
    "class EmotionLabel(Enum):\n",
    "    \"\"\"Etiquetas de emociones espec√≠ficas\"\"\"\n",
    "    JOY = \"joy\"\n",
    "    ANGER = \"anger\"\n",
    "    SADNESS = \"sadness\"\n",
    "    FEAR = \"fear\"\n",
    "    SURPRISE = \"surprise\"\n",
    "    LOVE = \"love\"\n",
    "    DISGUST = \"disgust\"\n",
    "\n",
    "@dataclass\n",
    "class SentimentResult:\n",
    "    \"\"\"Resultado de an√°lisis de sentimientos\"\"\"\n",
    "    text_id: Optional[str]\n",
    "    text: str\n",
    "    sentiment: SentimentLabel\n",
    "    confidence: float\n",
    "    emotion: Optional[EmotionLabel]\n",
    "    emotion_confidence: float\n",
    "    scores: Dict[str, float]\n",
    "    key_phrases: List[str]\n",
    "    aspects: Dict[str, SentimentLabel]\n",
    "    language: str\n",
    "    word_count: int\n",
    "    model_used: str\n",
    "    timestamp: datetime\n",
    "\n",
    "@dataclass\n",
    "class AspectSentiment:\n",
    "    \"\"\"Sentimiento por aspecto espec√≠fico\"\"\"\n",
    "    aspect: str\n",
    "    sentiment: SentimentLabel\n",
    "    confidence: float\n",
    "    mentions: List[str]\n",
    "\n",
    "class AdvancedSentimentAnalyzer:\n",
    "    \"\"\"Analizador de sentimientos avanzado empresarial\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.tokenizers = {}\n",
    "        self.vectorizers = {}\n",
    "        self.config = ML_MODEL_CONFIG[\"sentiment_analyzer\"]\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Modelos espec√≠ficos\n",
    "        self.bert_model = None\n",
    "        self.lstm_model = None\n",
    "        self.traditional_model = None\n",
    "        \n",
    "        # NLP components\n",
    "        self.lemmatizer = None\n",
    "        self.sentiment_analyzer = None\n",
    "        self.stop_words = set()\n",
    "        \n",
    "        # Aspect-based sentiment\n",
    "        self.aspect_keywords = {}\n",
    "        \n",
    "        self._initialize_nlp_components()\n",
    "        self._initialize_models()\n",
    "    \n",
    "    def _initialize_nlp_components(self):\n",
    "        \"\"\"Inicializar componentes de NLP\"\"\"\n",
    "        \n",
    "        # Download NLTK data if needed\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "        \n",
    "        try:\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "        except LookupError:\n",
    "            nltk.download('stopwords')\n",
    "        \n",
    "        try:\n",
    "            nltk.data.find('corpora/wordnet')\n",
    "        except LookupError:\n",
    "            nltk.download('wordnet')\n",
    "        \n",
    "        try:\n",
    "            nltk.data.find('vader_lexicon')\n",
    "        except LookupError:\n",
    "            nltk.download('vader_lexicon')\n",
    "        \n",
    "        # Initialize components\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Define aspect keywords for e-commerce\n",
    "        self.aspect_keywords = {\n",
    "            'quality': ['quality', 'build', 'material', 'durable', 'cheap', 'flimsy', 'solid', 'sturdy'],\n",
    "            'price': ['price', 'cost', 'expensive', 'cheap', 'affordable', 'value', 'money', 'worth'],\n",
    "            'shipping': ['shipping', 'delivery', 'fast', 'slow', 'quick', 'delayed', 'arrived', 'package'],\n",
    "            'service': ['service', 'support', 'customer', 'help', 'staff', 'friendly', 'rude', 'helpful'],\n",
    "            'usability': ['easy', 'difficult', 'user-friendly', 'intuitive', 'complex', 'simple', 'use'],\n",
    "            'appearance': ['look', 'appearance', 'design', 'beautiful', 'ugly', 'attractive', 'style']\n",
    "        }\n",
    "    \n",
    "    def _initialize_models(self):\n",
    "        \"\"\"Inicializar modelos de an√°lisis de sentimientos\"\"\"\n",
    "        \n",
    "        # Traditional ML models\n",
    "        self.models['logistic'] = LogisticRegression(random_state=settings.model_random_state)\n",
    "        self.models['random_forest'] = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=settings.model_random_state\n",
    "        )\n",
    "        self.models['svm'] = SVC(\n",
    "            kernel='linear',\n",
    "            probability=True,\n",
    "            random_state=settings.model_random_state\n",
    "        )\n",
    "        \n",
    "        # Vectorizers\n",
    "        self.vectorizers['tfidf'] = TfidfVectorizer(\n",
    "            max_features=10000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english'\n",
    "        )\n",
    "        \n",
    "        self.vectorizers['count'] = CountVectorizer(\n",
    "            max_features=10000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english'\n",
    "        )\n",
    "        \n",
    "        # Keras tokenizer for deep learning\n",
    "        self.tokenizers['keras'] = Tokenizer(\n",
    "            num_words=10000,\n",
    "            oov_token=\"<OOV>\"\n",
    "        )\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Preprocesar texto para an√°lisis\"\"\"\n",
    "        \n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convertir a min√∫sculas\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remover URLs\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        \n",
    "        # Remover menciones y hashtags\n",
    "        text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
    "        text = re.sub(r'#[A-Za-z0-9_]+', '', text)\n",
    "        \n",
    "        # Remover caracteres especiales pero mantener algunos signos de puntuaci√≥n\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s!?.,;]', '', text)\n",
    "        \n",
    "        # Normalizar espacios\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def extract_features(self, texts: List[str]) -> Dict[str, any]:\n",
    "        \"\"\"Extraer caracter√≠sticas de texto para ML tradicional\"\"\"\n",
    "        \n",
    "        # Preprocesar textos\n",
    "        processed_texts = [self.preprocess_text(text) for text in texts]\n",
    "        \n",
    "        # TF-IDF features\n",
    "        tfidf_features = self.vectorizers['tfidf'].fit_transform(processed_texts)\n",
    "        \n",
    "        # Caracter√≠sticas adicionales\n",
    "        additional_features = []\n",
    "        \n",
    "        for text in processed_texts:\n",
    "            features = {\n",
    "                'word_count': len(text.split()),\n",
    "                'char_count': len(text),\n",
    "                'exclamation_count': text.count('!'),\n",
    "                'question_count': text.count('?'),\n",
    "                'upper_count': sum(1 for c in text if c.isupper()),\n",
    "                'sentiment_words': self._count_sentiment_words(text)\n",
    "            }\n",
    "            additional_features.append(list(features.values()))\n",
    "        \n",
    "        additional_features = np.array(additional_features)\n",
    "        \n",
    "        return {\n",
    "            'tfidf': tfidf_features,\n",
    "            'additional': additional_features,\n",
    "            'processed_texts': processed_texts\n",
    "        }\n",
    "    \n",
    "    def _count_sentiment_words(self, text: str) -> int:\n",
    "        \"\"\"Contar palabras con carga sentimental\"\"\"\n",
    "        \n",
    "        positive_words = ['good', 'great', 'excellent', 'amazing', 'fantastic', 'love', 'perfect', 'awesome']\n",
    "        negative_words = ['bad', 'terrible', 'awful', 'hate', 'horrible', 'worst', 'disappointed', 'poor']\n",
    "        \n",
    "        words = text.split()\n",
    "        sentiment_count = 0\n",
    "        \n",
    "        for word in words:\n",
    "            if word in positive_words:\n",
    "                sentiment_count += 1\n",
    "            elif word in negative_words:\n",
    "                sentiment_count -= 1\n",
    "        \n",
    "        return sentiment_count\n",
    "    \n",
    "    def train_traditional_models(self, texts: List[str], labels: List[str]):\n",
    "        \"\"\"Entrenar modelos tradicionales de ML\"\"\"\n",
    "        \n",
    "        # Extraer caracter√≠sticas\n",
    "        features = self.extract_features(texts)\n",
    "        X_tfidf = features['tfidf']\n",
    "        X_additional = features['additional']\n",
    "        \n",
    "        # Combinar caracter√≠sticas\n",
    "        from scipy.sparse import hstack\n",
    "        X_combined = hstack([X_tfidf, X_additional])\n",
    "        \n",
    "        # Codificar etiquetas\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_encoded = label_encoder.fit_transform(labels)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_combined, y_encoded, test_size=0.2, random_state=settings.model_random_state\n",
    "        )\n",
    "        \n",
    "        # Entrenar modelos\n",
    "        for name, model in self.models.items():\n",
    "            if name in ['logistic', 'random_forest', 'svm']:\n",
    "                self.logger.info(f\"Entrenando modelo {name}...\")\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # Evaluar modelo\n",
    "                y_pred = model.predict(X_test)\n",
    "                accuracy = np.mean(y_pred == y_test)\n",
    "                self.logger.info(f\"Accuracy {name}: {accuracy:.3f}\")\n",
    "        \n",
    "        # Guardar label encoder\n",
    "        self.models['label_encoder'] = label_encoder\n",
    "        \n",
    "        self.logger.info(\"Modelos tradicionales entrenados exitosamente\")\n",
    "    \n",
    "    def _build_lstm_model(self, vocab_size: int, embedding_dim: int = 128, max_length: int = 100) -> Model:\n",
    "        \"\"\"Construir modelo LSTM para an√°lisis de sentimientos\"\"\"\n",
    "        \n",
    "        model = Sequential([\n",
    "            Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "            LSTM(64, dropout=0.5, recurrent_dropout=0.5),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(3, activation='softmax')  # 3 clases: positive, negative, neutral\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_lstm_model(self, texts: List[str], labels: List[str]):\n",
    "        \"\"\"Entrenar modelo LSTM\"\"\"\n",
    "        \n",
    "        # Preprocesar textos\n",
    "        processed_texts = [self.preprocess_text(text) for text in texts]\n",
    "        \n",
    "        # Tokenizar\n",
    "        self.tokenizers['keras'].fit_on_texts(processed_texts)\n",
    "        sequences = self.tokenizers['keras'].texts_to_sequences(processed_texts)\n",
    "        \n",
    "        # Padding\n",
    "        max_length = 100\n",
    "        X = pad_sequences(sequences, maxlen=max_length)\n",
    "        \n",
    "        # Codificar etiquetas (one-hot)\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_encoded = label_encoder.fit_transform(labels)\n",
    "        y_categorical = tf.keras.utils.to_categorical(y_encoded)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y_categorical, test_size=0.2, random_state=settings.model_random_state\n",
    "        )\n",
    "        \n",
    "        # Construir modelo\n",
    "        vocab_size = len(self.tokenizers['keras'].word_index) + 1\n",
    "        self.lstm_model = self._build_lstm_model(vocab_size, max_length=max_length)\n",
    "        \n",
    "        # Entrenar\n",
    "        history = self.lstm_model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=32,\n",
    "            epochs=10,\n",
    "            validation_data=(X_test, y_test),\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Guardar label encoder\n",
    "        self.models['lstm_label_encoder'] = label_encoder\n",
    "        \n",
    "        self.logger.info(\"Modelo LSTM entrenado exitosamente\")\n",
    "        return history\n",
    "    \n",
    "    def load_bert_model(self, model_name: str = \"nlptown/bert-base-multilingual-uncased-sentiment\"):\n",
    "        \"\"\"Cargar modelo BERT preentrenado\"\"\"\n",
    "        \n",
    "        if not TRANSFORMERS_AVAILABLE:\n",
    "            self.logger.warning(\"Transformers no disponible. Modelo BERT no cargado.\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Cargar modelo preentrenado\n",
    "            self.bert_model = pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=model_name,\n",
    "                return_all_scores=True\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f\"Modelo BERT cargado: {model_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error cargando modelo BERT: {e}\")\n",
    "            self.bert_model = None\n",
    "    \n",
    "    def analyze_sentiment(self, \n",
    "                         text: str,\n",
    "                         model_type: str = \"ensemble\",\n",
    "                         include_emotions: bool = True) -> SentimentResult:\n",
    "        \"\"\"Analizar sentimiento de un texto\"\"\"\n",
    "        \n",
    "        if not text or not isinstance(text, str):\n",
    "            return self._create_empty_result(text)\n",
    "        \n",
    "        text_clean = self.preprocess_text(text)\n",
    "        \n",
    "        # Obtener predicciones de diferentes modelos\n",
    "        predictions = {}\n",
    "        \n",
    "        if model_type in [\"traditional\", \"ensemble\"]:\n",
    "            predictions['traditional'] = self._predict_traditional(text_clean)\n",
    "        \n",
    "        if model_type in [\"lstm\", \"ensemble\"] and self.lstm_model:\n",
    "            predictions['lstm'] = self._predict_lstm(text_clean)\n",
    "        \n",
    "        if model_type in [\"bert\", \"ensemble\"] and self.bert_model:\n",
    "            predictions['bert'] = self._predict_bert(text)\n",
    "        \n",
    "        if model_type in [\"vader\", \"ensemble\"]:\n",
    "            predictions['vader'] = self._predict_vader(text_clean)\n",
    "        \n",
    "        # Combinar predicciones\n",
    "        final_sentiment, confidence, scores = self._combine_predictions(predictions)\n",
    "        \n",
    "        # Detectar emociones\n",
    "        emotion, emotion_confidence = None, 0.0\n",
    "        if include_emotions:\n",
    "            emotion, emotion_confidence = self._detect_emotion(text_clean)\n",
    "        \n",
    "        # Extraer frases clave\n",
    "        key_phrases = self._extract_key_phrases(text_clean)\n",
    "        \n",
    "        # An√°lisis por aspectos\n",
    "        aspects = self._analyze_aspects(text_clean)\n",
    "        \n",
    "        # Detectar idioma (simplificado)\n",
    "        language = self._detect_language(text)\n",
    "        \n",
    "        return SentimentResult(\n",
    "            text_id=None,\n",
    "            text=text,\n",
    "            sentiment=final_sentiment,\n",
    "            confidence=confidence,\n",
    "            emotion=emotion,\n",
    "            emotion_confidence=emotion_confidence,\n",
    "            scores=scores,\n",
    "            key_phrases=key_phrases,\n",
    "            aspects=aspects,\n",
    "            language=language,\n",
    "            word_count=len(text.split()),\n",
    "            model_used=model_type,\n",
    "            timestamp=datetime.utcnow()\n",
    "        )\n",
    "    \n",
    "    def _predict_traditional(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Predicci√≥n con modelos tradicionales\"\"\"\n",
    "        \n",
    "        if 'logistic' not in self.models:\n",
    "            return {}\n",
    "        \n",
    "        # Vectorizar texto\n",
    "        text_tfidf = self.vectorizers['tfidf'].transform([text])\n",
    "        \n",
    "        # Caracter√≠sticas adicionales\n",
    "        additional_features = np.array([[\n",
    "            len(text.split()),\n",
    "            len(text),\n",
    "            text.count('!'),\n",
    "            text.count('?'),\n",
    "            sum(1 for c in text if c.isupper()),\n",
    "            self._count_sentiment_words(text)\n",
    "        ]])\n",
    "        \n",
    "        # Combinar caracter√≠sticas\n",
    "        from scipy.sparse import hstack\n",
    "        X_combined = hstack([text_tfidf, additional_features])\n",
    "        \n",
    "        # Predecir con modelo log√≠stico\n",
    "        if 'label_encoder' in self.models:\n",
    "            proba = self.models['logistic'].predict_proba(X_combined)[0]\n",
    "            labels = self.models['label_encoder'].classes_\n",
    "            \n",
    "            return dict(zip(labels, proba))\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    def _predict_lstm(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Predicci√≥n con modelo LSTM\"\"\"\n",
    "        \n",
    "        if not self.lstm_model:\n",
    "            return {}\n",
    "        \n",
    "        # Tokenizar y hacer padding\n",
    "        sequence = self.tokenizers['keras'].texts_to_sequences([text])\n",
    "        padded = pad_sequences(sequence, maxlen=100)\n",
    "        \n",
    "        # Predecir\n",
    "        prediction = self.lstm_model.predict(padded)[0]\n",
    "        \n",
    "        # Mapear a etiquetas\n",
    "        if 'lstm_label_encoder' in self.models:\n",
    "            labels = self.models['lstm_label_encoder'].classes_\n",
    "            return dict(zip(labels, prediction))\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    def _predict_bert(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Predicci√≥n con modelo BERT\"\"\"\n",
    "        \n",
    "        if not self.bert_model:\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            # Predecir con BERT\n",
    "            results = self.bert_model(text)\n",
    "            \n",
    "            # Convertir a formato est√°ndar\n",
    "            prediction_dict = {}\n",
    "            for result in results:\n",
    "                label = result['label'].lower()\n",
    "                score = result['score']\n",
    "                \n",
    "                # Mapear etiquetas de BERT a nuestro formato\n",
    "                if 'pos' in label or label == 'positive':\n",
    "                    prediction_dict['positive'] = score\n",
    "                elif 'neg' in label or label == 'negative':\n",
    "                    prediction_dict['negative'] = score\n",
    "                else:\n",
    "                    prediction_dict['neutral'] = score\n",
    "            \n",
    "            return prediction_dict\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error en predicci√≥n BERT: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _predict_vader(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Predicci√≥n con VADER sentiment analyzer\"\"\"\n",
    "        \n",
    "        scores = self.sentiment_analyzer.polarity_scores(text)\n",
    "        \n",
    "        return {\n",
    "            'positive': scores['pos'],\n",
    "            'negative': scores['neg'],\n",
    "            'neutral': scores['neu']\n",
    "        }\n",
    "    \n",
    "    def _combine_predictions(self, predictions: Dict[str, Dict[str, float]]) -> Tuple[SentimentLabel, float, Dict[str, float]]:\n",
    "        \"\"\"Combinar predicciones de m√∫ltiples modelos\"\"\"\n",
    "        \n",
    "        if not predictions:\n",
    "            return SentimentLabel.NEUTRAL, 0.0, {}\n",
    "        \n",
    "        # Inicializar scores combinados\n",
    "        combined_scores = {'positive': 0.0, 'negative': 0.0, 'neutral': 0.0}\n",
    "        total_weight = 0\n",
    "        \n",
    "        # Pesos por modelo\n",
    "        model_weights = {\n",
    "            'bert': 0.4,\n",
    "            'lstm': 0.3,\n",
    "            'traditional': 0.2,\n",
    "            'vader': 0.1\n",
    "        }\n",
    "        \n",
    "        # Combinar scores\n",
    "        for model_name, model_predictions in predictions.items():\n",
    "            weight = model_weights.get(model_name, 0.1)\n",
    "            \n",
    "            for sentiment, score in model_predictions.items():\n",
    "                if sentiment in combined_scores:\n",
    "                    combined_scores[sentiment] += score * weight\n",
    "                    \n",
    "            total_weight += weight\n",
    "        \n",
    "        # Normalizar scores\n",
    "        if total_weight > 0:\n",
    "            for sentiment in combined_scores:\n",
    "                combined_scores[sentiment] /= total_weight\n",
    "        \n",
    "        # Determinar sentimiento final\n",
    "        max_sentiment = max(combined_scores, key=combined_scores.get)\n",
    "        confidence = combined_scores[max_sentiment]\n",
    "        \n",
    "        # Mapear a enum\n",
    "        sentiment_mapping = {\n",
    "            'positive': SentimentLabel.POSITIVE,\n",
    "            'negative': SentimentLabel.NEGATIVE,\n",
    "            'neutral': SentimentLabel.NEUTRAL\n",
    "        }\n",
    "        \n",
    "        final_sentiment = sentiment_mapping.get(max_sentiment, SentimentLabel.NEUTRAL)\n",
    "        \n",
    "        return final_sentiment, confidence, combined_scores\n",
    "    \n",
    "    def _detect_emotion(self, text: str) -> Tuple[Optional[EmotionLabel], float]:\n",
    "        \"\"\"Detectar emociones espec√≠ficas en el texto\"\"\"\n",
    "        \n",
    "        # Diccionarios de palabras por emoci√≥n\n",
    "        emotion_words = {\n",
    "            EmotionLabel.JOY: ['happy', 'joy', 'excited', 'pleased', 'delighted', 'amazing', 'fantastic'],\n",
    "            EmotionLabel.ANGER: ['angry', 'mad', 'furious', 'annoyed', 'irritated', 'hate', 'disgusted'],\n",
    "            EmotionLabel.SADNESS: ['sad', 'disappointed', 'depressed', 'unhappy', 'miserable', 'terrible'],\n",
    "            EmotionLabel.FEAR: ['afraid', 'scared', 'worried', 'anxious', 'nervous', 'concerned'],\n",
    "            EmotionLabel.SURPRISE: ['surprised', 'shocked', 'amazed', 'unexpected', 'wow'],\n",
    "            EmotionLabel.LOVE: ['love', 'adore', 'cherish', 'wonderful', 'perfect', 'excellent'],\n",
    "            EmotionLabel.DISGUST: ['disgusting', 'awful', 'horrible', 'revolting', 'nasty']\n",
    "        }\n",
    "        \n",
    "        words = text.lower().split()\n",
    "        emotion_scores = {}\n",
    "        \n",
    "        for emotion, keywords in emotion_words.items():\n",
    "            score = sum(1 for word in words if word in keywords)\n",
    "            if score > 0:\n",
    "                emotion_scores[emotion] = score / len(words)  # Normalizar por longitud\n",
    "        \n",
    "        if emotion_scores:\n",
    "            max_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "            confidence = emotion_scores[max_emotion]\n",
    "            return max_emotion, confidence\n",
    "        \n",
    "        return None, 0.0\n",
    "    \n",
    "    def _extract_key_phrases(self, text: str) -> List[str]:\n",
    "        \"\"\"Extraer frases clave del texto\"\"\"\n",
    "        \n",
    "        # Tokenizar y lematizar\n",
    "        words = word_tokenize(text.lower())\n",
    "        words = [self.lemmatizer.lemmatize(word) for word in words \n",
    "                if word.isalpha() and word not in self.stop_words]\n",
    "        \n",
    "        # Encontrar bigramas frecuentes\n",
    "        from itertools import combinations\n",
    "        bigrams = [' '.join(combo) for combo in combinations(words, 2)]\n",
    "        \n",
    "        # Contar frecuencias\n",
    "        word_freq = Counter(words)\n",
    "        bigram_freq = Counter(bigrams)\n",
    "        \n",
    "        # Seleccionar top phrases\n",
    "        key_words = [word for word, freq in word_freq.most_common(5) if freq > 1]\n",
    "        key_bigrams = [bigram for bigram, freq in bigram_freq.most_common(3) if freq > 1]\n",
    "        \n",
    "        return key_words + key_bigrams\n",
    "    \n",
    "    def _analyze_aspects(self, text: str) -> Dict[str, SentimentLabel]:\n",
    "        \"\"\"An√°lisis de sentimientos por aspectos\"\"\"\n",
    "        \n",
    "        aspects_found = {}\n",
    "        words = text.lower().split()\n",
    "        \n",
    "        for aspect, keywords in self.aspect_keywords.items():\n",
    "            # Buscar menciones del aspecto\n",
    "            aspect_mentions = []\n",
    "            for i, word in enumerate(words):\n",
    "                if word in keywords:\n",
    "                    # Extraer contexto alrededor de la palabra\n",
    "                    start = max(0, i - 2)\n",
    "                    end = min(len(words), i + 3)\n",
    "                    context = ' '.join(words[start:end])\n",
    "                    aspect_mentions.append(context)\n",
    "            \n",
    "            if aspect_mentions:\n",
    "                # Analizar sentimiento del contexto\n",
    "                combined_context = ' '.join(aspect_mentions)\n",
    "                vader_scores = self.sentiment_analyzer.polarity_scores(combined_context)\n",
    "                \n",
    "                if vader_scores['compound'] > 0.1:\n",
    "                    aspects_found[aspect] = SentimentLabel.POSITIVE\n",
    "                elif vader_scores['compound'] < -0.1:\n",
    "                    aspects_found[aspect] = SentimentLabel.NEGATIVE\n",
    "                else:\n",
    "                    aspects_found[aspect] = SentimentLabel.NEUTRAL\n",
    "        \n",
    "        return aspects_found\n",
    "    \n",
    "    def _detect_language(self, text: str) -> str:\n",
    "        \"\"\"Detectar idioma del texto (simplificado)\"\"\"\n",
    "        \n",
    "        # Implementaci√≥n simplificada - en producci√≥n usar librer√≠as como langdetect\n",
    "        english_words = ['the', 'and', 'is', 'a', 'to', 'of', 'in', 'that', 'have']\n",
    "        spanish_words = ['el', 'la', 'y', 'es', 'un', 'de', 'en', 'que', 'tiene']\n",
    "        \n",
    "        words = text.lower().split()\n",
    "        \n",
    "        english_count = sum(1 for word in words if word in english_words)\n",
    "        spanish_count = sum(1 for word in words if word in spanish_words)\n",
    "        \n",
    "        if english_count > spanish_count:\n",
    "            return \"en\"\n",
    "        elif spanish_count > 0:\n",
    "            return \"es\"\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "    \n",
    "    def _create_empty_result(self, text: str) -> SentimentResult:\n",
    "        \"\"\"Crear resultado vac√≠o para texto inv√°lido\"\"\"\n",
    "        \n",
    "        return SentimentResult(\n",
    "            text_id=None,\n",
    "            text=text if text else \"\",\n",
    "            sentiment=SentimentLabel.NEUTRAL,\n",
    "            confidence=0.0,\n",
    "            emotion=None,\n",
    "            emotion_confidence=0.0,\n",
    "            scores={'positive': 0.0, 'negative': 0.0, 'neutral': 1.0},\n",
    "            key_phrases=[],\n",
    "            aspects={},\n",
    "            language=\"unknown\",\n",
    "            word_count=0,\n",
    "            model_used=\"none\",\n",
    "            timestamp=datetime.utcnow()\n",
    "        )\n",
    "    \n",
    "    def batch_analyze(self, texts: List[str], model_type: str = \"ensemble\") -> List[SentimentResult]:\n",
    "        \"\"\"Analizar sentimientos en lote\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            try:\n",
    "                result = self.analyze_sentiment(text, model_type)\n",
    "                result.text_id = str(i)\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error analizando texto {i}: {e}\")\n",
    "                results.append(self._create_empty_result(text))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_sentiment_summary(self, results: List[SentimentResult]) -> Dict[str, any]:\n",
    "        \"\"\"Obtener resumen de an√°lisis de sentimientos\"\"\"\n",
    "        \n",
    "        if not results:\n",
    "            return {}\n",
    "        \n",
    "        # Contar sentimientos\n",
    "        sentiment_counts = Counter([r.sentiment.value for r in results])\n",
    "        \n",
    "        # Promedio de confianza\n",
    "        avg_confidence = np.mean([r.confidence for r in results])\n",
    "        \n",
    "        # Emociones m√°s comunes\n",
    "        emotions = [r.emotion.value for r in results if r.emotion]\n",
    "        emotion_counts = Counter(emotions)\n",
    "        \n",
    "        # Aspectos m√°s mencionados\n",
    "        all_aspects = {}\n",
    "        for result in results:\n",
    "            for aspect, sentiment in result.aspects.items():\n",
    "                if aspect not in all_aspects:\n",
    "                    all_aspects[aspect] = []\n",
    "                all_aspects[aspect].append(sentiment.value)\n",
    "        \n",
    "        aspect_summary = {\n",
    "            aspect: Counter(sentiments).most_common(1)[0][0] \n",
    "            for aspect, sentiments in all_aspects.items()\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'total_texts': len(results),\n",
    "            'sentiment_distribution': dict(sentiment_counts),\n",
    "            'average_confidence': avg_confidence,\n",
    "            'top_emotions': dict(emotion_counts.most_common(5)),\n",
    "            'aspect_sentiments': aspect_summary,\n",
    "            'overall_sentiment': sentiment_counts.most_common(1)[0][0] if sentiment_counts else 'neutral'\n",
    "        }\n",
    "    \n",
    "    def save_models(self, model_path: str):\n",
    "        \"\"\"Guardar modelos de an√°lisis de sentimientos\"\"\"\n",
    "        \n",
    "        # Guardar modelos tradicionales\n",
    "        joblib.dump(self.models, f\"{model_path}/sentiment_models.pkl\")\n",
    "        joblib.dump(self.vectorizers, f\"{model_path}/sentiment_vectorizers.pkl\")\n",
    "        joblib.dump(self.tokenizers, f\"{model_path}/sentiment_tokenizers.pkl\")\n",
    "        \n",
    "        # Guardar modelo LSTM\n",
    "        if self.lstm_model:\n",
    "            self.lstm_model.save(f\"{model_path}/lstm_sentiment.h5\")\n",
    "        \n",
    "        self.logger.info(f\"Modelos de an√°lisis de sentimientos guardados en {model_path}\")\n",
    "    \n",
    "    def load_models(self, model_path: str):\n",
    "        \"\"\"Cargar modelos de an√°lisis de sentimientos\"\"\"\n",
    "        \n",
    "        try:\n",
    "            self.models = joblib.load(f\"{model_path}/sentiment_models.pkl\")\n",
    "            self.vectorizers = joblib.load(f\"{model_path}/sentiment_vectorizers.pkl\")\n",
    "            self.tokenizers = joblib.load(f\"{model_path}/sentiment_tokenizers.pkl\")\n",
    "            \n",
    "            # Cargar modelo LSTM\n",
    "            try:\n",
    "                self.lstm_model = load_model(f\"{model_path}/lstm_sentiment.h5\")\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            \n",
    "            self.logger.info(f\"Modelos de an√°lisis de sentimientos cargados desde {model_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error cargando modelos de sentimientos: {e}\")\n",
    "            raise\n",
    "\n",
    "# Factory function\n",
    "def create_sentiment_analyzer() -> AdvancedSentimentAnalyzer:\n",
    "    \"\"\"Factory para crear instancia del analizador de sentimientos\"\"\"\n",
    "    return AdvancedSentimentAnalyzer()\n",
    "'''\n",
    "\n",
    "# Escribir sentiment_analyzer.py\n",
    "with open(\"../app/models/sentiment_analyzer.py\", \"w\") as f:\n",
    "    f.write(sentiment_analyzer_content)\n",
    "\n",
    "print(\"‚úÖ sentiment_analyzer.py creado exitosamente\")\n",
    "print(\"üß† Analizador de sentimientos implementado:\")\n",
    "print(\"   ‚Ä¢ BERT: Transformer preentrenado para an√°lisis avanzado\")\n",
    "print(\"   ‚Ä¢ LSTM: Red neuronal recurrente para secuencias de texto\")\n",
    "print(\"   ‚Ä¢ Traditional ML: Logistic Regression, Random Forest, SVM\")\n",
    "print(\"   ‚Ä¢ VADER: Analizador lexical especializado en redes sociales\")\n",
    "print(\"   ‚Ä¢ Ensemble: Combinaci√≥n inteligente de m√∫ltiples modelos\")\n",
    "print(\"   ‚Ä¢ Emotion Detection: Identificaci√≥n de emociones espec√≠ficas\")\n",
    "print(\"   ‚Ä¢ Aspect-based Analysis: Sentimientos por aspectos del producto\")\n",
    "print(\"   ‚Ä¢ Key Phrase Extraction: Identificaci√≥n de frases importantes\")\n",
    "print(\"   ‚Ä¢ Language Detection: Identificaci√≥n autom√°tica de idioma\")\n",
    "print(\"   ‚Ä¢ Batch Processing: An√°lisis masivo de textos\")\n",
    "print(\"   ‚Ä¢ Comprehensive Reporting: Res√∫menes y estad√≠sticas detalladas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5800e7",
   "metadata": {},
   "source": [
    "## üîß 9. Servicios de Negocio (Business Services)\n",
    "Implementaci√≥n de la l√≥gica de negocio que orquesta los modelos ML y maneja las operaciones complejas del microservicio. Incluye caching, validaci√≥n, orchestration y patrones empresariales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bc22c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ml_service.py creado exitosamente\n",
      "üîß Servicio de orquestaci√≥n ML implementado:\n",
      "   ‚Ä¢ Orchestration: Coordinaci√≥n de todos los modelos ML\n",
      "   ‚Ä¢ Caching: Redis para optimizaci√≥n de performance\n",
      "   ‚Ä¢ Async Operations: Operaciones as√≠ncronas para escalabilidad\n",
      "   ‚Ä¢ Batch Processing: Procesamiento en lotes\n",
      "   ‚Ä¢ Error Handling: Manejo robusto de errores\n",
      "   ‚Ä¢ Performance Metrics: M√©tricas y monitoreo\n",
      "   ‚Ä¢ Health Checks: Endpoints de salud\n",
      "   ‚Ä¢ Comprehensive Analysis: An√°lisis multi-modelo\n",
      "   ‚Ä¢ Data Abstraction: Capa de abstracci√≥n de datos\n",
      "   ‚Ä¢ Enterprise Patterns: Patrones empresariales\n"
     ]
    }
   ],
   "source": [
    "# ml_service.py - Servicio principal de ML orchestration\n",
    "ml_service_content = '''\n",
    "\"\"\"\n",
    "Servicio principal de ML que orquesta todos los modelos y algoritmos\n",
    "Maneja caching, validaci√≥n, orchestration y patrones empresariales\n",
    "\"\"\"\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Any, Union\n",
    "from dataclasses import asdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# FastAPI and async\n",
    "from fastapi import HTTPException\n",
    "import aioredis\n",
    "from sqlalchemy.ext.asyncio import AsyncSession\n",
    "from sqlalchemy import select, and_, func\n",
    "\n",
    "# Internal imports\n",
    "from ..database import get_async_session\n",
    "from ..models.stock_predictor import StockPredictor, PredictionResult\n",
    "from ..models.recommender import HybridRecommender, RecommendationResult\n",
    "from ..models.price_optimizer import DynamicPriceOptimizer, PriceOptimizationResult\n",
    "from ..models.anomaly_detector import AdvancedAnomalyDetector, AnomalyResult\n",
    "from ..models.sentiment_analyzer import AdvancedSentimentAnalyzer, SentimentResult\n",
    "from ..schemas.ml_schemas import *\n",
    "from ..config import settings\n",
    "\n",
    "class MLOrchestrationService:\n",
    "    \"\"\"Servicio principal de orquestaci√≥n de ML\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.redis_client = None\n",
    "        self.cache_ttl = 3600  # 1 hora\n",
    "        \n",
    "        # ML Models\n",
    "        self.stock_predictor = None\n",
    "        self.recommender = None\n",
    "        self.price_optimizer = None\n",
    "        self.anomaly_detector = None\n",
    "        self.sentiment_analyzer = None\n",
    "        \n",
    "        # Performance metrics\n",
    "        self.metrics = {\n",
    "            'predictions_made': 0,\n",
    "            'recommendations_generated': 0,\n",
    "            'anomalies_detected': 0,\n",
    "            'cache_hits': 0,\n",
    "            'cache_misses': 0\n",
    "        }\n",
    "    \n",
    "    async def initialize(self):\n",
    "        \"\"\"Inicializar servicio y conexiones\"\"\"\n",
    "        \n",
    "        # Initialize Redis\n",
    "        try:\n",
    "            self.redis_client = aioredis.from_url(\n",
    "                f\"redis://{settings.redis_host}:{settings.redis_port}\",\n",
    "                decode_responses=True\n",
    "            )\n",
    "            await self.redis_client.ping()\n",
    "            self.logger.info(\"Conexi√≥n a Redis establecida\")\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Redis no disponible: {e}\")\n",
    "            self.redis_client = None\n",
    "        \n",
    "        # Initialize ML models\n",
    "        await self._initialize_ml_models()\n",
    "        \n",
    "        self.logger.info(\"MLOrchestrationService inicializado exitosamente\")\n",
    "    \n",
    "    async def _initialize_ml_models(self):\n",
    "        \"\"\"Inicializar modelos de ML\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Stock Predictor\n",
    "            self.stock_predictor = StockPredictor()\n",
    "            \n",
    "            # Recommender\n",
    "            self.recommender = HybridRecommender()\n",
    "            \n",
    "            # Price Optimizer\n",
    "            self.price_optimizer = DynamicPriceOptimizer()\n",
    "            \n",
    "            # Anomaly Detector\n",
    "            self.anomaly_detector = AdvancedAnomalyDetector()\n",
    "            \n",
    "            # Sentiment Analyzer\n",
    "            self.sentiment_analyzer = AdvancedSentimentAnalyzer()\n",
    "            \n",
    "            self.logger.info(\"Modelos ML inicializados\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error inicializando modelos ML: {e}\")\n",
    "            raise\n",
    "    \n",
    "    async def _get_cache_key(self, prefix: str, **kwargs) -> str:\n",
    "        \"\"\"Generar clave de cache\"\"\"\n",
    "        \n",
    "        key_parts = [prefix]\n",
    "        for k, v in sorted(kwargs.items()):\n",
    "            if isinstance(v, (dict, list)):\n",
    "                v = json.dumps(v, sort_keys=True)\n",
    "            key_parts.append(f\"{k}:{v}\")\n",
    "        \n",
    "        return \":\".join(key_parts)\n",
    "    \n",
    "    async def _get_from_cache(self, cache_key: str) -> Optional[Dict]:\n",
    "        \"\"\"Obtener datos del cache\"\"\"\n",
    "        \n",
    "        if not self.redis_client:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            cached_data = await self.redis_client.get(cache_key)\n",
    "            if cached_data:\n",
    "                self.metrics['cache_hits'] += 1\n",
    "                return json.loads(cached_data)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error obteniendo cache: {e}\")\n",
    "        \n",
    "        self.metrics['cache_misses'] += 1\n",
    "        return None\n",
    "    \n",
    "    async def _set_cache(self, cache_key: str, data: Dict, ttl: int = None):\n",
    "        \"\"\"Guardar datos en cache\"\"\"\n",
    "        \n",
    "        if not self.redis_client:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            ttl = ttl or self.cache_ttl\n",
    "            await self.redis_client.setex(\n",
    "                cache_key, \n",
    "                ttl, \n",
    "                json.dumps(data, default=str)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error guardando cache: {e}\")\n",
    "    \n",
    "    # Stock Prediction Services\n",
    "    async def predict_stock_demand(self, \n",
    "                                 product_id: int,\n",
    "                                 days_ahead: int = 30,\n",
    "                                 include_confidence_intervals: bool = True) -> PredictionResult:\n",
    "        \"\"\"Predecir demanda de stock para un producto\"\"\"\n",
    "        \n",
    "        cache_key = await self._get_cache_key(\n",
    "            \"stock_prediction\",\n",
    "            product_id=product_id,\n",
    "            days_ahead=days_ahead\n",
    "        )\n",
    "        \n",
    "        # Check cache\n",
    "        cached_result = await self._get_from_cache(cache_key)\n",
    "        if cached_result:\n",
    "            return PredictionResult(**cached_result)\n",
    "        \n",
    "        try:\n",
    "            # Obtener datos hist√≥ricos\n",
    "            historical_data = await self._get_historical_stock_data(product_id)\n",
    "            \n",
    "            if historical_data.empty:\n",
    "                raise HTTPException(\n",
    "                    status_code=404,\n",
    "                    detail=f\"No hay datos hist√≥ricos para producto {product_id}\"\n",
    "                )\n",
    "            \n",
    "            # Hacer predicci√≥n\n",
    "            result = self.stock_predictor.predict_demand(\n",
    "                product_id=product_id,\n",
    "                historical_data=historical_data,\n",
    "                forecast_periods=days_ahead,\n",
    "                confidence_intervals=include_confidence_intervals\n",
    "            )\n",
    "            \n",
    "            # Cache result\n",
    "            await self._set_cache(cache_key, asdict(result))\n",
    "            \n",
    "            self.metrics['predictions_made'] += 1\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error en predicci√≥n de stock: {e}\")\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "    \n",
    "    async def predict_stock_batch(self, \n",
    "                                product_ids: List[int],\n",
    "                                days_ahead: int = 30) -> List[PredictionResult]:\n",
    "        \"\"\"Predicci√≥n de stock en lote\"\"\"\n",
    "        \n",
    "        tasks = []\n",
    "        for product_id in product_ids:\n",
    "            task = self.predict_stock_demand(product_id, days_ahead)\n",
    "            tasks.append(task)\n",
    "        \n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        # Filter out exceptions\n",
    "        valid_results = [r for r in results if isinstance(r, PredictionResult)]\n",
    "        return valid_results\n",
    "    \n",
    "    # Recommendation Services\n",
    "    async def get_user_recommendations(self,\n",
    "                                     user_id: int,\n",
    "                                     num_recommendations: int = 10,\n",
    "                                     algorithm: str = 'hybrid') -> RecommendationResult:\n",
    "        \"\"\"Obtener recomendaciones para usuario\"\"\"\n",
    "        \n",
    "        cache_key = await self._get_cache_key(\n",
    "            \"user_recommendations\",\n",
    "            user_id=user_id,\n",
    "            num_recommendations=num_recommendations,\n",
    "            algorithm=algorithm\n",
    "        )\n",
    "        \n",
    "        cached_result = await self._get_from_cache(cache_key)\n",
    "        if cached_result:\n",
    "            return RecommendationResult(**cached_result)\n",
    "        \n",
    "        try:\n",
    "            # Obtener datos de usuario e interacciones\n",
    "            user_data = await self._get_user_interaction_data(user_id)\n",
    "            \n",
    "            # Generar recomendaciones\n",
    "            result = self.recommender.get_user_recommendations(\n",
    "                user_id=user_id,\n",
    "                top_k=num_recommendations,\n",
    "                algorithm=algorithm\n",
    "            )\n",
    "            \n",
    "            # Cache result\n",
    "            await self._set_cache(cache_key, asdict(result), ttl=1800)  # 30 min\n",
    "            \n",
    "            self.metrics['recommendations_generated'] += 1\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error en recomendaciones: {e}\")\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "    \n",
    "    async def get_similar_products(self,\n",
    "                                 product_id: int,\n",
    "                                 num_similar: int = 10) -> RecommendationResult:\n",
    "        \"\"\"Obtener productos similares\"\"\"\n",
    "        \n",
    "        cache_key = await self._get_cache_key(\n",
    "            \"similar_products\",\n",
    "            product_id=product_id,\n",
    "            num_similar=num_similar\n",
    "        )\n",
    "        \n",
    "        cached_result = await self._get_from_cache(cache_key)\n",
    "        if cached_result:\n",
    "            return RecommendationResult(**cached_result)\n",
    "        \n",
    "        try:\n",
    "            result = self.recommender.get_similar_products(\n",
    "                product_id=product_id,\n",
    "                top_k=num_similar\n",
    "            )\n",
    "            \n",
    "            await self._set_cache(cache_key, asdict(result))\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error en productos similares: {e}\")\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "    \n",
    "    # Price Optimization Services\n",
    "    async def optimize_product_price(self,\n",
    "                                   product_id: int,\n",
    "                                   current_price: float,\n",
    "                                   strategy: str = 'dynamic') -> PriceOptimizationResult:\n",
    "        \"\"\"Optimizar precio de producto\"\"\"\n",
    "        \n",
    "        cache_key = await self._get_cache_key(\n",
    "            \"price_optimization\",\n",
    "            product_id=product_id,\n",
    "            current_price=current_price,\n",
    "            strategy=strategy\n",
    "        )\n",
    "        \n",
    "        cached_result = await self._get_from_cache(cache_key)\n",
    "        if cached_result:\n",
    "            return PriceOptimizationResult(**cached_result)\n",
    "        \n",
    "        try:\n",
    "            # Obtener caracter√≠sticas del producto\n",
    "            product_features = await self._get_product_features(product_id)\n",
    "            \n",
    "            # Optimizar precio\n",
    "            result = self.price_optimizer.optimize_price(\n",
    "                product_id=product_id,\n",
    "                current_price=current_price,\n",
    "                product_features=product_features,\n",
    "                strategy=strategy\n",
    "            )\n",
    "            \n",
    "            await self._set_cache(cache_key, asdict(result), ttl=7200)  # 2 hours\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error en optimizaci√≥n de precios: {e}\")\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "    \n",
    "    async def optimize_prices_batch(self,\n",
    "                                  products_data: List[Dict]) -> List[PriceOptimizationResult]:\n",
    "        \"\"\"Optimizaci√≥n de precios en lote\"\"\"\n",
    "        \n",
    "        try:\n",
    "            results = self.price_optimizer.batch_optimize_prices(products_data)\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error en optimizaci√≥n batch: {e}\")\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "    \n",
    "    # Anomaly Detection Services\n",
    "    async def detect_transaction_anomalies(self,\n",
    "                                         transactions_data: List[Dict]) -> List[AnomalyResult]:\n",
    "        \"\"\"Detectar anomal√≠as en transacciones\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Convertir a DataFrame\n",
    "            df = pd.DataFrame(transactions_data)\n",
    "            \n",
    "            # Detectar anomal√≠as\n",
    "            results = self.anomaly_detector.detect_anomalies(\n",
    "                data=df,\n",
    "                entity_type='transaction'\n",
    "            )\n",
    "            \n",
    "            self.metrics['anomalies_detected'] += len(results)\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error en detecci√≥n de anomal√≠as: {e}\")\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "    \n",
    "    async def detect_user_behavior_anomalies(self,\n",
    "                                           user_id: int,\n",
    "                                           time_window_days: int = 30) -> List[AnomalyResult]:\n",
    "        \"\"\"Detectar anomal√≠as en comportamiento de usuario\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Obtener datos de comportamiento\n",
    "            user_behavior_data = await self._get_user_behavior_data(user_id, time_window_days)\n",
    "            \n",
    "            if user_behavior_data.empty:\n",
    "                return []\n",
    "            \n",
    "            results = self.anomaly_detector.detect_anomalies(\n",
    "                data=user_behavior_data,\n",
    "                entity_type='user'\n",
    "            )\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error en anomal√≠as de usuario: {e}\")\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "    \n",
    "    # Sentiment Analysis Services\n",
    "    async def analyze_product_sentiment(self,\n",
    "                                      product_id: int,\n",
    "                                      reviews_text: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Analizar sentimientos de rese√±as de producto\"\"\"\n",
    "        \n",
    "        cache_key = await self._get_cache_key(\n",
    "            \"product_sentiment\",\n",
    "            product_id=product_id,\n",
    "            reviews_hash=hash(tuple(reviews_text))\n",
    "        )\n",
    "        \n",
    "        cached_result = await self._get_from_cache(cache_key)\n",
    "        if cached_result:\n",
    "            return cached_result\n",
    "        \n",
    "        try:\n",
    "            # Analizar sentimientos\n",
    "            sentiment_results = self.sentiment_analyzer.batch_analyze(reviews_text)\n",
    "            \n",
    "            # Generar resumen\n",
    "            summary = self.sentiment_analyzer.get_sentiment_summary(sentiment_results)\n",
    "            \n",
    "            result = {\n",
    "                'product_id': product_id,\n",
    "                'total_reviews': len(reviews_text),\n",
    "                'sentiment_summary': summary,\n",
    "                'detailed_results': [asdict(r) for r in sentiment_results]\n",
    "            }\n",
    "            \n",
    "            await self._set_cache(cache_key, result)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error en an√°lisis de sentimientos: {e}\")\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "    \n",
    "    async def analyze_text_sentiment(self,\n",
    "                                   text: str,\n",
    "                                   model_type: str = \"ensemble\") -> SentimentResult:\n",
    "        \"\"\"Analizar sentimiento de texto individual\"\"\"\n",
    "        \n",
    "        try:\n",
    "            result = self.sentiment_analyzer.analyze_sentiment(text, model_type)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error en an√°lisis de texto: {e}\")\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "    \n",
    "    # Comprehensive Analysis Services\n",
    "    async def get_product_insights(self,\n",
    "                                 product_id: int,\n",
    "                                 days_back: int = 90) -> Dict[str, Any]:\n",
    "        \"\"\"Obtener insights comprehensivos de producto\"\"\"\n",
    "        \n",
    "        cache_key = await self._get_cache_key(\n",
    "            \"product_insights\",\n",
    "            product_id=product_id,\n",
    "            days_back=days_back\n",
    "        )\n",
    "        \n",
    "        cached_result = await self._get_from_cache(cache_key)\n",
    "        if cached_result:\n",
    "            return cached_result\n",
    "        \n",
    "        try:\n",
    "            # Ejecutar an√°lisis en paralelo\n",
    "            tasks = [\n",
    "                self.predict_stock_demand(product_id, 30),\n",
    "                self.get_similar_products(product_id, 5),\n",
    "                self._get_product_price_optimization(product_id),\n",
    "                self._get_product_reviews_sentiment(product_id, days_back)\n",
    "            ]\n",
    "            \n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            \n",
    "            insights = {\n",
    "                'product_id': product_id,\n",
    "                'analysis_date': datetime.utcnow().isoformat(),\n",
    "                'stock_prediction': results[0] if not isinstance(results[0], Exception) else None,\n",
    "                'similar_products': results[1] if not isinstance(results[1], Exception) else None,\n",
    "                'price_optimization': results[2] if not isinstance(results[2], Exception) else None,\n",
    "                'sentiment_analysis': results[3] if not isinstance(results[3], Exception) else None\n",
    "            }\n",
    "            \n",
    "            await self._set_cache(cache_key, insights, ttl=7200)\n",
    "            return insights\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error en insights de producto: {e}\")\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "    \n",
    "    async def get_user_profile_analysis(self,\n",
    "                                      user_id: int) -> Dict[str, Any]:\n",
    "        \"\"\"An√°lisis comprehensivo de perfil de usuario\"\"\"\n",
    "        \n",
    "        try:\n",
    "            tasks = [\n",
    "                self.get_user_recommendations(user_id, 10),\n",
    "                self.detect_user_behavior_anomalies(user_id, 30),\n",
    "                self._get_user_purchase_patterns(user_id),\n",
    "                self._get_user_sentiment_profile(user_id)\n",
    "            ]\n",
    "            \n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            \n",
    "            profile_analysis = {\n",
    "                'user_id': user_id,\n",
    "                'analysis_date': datetime.utcnow().isoformat(),\n",
    "                'recommendations': results[0] if not isinstance(results[0], Exception) else None,\n",
    "                'behavior_anomalies': results[1] if not isinstance(results[1], Exception) else [],\n",
    "                'purchase_patterns': results[2] if not isinstance(results[2], Exception) else None,\n",
    "                'sentiment_profile': results[3] if not isinstance(results[3], Exception) else None\n",
    "            }\n",
    "            \n",
    "            return profile_analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error en an√°lisis de usuario: {e}\")\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "    \n",
    "    # Data Retrieval Methods\n",
    "    async def _get_historical_stock_data(self, product_id: int) -> pd.DataFrame:\n",
    "        \"\"\"Obtener datos hist√≥ricos de stock\"\"\"\n",
    "        \n",
    "        async with get_async_session() as session:\n",
    "            # Placeholder query - adjust based on your schema\n",
    "            query = select(\"*\").where(\"product_id = :product_id\")\n",
    "            result = await session.execute(query, {\"product_id\": product_id})\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            data = result.fetchall()\n",
    "            if data:\n",
    "                return pd.DataFrame(data)\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    async def _get_user_interaction_data(self, user_id: int) -> pd.DataFrame:\n",
    "        \"\"\"Obtener datos de interacci√≥n de usuario\"\"\"\n",
    "        \n",
    "        # Placeholder - implement based on your schema\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    async def _get_product_features(self, product_id: int) -> Dict[str, Any]:\n",
    "        \"\"\"Obtener caracter√≠sticas de producto\"\"\"\n",
    "        \n",
    "        # Placeholder - implement based on your schema\n",
    "        return {\n",
    "            'cost': 50.0,\n",
    "            'category': 'electronics',\n",
    "            'brand': 'generic'\n",
    "        }\n",
    "    \n",
    "    async def _get_user_behavior_data(self, user_id: int, days_back: int) -> pd.DataFrame:\n",
    "        \"\"\"Obtener datos de comportamiento de usuario\"\"\"\n",
    "        \n",
    "        # Placeholder - implement based on your schema\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    async def _get_product_price_optimization(self, product_id: int) -> Optional[Dict]:\n",
    "        \"\"\"Obtener optimizaci√≥n de precio para producto\"\"\"\n",
    "        \n",
    "        try:\n",
    "            current_price = 100.0  # Placeholder - get from database\n",
    "            result = await self.optimize_product_price(product_id, current_price)\n",
    "            return asdict(result)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    async def _get_product_reviews_sentiment(self, product_id: int, days_back: int) -> Optional[Dict]:\n",
    "        \"\"\"Obtener an√°lisis de sentimientos de rese√±as\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Placeholder - get reviews from database\n",
    "            reviews = [\"Great product!\", \"Poor quality\", \"Average experience\"]\n",
    "            result = await self.analyze_product_sentiment(product_id, reviews)\n",
    "            return result\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    async def _get_user_purchase_patterns(self, user_id: int) -> Optional[Dict]:\n",
    "        \"\"\"Obtener patrones de compra de usuario\"\"\"\n",
    "        \n",
    "        # Placeholder - implement based on your schema\n",
    "        return {\n",
    "            'avg_order_value': 75.5,\n",
    "            'purchase_frequency': 'monthly',\n",
    "            'preferred_categories': ['electronics', 'books']\n",
    "        }\n",
    "    \n",
    "    async def _get_user_sentiment_profile(self, user_id: int) -> Optional[Dict]:\n",
    "        \"\"\"Obtener perfil de sentimientos de usuario\"\"\"\n",
    "        \n",
    "        # Placeholder - implement based on your schema\n",
    "        return {\n",
    "            'overall_satisfaction': 'positive',\n",
    "            'review_sentiment_avg': 0.7,\n",
    "            'complaint_frequency': 'low'\n",
    "        }\n",
    "    \n",
    "    # Health and Metrics\n",
    "    async def get_service_health(self) -> Dict[str, Any]:\n",
    "        \"\"\"Obtener estado de salud del servicio\"\"\"\n",
    "        \n",
    "        health_status = {\n",
    "            'status': 'healthy',\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'models_loaded': {\n",
    "                'stock_predictor': self.stock_predictor is not None,\n",
    "                'recommender': self.recommender is not None,\n",
    "                'price_optimizer': self.price_optimizer is not None,\n",
    "                'anomaly_detector': self.anomaly_detector is not None,\n",
    "                'sentiment_analyzer': self.sentiment_analyzer is not None\n",
    "            },\n",
    "            'redis_connected': self.redis_client is not None,\n",
    "            'performance_metrics': self.metrics.copy()\n",
    "        }\n",
    "        \n",
    "        return health_status\n",
    "    \n",
    "    async def get_performance_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Obtener m√©tricas de performance\"\"\"\n",
    "        \n",
    "        return {\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'metrics': self.metrics.copy(),\n",
    "            'cache_hit_ratio': (\n",
    "                self.metrics['cache_hits'] / \n",
    "                (self.metrics['cache_hits'] + self.metrics['cache_misses'])\n",
    "                if (self.metrics['cache_hits'] + self.metrics['cache_misses']) > 0 \n",
    "                else 0\n",
    "            )\n",
    "        }\n",
    "\n",
    "# Singleton instance\n",
    "ml_service = MLOrchestrationService()\n",
    "'''\n",
    "\n",
    "# Escribir ml_service.py\n",
    "with open(\"../app/services/ml_service.py\", \"w\") as f:\n",
    "    f.write(ml_service_content)\n",
    "\n",
    "print(\"‚úÖ ml_service.py creado exitosamente\")\n",
    "print(\"üîß Servicio de orquestaci√≥n ML implementado:\")\n",
    "print(\"   ‚Ä¢ Orchestration: Coordinaci√≥n de todos los modelos ML\")\n",
    "print(\"   ‚Ä¢ Caching: Redis para optimizaci√≥n de performance\")\n",
    "print(\"   ‚Ä¢ Async Operations: Operaciones as√≠ncronas para escalabilidad\")\n",
    "print(\"   ‚Ä¢ Batch Processing: Procesamiento en lotes\")\n",
    "print(\"   ‚Ä¢ Error Handling: Manejo robusto de errores\")\n",
    "print(\"   ‚Ä¢ Performance Metrics: M√©tricas y monitoreo\")\n",
    "print(\"   ‚Ä¢ Health Checks: Endpoints de salud\")\n",
    "print(\"   ‚Ä¢ Comprehensive Analysis: An√°lisis multi-modelo\")\n",
    "print(\"   ‚Ä¢ Data Abstraction: Capa de abstracci√≥n de datos\")\n",
    "print(\"   ‚Ä¢ Enterprise Patterns: Patrones empresariales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fdb568",
   "metadata": {},
   "source": [
    "## üìã 10. Esquemas Pydantic (Data Validation)\n",
    "Esquemas de validaci√≥n de datos para APIs usando Pydantic. Incluye modelos de request/response, validaci√≥n empresarial y serializaci√≥n autom√°tica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "672639a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ml_schemas.py creado exitosamente\n",
      "üìã Esquemas Pydantic implementados:\n",
      "   ‚Ä¢ Request/Response Models: Validaci√≥n completa de entrada y salida\n",
      "   ‚Ä¢ Enterprise Validation: Reglas de negocio y validaci√≥n empresarial\n",
      "   ‚Ä¢ Type Safety: Tipos estrictos con validaci√≥n autom√°tica\n",
      "   ‚Ä¢ Documentation: Documentaci√≥n autom√°tica de API\n",
      "   ‚Ä¢ Error Handling: Modelos de error estandarizados\n",
      "   ‚Ä¢ Pagination: Soporte para paginaci√≥n est√°ndar\n",
      "   ‚Ä¢ Filtering: Par√°metros de filtrado y b√∫squeda\n",
      "   ‚Ä¢ Webhooks: Configuraci√≥n de notificaciones\n",
      "   ‚Ä¢ Model Management: Esquemas para gesti√≥n de modelos\n",
      "   ‚Ä¢ Comprehensive Coverage: Todos los endpoints cubiertos\n"
     ]
    }
   ],
   "source": [
    "# ml_schemas.py - Esquemas Pydantic para validaci√≥n de datos\n",
    "ml_schemas_content = '''\n",
    "\"\"\"\n",
    "Esquemas Pydantic para validaci√≥n y serializaci√≥n de datos del microservicio ML\n",
    "Incluye request/response models, validaci√≥n empresarial y documentaci√≥n autom√°tica\n",
    "\"\"\"\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field, validator, root_validator\n",
    "import re\n",
    "\n",
    "# Base Models\n",
    "class BaseResponse(BaseModel):\n",
    "    \"\"\"Modelo base para todas las respuestas\"\"\"\n",
    "    success: bool = True\n",
    "    timestamp: datetime = Field(default_factory=datetime.utcnow)\n",
    "    message: Optional[str] = None\n",
    "\n",
    "class ErrorResponse(BaseResponse):\n",
    "    \"\"\"Modelo para respuestas de error\"\"\"\n",
    "    success: bool = False\n",
    "    error_code: str\n",
    "    error_details: Optional[Dict[str, Any]] = None\n",
    "\n",
    "# Enums\n",
    "class SentimentType(str, Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEGATIVE = \"negative\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    MIXED = \"mixed\"\n",
    "\n",
    "class AnomalyType(str, Enum):\n",
    "    FRAUD = \"fraud\"\n",
    "    OUTLIER = \"outlier\"\n",
    "    BEHAVIORAL = \"behavioral\"\n",
    "    INVENTORY = \"inventory\"\n",
    "    PRICE = \"price\"\n",
    "    PATTERN = \"pattern\"\n",
    "\n",
    "class PricingStrategy(str, Enum):\n",
    "    PENETRATION = \"penetration\"\n",
    "    SKIMMING = \"skimming\"\n",
    "    COMPETITIVE = \"competitive\"\n",
    "    DYNAMIC = \"dynamic\"\n",
    "    VALUE_BASED = \"value_based\"\n",
    "\n",
    "class RecommendationAlgorithm(str, Enum):\n",
    "    COLLABORATIVE = \"collaborative\"\n",
    "    CONTENT = \"content\"\n",
    "    HYBRID = \"hybrid\"\n",
    "    NEURAL = \"neural\"\n",
    "\n",
    "# Stock Prediction Schemas\n",
    "class StockPredictionRequest(BaseModel):\n",
    "    \"\"\"Request para predicci√≥n de stock\"\"\"\n",
    "    product_id: int = Field(..., gt=0, description=\"ID del producto\")\n",
    "    days_ahead: int = Field(30, ge=1, le=365, description=\"D√≠as a predecir\")\n",
    "    include_confidence_intervals: bool = Field(True, description=\"Incluir intervalos de confianza\")\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"product_id\": 123,\n",
    "                \"days_ahead\": 30,\n",
    "                \"include_confidence_intervals\": True\n",
    "            }\n",
    "        }\n",
    "\n",
    "class StockPredictionBatchRequest(BaseModel):\n",
    "    \"\"\"Request para predicci√≥n de stock en lote\"\"\"\n",
    "    product_ids: List[int] = Field(..., min_items=1, max_items=100)\n",
    "    days_ahead: int = Field(30, ge=1, le=365)\n",
    "    \n",
    "    @validator('product_ids')\n",
    "    def validate_product_ids(cls, v):\n",
    "        if not all(pid > 0 for pid in v):\n",
    "            raise ValueError('Todos los product_ids deben ser positivos')\n",
    "        return v\n",
    "\n",
    "class ConfidenceInterval(BaseModel):\n",
    "    \"\"\"Intervalo de confianza\"\"\"\n",
    "    lower_bound: float\n",
    "    upper_bound: float\n",
    "    confidence_level: float = Field(ge=0, le=1)\n",
    "\n",
    "class StockPredictionResponse(BaseResponse):\n",
    "    \"\"\"Respuesta de predicci√≥n de stock\"\"\"\n",
    "    product_id: int\n",
    "    predictions: List[float]\n",
    "    dates: List[str]\n",
    "    confidence_intervals: Optional[List[ConfidenceInterval]] = None\n",
    "    model_accuracy: float = Field(ge=0, le=1)\n",
    "    trend_analysis: Dict[str, Any]\n",
    "    risk_factors: List[str]\n",
    "\n",
    "# Recommendation Schemas\n",
    "class RecommendationRequest(BaseModel):\n",
    "    \"\"\"Request para recomendaciones de usuario\"\"\"\n",
    "    user_id: int = Field(..., gt=0)\n",
    "    num_recommendations: int = Field(10, ge=1, le=50)\n",
    "    algorithm: RecommendationAlgorithm = RecommendationAlgorithm.HYBRID\n",
    "    include_explanation: bool = True\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"user_id\": 456,\n",
    "                \"num_recommendations\": 10,\n",
    "                \"algorithm\": \"hybrid\",\n",
    "                \"include_explanation\": True\n",
    "            }\n",
    "        }\n",
    "\n",
    "class SimilarProductsRequest(BaseModel):\n",
    "    \"\"\"Request para productos similares\"\"\"\n",
    "    product_id: int = Field(..., gt=0)\n",
    "    num_similar: int = Field(10, ge=1, le=50)\n",
    "    similarity_threshold: float = Field(0.5, ge=0, le=1)\n",
    "\n",
    "class ProductRecommendation(BaseModel):\n",
    "    \"\"\"Recomendaci√≥n individual de producto\"\"\"\n",
    "    product_id: int\n",
    "    score: float = Field(ge=0, le=1)\n",
    "    reason: str\n",
    "    category: Optional[str] = None\n",
    "    price: Optional[float] = Field(None, ge=0)\n",
    "    confidence: float = Field(ge=0, le=1)\n",
    "\n",
    "class RecommendationResponse(BaseResponse):\n",
    "    \"\"\"Respuesta de recomendaciones\"\"\"\n",
    "    user_id: Optional[int] = None\n",
    "    product_id: Optional[int] = None\n",
    "    recommendations: List[ProductRecommendation]\n",
    "    algorithm_used: str\n",
    "    diversification_score: float = Field(ge=0, le=1)\n",
    "    explanation: str\n",
    "\n",
    "# Price Optimization Schemas\n",
    "class PriceOptimizationRequest(BaseModel):\n",
    "    \"\"\"Request para optimizaci√≥n de precios\"\"\"\n",
    "    product_id: int = Field(..., gt=0)\n",
    "    current_price: float = Field(..., gt=0)\n",
    "    strategy: PricingStrategy = PricingStrategy.DYNAMIC\n",
    "    constraints: Optional[Dict[str, float]] = None\n",
    "    \n",
    "    @validator('constraints')\n",
    "    def validate_constraints(cls, v):\n",
    "        if v:\n",
    "            if 'min_price' in v and 'max_price' in v:\n",
    "                if v['min_price'] >= v['max_price']:\n",
    "                    raise ValueError('min_price debe ser menor que max_price')\n",
    "        return v\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"product_id\": 789,\n",
    "                \"current_price\": 99.99,\n",
    "                \"strategy\": \"dynamic\",\n",
    "                \"constraints\": {\n",
    "                    \"min_price\": 80.0,\n",
    "                    \"max_price\": 120.0\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "class PriceBatchOptimizationRequest(BaseModel):\n",
    "    \"\"\"Request para optimizaci√≥n de precios en lote\"\"\"\n",
    "    products: List[Dict[str, Any]] = Field(..., min_items=1, max_items=100)\n",
    "    strategy: PricingStrategy = PricingStrategy.DYNAMIC\n",
    "\n",
    "class PriceOptimizationResponse(BaseResponse):\n",
    "    \"\"\"Respuesta de optimizaci√≥n de precios\"\"\"\n",
    "    product_id: int\n",
    "    current_price: float\n",
    "    optimal_price: float\n",
    "    price_change_percent: float\n",
    "    expected_revenue: float\n",
    "    expected_profit: float\n",
    "    demand_elasticity: float\n",
    "    confidence_score: float = Field(ge=0, le=1)\n",
    "    strategy_used: str\n",
    "    reasoning: str\n",
    "    market_conditions: Dict[str, Any]\n",
    "\n",
    "# Anomaly Detection Schemas\n",
    "class AnomalyDetectionRequest(BaseModel):\n",
    "    \"\"\"Request para detecci√≥n de anomal√≠as\"\"\"\n",
    "    data: List[Dict[str, Any]] = Field(..., min_items=1)\n",
    "    entity_type: str = Field(..., regex=\"^(transaction|user|product|inventory)$\")\n",
    "    detection_methods: Optional[List[str]] = None\n",
    "    sensitivity: float = Field(0.5, ge=0, le=1)\n",
    "\n",
    "class UserAnomalyRequest(BaseModel):\n",
    "    \"\"\"Request para anomal√≠as de usuario\"\"\"\n",
    "    user_id: int = Field(..., gt=0)\n",
    "    time_window_days: int = Field(30, ge=1, le=365)\n",
    "    include_patterns: bool = True\n",
    "\n",
    "class AnomalyResult(BaseModel):\n",
    "    \"\"\"Resultado de anomal√≠a detectada\"\"\"\n",
    "    entity_id: Union[int, str]\n",
    "    entity_type: str\n",
    "    anomaly_type: AnomalyType\n",
    "    severity: str = Field(..., regex=\"^(low|medium|high|critical)$\")\n",
    "    anomaly_score: float = Field(ge=0, le=1)\n",
    "    confidence: float = Field(ge=0, le=1)\n",
    "    description: str\n",
    "    anomalous_features: Dict[str, float]\n",
    "    detection_method: str\n",
    "    recommendations: List[str]\n",
    "\n",
    "class AnomalyDetectionResponse(BaseResponse):\n",
    "    \"\"\"Respuesta de detecci√≥n de anomal√≠as\"\"\"\n",
    "    total_entities_analyzed: int\n",
    "    anomalies_detected: int\n",
    "    anomaly_rate: float = Field(ge=0, le=1)\n",
    "    anomalies: List[AnomalyResult]\n",
    "    patterns_detected: Optional[List[Dict[str, Any]]] = None\n",
    "\n",
    "# Sentiment Analysis Schemas\n",
    "class SentimentAnalysisRequest(BaseModel):\n",
    "    \"\"\"Request para an√°lisis de sentimientos\"\"\"\n",
    "    text: str = Field(..., min_length=1, max_length=10000)\n",
    "    model_type: str = Field(\"ensemble\", regex=\"^(bert|lstm|traditional|ensemble|vader)$\")\n",
    "    include_emotions: bool = True\n",
    "    language: Optional[str] = Field(None, regex=\"^(en|es|auto)$\")\n",
    "    \n",
    "    @validator('text')\n",
    "    def validate_text(cls, v):\n",
    "        # Remove excessive whitespace\n",
    "        v = re.sub(r'\\s+', ' ', v.strip())\n",
    "        if not v:\n",
    "            raise ValueError('Text cannot be empty after cleaning')\n",
    "        return v\n",
    "\n",
    "class BatchSentimentRequest(BaseModel):\n",
    "    \"\"\"Request para an√°lisis de sentimientos en lote\"\"\"\n",
    "    texts: List[str] = Field(..., min_items=1, max_items=1000)\n",
    "    model_type: str = Field(\"ensemble\", regex=\"^(bert|lstm|traditional|ensemble|vader)$\")\n",
    "    \n",
    "    @validator('texts')\n",
    "    def validate_texts(cls, v):\n",
    "        cleaned_texts = []\n",
    "        for text in v:\n",
    "            cleaned = re.sub(r'\\s+', ' ', text.strip())\n",
    "            if cleaned:\n",
    "                cleaned_texts.append(cleaned)\n",
    "        if not cleaned_texts:\n",
    "            raise ValueError('At least one valid text is required')\n",
    "        return cleaned_texts\n",
    "\n",
    "class ProductSentimentRequest(BaseModel):\n",
    "    \"\"\"Request para an√°lisis de sentimientos de producto\"\"\"\n",
    "    product_id: int = Field(..., gt=0)\n",
    "    reviews: List[str] = Field(..., min_items=1)\n",
    "    include_aspects: bool = True\n",
    "\n",
    "class SentimentResult(BaseModel):\n",
    "    \"\"\"Resultado de an√°lisis de sentimiento\"\"\"\n",
    "    text: str\n",
    "    sentiment: SentimentType\n",
    "    confidence: float = Field(ge=0, le=1)\n",
    "    scores: Dict[str, float]\n",
    "    emotion: Optional[str] = None\n",
    "    emotion_confidence: Optional[float] = Field(None, ge=0, le=1)\n",
    "    key_phrases: List[str]\n",
    "    aspects: Optional[Dict[str, str]] = None\n",
    "    language: str\n",
    "    word_count: int = Field(ge=0)\n",
    "\n",
    "class SentimentAnalysisResponse(BaseResponse):\n",
    "    \"\"\"Respuesta de an√°lisis de sentimientos\"\"\"\n",
    "    sentiment_result: SentimentResult\n",
    "    model_used: str\n",
    "    processing_time_ms: Optional[float] = None\n",
    "\n",
    "class BatchSentimentResponse(BaseResponse):\n",
    "    \"\"\"Respuesta de an√°lisis de sentimientos en lote\"\"\"\n",
    "    total_texts: int\n",
    "    results: List[SentimentResult]\n",
    "    summary: Dict[str, Any]\n",
    "    processing_time_ms: float\n",
    "\n",
    "# Comprehensive Analysis Schemas\n",
    "class ProductInsightsRequest(BaseModel):\n",
    "    \"\"\"Request para insights comprehensivos de producto\"\"\"\n",
    "    product_id: int = Field(..., gt=0)\n",
    "    days_back: int = Field(90, ge=7, le=365)\n",
    "    include_predictions: bool = True\n",
    "    include_recommendations: bool = True\n",
    "    include_sentiment: bool = True\n",
    "    include_pricing: bool = True\n",
    "\n",
    "class UserProfileRequest(BaseModel):\n",
    "    \"\"\"Request para an√°lisis de perfil de usuario\"\"\"\n",
    "    user_id: int = Field(..., gt=0)\n",
    "    include_recommendations: bool = True\n",
    "    include_anomalies: bool = True\n",
    "    include_patterns: bool = True\n",
    "\n",
    "class ProductInsightsResponse(BaseResponse):\n",
    "    \"\"\"Respuesta de insights de producto\"\"\"\n",
    "    product_id: int\n",
    "    analysis_period_days: int\n",
    "    stock_insights: Optional[Dict[str, Any]] = None\n",
    "    pricing_insights: Optional[Dict[str, Any]] = None\n",
    "    sentiment_insights: Optional[Dict[str, Any]] = None\n",
    "    recommendation_insights: Optional[Dict[str, Any]] = None\n",
    "    risk_assessment: Dict[str, Any]\n",
    "    action_recommendations: List[str]\n",
    "\n",
    "class UserProfileResponse(BaseResponse):\n",
    "    \"\"\"Respuesta de an√°lisis de perfil de usuario\"\"\"\n",
    "    user_id: int\n",
    "    profile_summary: Dict[str, Any]\n",
    "    behavioral_insights: Dict[str, Any]\n",
    "    recommendations: Optional[List[ProductRecommendation]] = None\n",
    "    anomalies: Optional[List[AnomalyResult]] = None\n",
    "    risk_score: float = Field(ge=0, le=1)\n",
    "    engagement_score: float = Field(ge=0, le=1)\n",
    "\n",
    "# Health and Metrics Schemas\n",
    "class HealthCheckResponse(BaseResponse):\n",
    "    \"\"\"Respuesta de health check\"\"\"\n",
    "    status: str = Field(..., regex=\"^(healthy|degraded|unhealthy)$\")\n",
    "    models_status: Dict[str, bool]\n",
    "    database_connected: bool\n",
    "    redis_connected: bool\n",
    "    memory_usage_mb: Optional[float] = None\n",
    "    uptime_seconds: Optional[float] = None\n",
    "\n",
    "class MetricsResponse(BaseResponse):\n",
    "    \"\"\"Respuesta de m√©tricas\"\"\"\n",
    "    total_requests: int = Field(ge=0)\n",
    "    successful_requests: int = Field(ge=0)\n",
    "    error_rate: float = Field(ge=0, le=1)\n",
    "    average_response_time_ms: float = Field(ge=0)\n",
    "    cache_hit_ratio: float = Field(ge=0, le=1)\n",
    "    models_performance: Dict[str, Dict[str, float]]\n",
    "    resource_usage: Dict[str, float]\n",
    "\n",
    "# Training and Model Management Schemas\n",
    "class ModelTrainingRequest(BaseModel):\n",
    "    \"\"\"Request para entrenamiento de modelos\"\"\"\n",
    "    model_type: str = Field(..., regex=\"^(stock_predictor|recommender|price_optimizer|anomaly_detector|sentiment_analyzer)$\")\n",
    "    training_data_path: Optional[str] = None\n",
    "    hyperparameters: Optional[Dict[str, Any]] = None\n",
    "    validation_split: float = Field(0.2, ge=0.1, le=0.5)\n",
    "\n",
    "class ModelTrainingResponse(BaseResponse):\n",
    "    \"\"\"Respuesta de entrenamiento de modelos\"\"\"\n",
    "    model_type: str\n",
    "    training_id: str\n",
    "    status: str = Field(..., regex=\"^(started|in_progress|completed|failed)$\")\n",
    "    training_metrics: Optional[Dict[str, float]] = None\n",
    "    estimated_completion_time: Optional[datetime] = None\n",
    "\n",
    "class ModelStatusRequest(BaseModel):\n",
    "    \"\"\"Request para estado de modelo\"\"\"\n",
    "    model_type: str = Field(..., regex=\"^(stock_predictor|recommender|price_optimizer|anomaly_detector|sentiment_analyzer)$\")\n",
    "\n",
    "class ModelStatusResponse(BaseResponse):\n",
    "    \"\"\"Respuesta de estado de modelo\"\"\"\n",
    "    model_type: str\n",
    "    is_loaded: bool\n",
    "    last_trained: Optional[datetime] = None\n",
    "    accuracy_metrics: Optional[Dict[str, float]] = None\n",
    "    version: str\n",
    "    size_mb: Optional[float] = None\n",
    "\n",
    "# Validation Helpers\n",
    "class PaginationParams(BaseModel):\n",
    "    \"\"\"Par√°metros de paginaci√≥n\"\"\"\n",
    "    page: int = Field(1, ge=1)\n",
    "    page_size: int = Field(20, ge=1, le=100)\n",
    "    \n",
    "    @property\n",
    "    def offset(self) -> int:\n",
    "        return (self.page - 1) * self.page_size\n",
    "\n",
    "class DateRangeParams(BaseModel):\n",
    "    \"\"\"Par√°metros de rango de fechas\"\"\"\n",
    "    start_date: Optional[datetime] = None\n",
    "    end_date: Optional[datetime] = None\n",
    "    \n",
    "    @root_validator\n",
    "    def validate_date_range(cls, values):\n",
    "        start = values.get('start_date')\n",
    "        end = values.get('end_date')\n",
    "        \n",
    "        if start and end and start >= end:\n",
    "            raise ValueError('start_date must be before end_date')\n",
    "        \n",
    "        return values\n",
    "\n",
    "class FilterParams(BaseModel):\n",
    "    \"\"\"Par√°metros de filtrado\"\"\"\n",
    "    category: Optional[str] = None\n",
    "    price_min: Optional[float] = Field(None, ge=0)\n",
    "    price_max: Optional[float] = Field(None, ge=0)\n",
    "    rating_min: Optional[float] = Field(None, ge=0, le=5)\n",
    "    \n",
    "    @root_validator\n",
    "    def validate_price_range(cls, values):\n",
    "        price_min = values.get('price_min')\n",
    "        price_max = values.get('price_max')\n",
    "        \n",
    "        if price_min and price_max and price_min >= price_max:\n",
    "            raise ValueError('price_min must be less than price_max')\n",
    "        \n",
    "        return values\n",
    "\n",
    "# Webhook and Notification Schemas\n",
    "class WebhookConfig(BaseModel):\n",
    "    \"\"\"Configuraci√≥n de webhook\"\"\"\n",
    "    url: str = Field(..., regex=r'^https?://.+')\n",
    "    events: List[str] = Field(..., min_items=1)\n",
    "    secret_key: Optional[str] = None\n",
    "    retry_attempts: int = Field(3, ge=1, le=10)\n",
    "    timeout_seconds: int = Field(30, ge=5, le=300)\n",
    "\n",
    "class NotificationRequest(BaseModel):\n",
    "    \"\"\"Request para notificaci√≥n\"\"\"\n",
    "    event_type: str\n",
    "    entity_id: Union[int, str]\n",
    "    data: Dict[str, Any]\n",
    "    priority: str = Field(\"normal\", regex=\"^(low|normal|high|critical)$\")\n",
    "    \n",
    "class NotificationResponse(BaseResponse):\n",
    "    \"\"\"Respuesta de notificaci√≥n\"\"\"\n",
    "    notification_id: str\n",
    "    status: str = Field(..., regex=\"^(sent|failed|queued)$\")\n",
    "    delivery_attempts: int = Field(ge=0)\n",
    "\n",
    "# Export all schemas for easy importing\n",
    "__all__ = [\n",
    "    # Base\n",
    "    'BaseResponse', 'ErrorResponse',\n",
    "    \n",
    "    # Enums\n",
    "    'SentimentType', 'AnomalyType', 'PricingStrategy', 'RecommendationAlgorithm',\n",
    "    \n",
    "    # Stock Prediction\n",
    "    'StockPredictionRequest', 'StockPredictionBatchRequest', 'StockPredictionResponse',\n",
    "    'ConfidenceInterval',\n",
    "    \n",
    "    # Recommendations\n",
    "    'RecommendationRequest', 'SimilarProductsRequest', 'RecommendationResponse',\n",
    "    'ProductRecommendation',\n",
    "    \n",
    "    # Price Optimization\n",
    "    'PriceOptimizationRequest', 'PriceBatchOptimizationRequest', 'PriceOptimizationResponse',\n",
    "    \n",
    "    # Anomaly Detection\n",
    "    'AnomalyDetectionRequest', 'UserAnomalyRequest', 'AnomalyDetectionResponse',\n",
    "    'AnomalyResult',\n",
    "    \n",
    "    # Sentiment Analysis\n",
    "    'SentimentAnalysisRequest', 'BatchSentimentRequest', 'ProductSentimentRequest',\n",
    "    'SentimentAnalysisResponse', 'BatchSentimentResponse', 'SentimentResult',\n",
    "    \n",
    "    # Comprehensive Analysis\n",
    "    'ProductInsightsRequest', 'UserProfileRequest', 'ProductInsightsResponse',\n",
    "    'UserProfileResponse',\n",
    "    \n",
    "    # Health and Metrics\n",
    "    'HealthCheckResponse', 'MetricsResponse',\n",
    "    \n",
    "    # Model Management\n",
    "    'ModelTrainingRequest', 'ModelTrainingResponse', 'ModelStatusRequest',\n",
    "    'ModelStatusResponse',\n",
    "    \n",
    "    # Helpers\n",
    "    'PaginationParams', 'DateRangeParams', 'FilterParams',\n",
    "    \n",
    "    # Webhooks\n",
    "    'WebhookConfig', 'NotificationRequest', 'NotificationResponse'\n",
    "]\n",
    "'''\n",
    "\n",
    "# Escribir ml_schemas.py\n",
    "with open(\"../app/schemas/ml_schemas.py\", \"w\") as f:\n",
    "    f.write(ml_schemas_content)\n",
    "\n",
    "print(\"‚úÖ ml_schemas.py creado exitosamente\")\n",
    "print(\"üìã Esquemas Pydantic implementados:\")\n",
    "print(\"   ‚Ä¢ Request/Response Models: Validaci√≥n completa de entrada y salida\")\n",
    "print(\"   ‚Ä¢ Enterprise Validation: Reglas de negocio y validaci√≥n empresarial\")\n",
    "print(\"   ‚Ä¢ Type Safety: Tipos estrictos con validaci√≥n autom√°tica\")\n",
    "print(\"   ‚Ä¢ Documentation: Documentaci√≥n autom√°tica de API\")\n",
    "print(\"   ‚Ä¢ Error Handling: Modelos de error estandarizados\")\n",
    "print(\"   ‚Ä¢ Pagination: Soporte para paginaci√≥n est√°ndar\")\n",
    "print(\"   ‚Ä¢ Filtering: Par√°metros de filtrado y b√∫squeda\")\n",
    "print(\"   ‚Ä¢ Webhooks: Configuraci√≥n de notificaciones\")\n",
    "print(\"   ‚Ä¢ Model Management: Esquemas para gesti√≥n de modelos\")\n",
    "print(\"   ‚Ä¢ Comprehensive Coverage: Todos los endpoints cubiertos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
